{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"plkit A wrapper of pytorch-lightning that makes you write even less code. Installation pip install -U plkit Principles Being compatible with pytorch-lightning Using configurations instead of coding if possible Features Compatible with pytorch-lightning Exposed terminal logger Better formatted warnings and errors from pytorch-lightning Trainer from a dictionary (not only from an ArgumentParser or a Namespace ) Data module with automatic split for train, val and test datasets Auto loss function and optimizer Optuna integration Usage From pytorch-lightning's minimal example \"\"\"A minimal example for plkit\"\"\" from pathlib import Path import torch from torchvision import transforms from torchvision.datasets import MNIST from plkit import Module , DataModule , run class Data ( DataModule ): def data_reader ( self ): return MNIST ( Path ( __file__ ) . parent / 'data' , train = True , download = True , transform = transforms . ToTensor ()) class LitClassifier ( Module ): def __init__ ( self , config ): super () . __init__ ( config ) self . l1 = torch . nn . Linear ( 28 * 28 , 10 ) def forward ( self , x ): return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ) . float ())) def training_step ( self , batch , _ ): x , y = batch loss = self . loss_function ( self ( x ), y ) return { 'loss' : loss } if __name__ == '__main__' : configuration = { 'gpus' : 1 , 'data_tvt' : . 05 , # use a small proportion for training 'batch_size' : 32 , 'max_epochs' : 11 } run ( configuration , Data , LitClassifier ) Using exposed logger from plkit import logger # Your logic if __name__ == \"__main__\" : # Now you are able to log something outside Modules logger . info ( 'Fantastic starts ...' ) # ... logger . info ( 'Pipeline done.' ) Best practice (Boilerplate) You can use each objects (trainer, module, datamodule) as the way you do with pytorch-lightning , but we suggest you use them in the following ways: Running locally from plkit import Module , DataModule , run class MyData ( DataModule ): ... class MyModel ( Module ): ... if __name__ == '__main__' : config = { ... } run ( config , MyData , MyModel ) Running via SGE from plkit import Module , DataModule , SGERunner , run # MyData and MyModel definitions if __name__ == '__main__' : config = { ... } sge = SGERunner ( ... ) run ( config , MyData , MyModel , runner = sge ) # or sge . run ( config , MyData , MyModel ) With optuna from plkit import ( Module , DataModule , Optuna , OptunaSuggest , LocalRunner , run ) # MyData and MyModel definitions if __name__ == '__main__' : config = { ... hparam1 = OptunaSuggest ( < default > , < type > , * args , ** kwargs ), hparam2 = OptunaSuggest ( < default > , < type > , * args , ** kwargs ), } runner = LocalRunner () # requires `val_acc` to be logged in `validation_epoch_end` optuna = Optuna ( on = 'val_acc' , n_trials = 10 , direction = 'maximize' ) run ( config , MyData , MyModel , runner = runner , optuna = optuna ) # or runner . run ( config , MyData , MyModel , optuna ) Resources See more examples in ./examples and full documentation","title":"Home"},{"location":"#plkit","text":"A wrapper of pytorch-lightning that makes you write even less code.","title":"plkit"},{"location":"#installation","text":"pip install -U plkit","title":"Installation"},{"location":"#principles","text":"Being compatible with pytorch-lightning Using configurations instead of coding if possible","title":"Principles"},{"location":"#features","text":"Compatible with pytorch-lightning Exposed terminal logger Better formatted warnings and errors from pytorch-lightning Trainer from a dictionary (not only from an ArgumentParser or a Namespace ) Data module with automatic split for train, val and test datasets Auto loss function and optimizer Optuna integration","title":"Features"},{"location":"#usage","text":"","title":"Usage"},{"location":"#from-pytorch-lightnings-minimal-example","text":"\"\"\"A minimal example for plkit\"\"\" from pathlib import Path import torch from torchvision import transforms from torchvision.datasets import MNIST from plkit import Module , DataModule , run class Data ( DataModule ): def data_reader ( self ): return MNIST ( Path ( __file__ ) . parent / 'data' , train = True , download = True , transform = transforms . ToTensor ()) class LitClassifier ( Module ): def __init__ ( self , config ): super () . __init__ ( config ) self . l1 = torch . nn . Linear ( 28 * 28 , 10 ) def forward ( self , x ): return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ) . float ())) def training_step ( self , batch , _ ): x , y = batch loss = self . loss_function ( self ( x ), y ) return { 'loss' : loss } if __name__ == '__main__' : configuration = { 'gpus' : 1 , 'data_tvt' : . 05 , # use a small proportion for training 'batch_size' : 32 , 'max_epochs' : 11 } run ( configuration , Data , LitClassifier )","title":"From pytorch-lightning's minimal example"},{"location":"#using-exposed-logger","text":"from plkit import logger # Your logic if __name__ == \"__main__\" : # Now you are able to log something outside Modules logger . info ( 'Fantastic starts ...' ) # ... logger . info ( 'Pipeline done.' )","title":"Using exposed logger"},{"location":"#best-practice-boilerplate","text":"You can use each objects (trainer, module, datamodule) as the way you do with pytorch-lightning , but we suggest you use them in the following ways:","title":"Best practice (Boilerplate)"},{"location":"#running-locally","text":"from plkit import Module , DataModule , run class MyData ( DataModule ): ... class MyModel ( Module ): ... if __name__ == '__main__' : config = { ... } run ( config , MyData , MyModel )","title":"Running locally"},{"location":"#running-via-sge","text":"from plkit import Module , DataModule , SGERunner , run # MyData and MyModel definitions if __name__ == '__main__' : config = { ... } sge = SGERunner ( ... ) run ( config , MyData , MyModel , runner = sge ) # or sge . run ( config , MyData , MyModel )","title":"Running via SGE"},{"location":"#with-optuna","text":"from plkit import ( Module , DataModule , Optuna , OptunaSuggest , LocalRunner , run ) # MyData and MyModel definitions if __name__ == '__main__' : config = { ... hparam1 = OptunaSuggest ( < default > , < type > , * args , ** kwargs ), hparam2 = OptunaSuggest ( < default > , < type > , * args , ** kwargs ), } runner = LocalRunner () # requires `val_acc` to be logged in `validation_epoch_end` optuna = Optuna ( on = 'val_acc' , n_trials = 10 , direction = 'maximize' ) run ( config , MyData , MyModel , runner = runner , optuna = optuna ) # or runner . run ( config , MyData , MyModel , optuna )","title":"With optuna"},{"location":"#resources","text":"See more examples in ./examples and full documentation","title":"Resources"},{"location":"configurations/","text":"Ones of the principles of plkit is to try to put configuration items in just one dictionary for data and module construction. Any items that work as arguments for pytorch-lightning 's Trainer initialization could be valid configuration items ( See Trainer API from pytorch-lightning 's documentation ). We do have some different or additional configuration items, in terms of their values or behaviors. seed For full reproducibility, one should call seed_everything and set deterministic to True for trainer initialization using pytorch-lightning (see Reproducibility ) However, with plkit , you only need to set a seed in the configuration ( seed_everything will be set automatically), and deterministic will be automatically set to True for trainer initialization. If you don't want deterministic to be True when a seed is specified, you can set deterministric to False in configuration. num_classes Specification of num_classes in configuration ensures the builtin measurement calling the right loss function and metric for the output and labels (see configuration loss and Builtin measurement for more details). data_num_workers num_workers argument of DataLoader for DataModule data_tvt Train-val-test ratio for splitting the data read by DataModule.data_reader . It could be a tuple with no more than 3 elements or just a single scalar element. The elements could be ratios (<=1) or absolute numbers. The first element is for train set and later two are for validation and test sets, which can be a list respectively as multiple sets for validation and test. If the ratio or number is not specified for the corresponding dataset, such dataset will not be generated. For example: data_tvt Meaning .7 Use 70% of data for training (no val or test data). (.7, .1) Use 70% for training, 10% for val (no test data) (.7, .15, .15) Use 70% for training, 15% for val and 15% for test 300 Use 300 samples for training (300, [100, 100], [100, 100] Use 300 samples for training, 100 samples for validation (x2) and 100 for testing (x2) optim The name of optimizer (Currently only adam and sgd are supported). learning_rate Learning rate for optimizers momentum Momentum for SGD optimizer. loss The loss function. It's auto by default, meaning nn.MSELoss() for regression ( num_classes=1 ) and nn.CrossEntropyLoss() for classification. You can specifiy your own loss function: loss=nn.L1Loss() for example.","title":"Configurations"},{"location":"configurations/#seed","text":"For full reproducibility, one should call seed_everything and set deterministic to True for trainer initialization using pytorch-lightning (see Reproducibility ) However, with plkit , you only need to set a seed in the configuration ( seed_everything will be set automatically), and deterministic will be automatically set to True for trainer initialization. If you don't want deterministic to be True when a seed is specified, you can set deterministric to False in configuration.","title":"seed"},{"location":"configurations/#num_classes","text":"Specification of num_classes in configuration ensures the builtin measurement calling the right loss function and metric for the output and labels (see configuration loss and Builtin measurement for more details).","title":"num_classes"},{"location":"configurations/#data_num_workers","text":"num_workers argument of DataLoader for DataModule","title":"data_num_workers"},{"location":"configurations/#data_tvt","text":"Train-val-test ratio for splitting the data read by DataModule.data_reader . It could be a tuple with no more than 3 elements or just a single scalar element. The elements could be ratios (<=1) or absolute numbers. The first element is for train set and later two are for validation and test sets, which can be a list respectively as multiple sets for validation and test. If the ratio or number is not specified for the corresponding dataset, such dataset will not be generated. For example: data_tvt Meaning .7 Use 70% of data for training (no val or test data). (.7, .1) Use 70% for training, 10% for val (no test data) (.7, .15, .15) Use 70% for training, 15% for val and 15% for test 300 Use 300 samples for training (300, [100, 100], [100, 100] Use 300 samples for training, 100 samples for validation (x2) and 100 for testing (x2)","title":"data_tvt"},{"location":"configurations/#optim","text":"The name of optimizer (Currently only adam and sgd are supported).","title":"optim"},{"location":"configurations/#learning_rate","text":"Learning rate for optimizers","title":"learning_rate"},{"location":"configurations/#momentum","text":"Momentum for SGD optimizer.","title":"momentum"},{"location":"configurations/#loss","text":"The loss function. It's auto by default, meaning nn.MSELoss() for regression ( num_classes=1 ) and nn.CrossEntropyLoss() for classification. You can specifiy your own loss function: loss=nn.L1Loss() for example.","title":"loss"},{"location":"datamodule/","text":"DatModule from plkit is a subclass of LightningDataModule from pytorch-lightning . So you can follow everything that is documented by pytorch-lightning Other than that, we have two additional methods defined: data_reader to read the data into a collection and data_splits to split the data collection for training, validation and testing. The data_reader method is required to be defined if you want to use the features of plkit 's DataModule (auto-splitting data for example). Once you have it defined, you don't need to care about data_prepare and setup methods that LightningDataModule requires. data_reader You can read data into a list of samples by data_reader or yield the samples to save some memory. Note If data_reader is yielding (it is a generateor), plkit.data.IterDataset will be used, and samples can not be suffled. Tip You can yield multiple features, as well as the labels at the same time as a tuple. For example: class MyData ( plkit . DataModule ): ... def data_reader ( yield ( sample_name , feature_a , feature_b , label ) Then in training_step , validation_step or test_step , you can easily decouple them like this: class MyModule ( plkit . Module ): ... def training_step ( self , batch , _ ): # Each variable has this batch of features sample_name , feature_a , feature_b , label = batch This is also the case when a list of samples returned from data_reader . data_splits This method is supposed to split the data collection returned (yielded) from data_reader . If you have data_tvt (see data_tvt in configuration ) specified in configuration, the collection will be specified automatically based on data_tvt . Otherwise, you can return a dictionary like this from data_splits to split the data by yourself: from plkit.data import DataModule , Dataset , IterDataset class MyData ( DataModule ): ... def data_splits ( self , data , stage ): return { 'train' : Dataset ( ... ), 'val' : Dataset ( ... ), # or a list of datasets, 'test' : Dataset ( ... ), # or a list of datasets } Note setup is only calling at stage fit . If you want to do it at test stage, you will need to override setup method.","title":"DataModule"},{"location":"datamodule/#data_reader","text":"You can read data into a list of samples by data_reader or yield the samples to save some memory. Note If data_reader is yielding (it is a generateor), plkit.data.IterDataset will be used, and samples can not be suffled. Tip You can yield multiple features, as well as the labels at the same time as a tuple. For example: class MyData ( plkit . DataModule ): ... def data_reader ( yield ( sample_name , feature_a , feature_b , label ) Then in training_step , validation_step or test_step , you can easily decouple them like this: class MyModule ( plkit . Module ): ... def training_step ( self , batch , _ ): # Each variable has this batch of features sample_name , feature_a , feature_b , label = batch This is also the case when a list of samples returned from data_reader .","title":"data_reader"},{"location":"datamodule/#data_splits","text":"This method is supposed to split the data collection returned (yielded) from data_reader . If you have data_tvt (see data_tvt in configuration ) specified in configuration, the collection will be specified automatically based on data_tvt . Otherwise, you can return a dictionary like this from data_splits to split the data by yourself: from plkit.data import DataModule , Dataset , IterDataset class MyData ( DataModule ): ... def data_splits ( self , data , stage ): return { 'train' : Dataset ( ... ), 'val' : Dataset ( ... ), # or a list of datasets, 'test' : Dataset ( ... ), # or a list of datasets } Note setup is only calling at stage fit . If you want to do it at test stage, you will need to override setup method.","title":"data_splits"},{"location":"module/","text":"Class Module is subclassed from LightningModule of pytorch-lightning . The differences are list below. Initialization Every instance of Module needs a config argument to initialize. It collapses the OptunaSuggest objects in it and becomes an attributes of the instance. So the configuration item can be accessed by self.config.xxx . Danger self.config is a FrozenDiot object to prevent the changes to be passed down to other modules (See diot ). on_epoch_end An empty line is printed on every epoch end to keep the progress bar of the previous epoch run so that we can easily keep track of the metrics of all epochs. If you want to disable this just overwrite on_epoch_end with a pass statement. Or if you want to keep this and do something else in on_epoch_end , you should call super().on_epoch_end() inside your on_epoch_end . loss_function Calculate loss using the loss function according to the loss configuration item. configure_optimizers You don't have to implement this function if you have optim specified in configuration. Note Currently only adam and sgd are supported. Please also specify learning_rate and/or momentum for corresponding optimizers (see configurations ). measure This method calculates a metric according to num_classes . For num_classes=1 (regression), available metrics are mse , rmse , mae and rmsle . And for classifications, avaiable metrics are accuracy , precision , recall , f1_score , iou , fbeta_score , auroc , average_precision and dice_score . To use it in your training_step , validation_step or test_step , you can do: self . measure ( output , labels , 'accuracy' ) If the metric need extra arguments, you can pass them in as well: self . measure ( output , labels , 'fbeta_score' , beta = 1.0 ) For more details of the metrics, see pytorch-lightning 's documentation","title":"Module"},{"location":"module/#initialization","text":"Every instance of Module needs a config argument to initialize. It collapses the OptunaSuggest objects in it and becomes an attributes of the instance. So the configuration item can be accessed by self.config.xxx . Danger self.config is a FrozenDiot object to prevent the changes to be passed down to other modules (See diot ).","title":"Initialization"},{"location":"module/#on_epoch_end","text":"An empty line is printed on every epoch end to keep the progress bar of the previous epoch run so that we can easily keep track of the metrics of all epochs. If you want to disable this just overwrite on_epoch_end with a pass statement. Or if you want to keep this and do something else in on_epoch_end , you should call super().on_epoch_end() inside your on_epoch_end .","title":"on_epoch_end"},{"location":"module/#loss_function","text":"Calculate loss using the loss function according to the loss configuration item.","title":"loss_function"},{"location":"module/#configure_optimizers","text":"You don't have to implement this function if you have optim specified in configuration. Note Currently only adam and sgd are supported. Please also specify learning_rate and/or momentum for corresponding optimizers (see configurations ).","title":"configure_optimizers"},{"location":"module/#measure","text":"This method calculates a metric according to num_classes . For num_classes=1 (regression), available metrics are mse , rmse , mae and rmsle . And for classifications, avaiable metrics are accuracy , precision , recall , f1_score , iou , fbeta_score , auroc , average_precision and dice_score . To use it in your training_step , validation_step or test_step , you can do: self . measure ( output , labels , 'accuracy' ) If the metric need extra arguments, you can pass them in as well: self . measure ( output , labels , 'fbeta_score' , beta = 1.0 ) For more details of the metrics, see pytorch-lightning 's documentation","title":"measure"},{"location":"optuna/","text":"plkit integrates the optuna framework to tune the hyperparameters. You don't have to modify your code to much to enable it by following our best practice . To enable optuna, you just need to pass an optuna object to plkit.run or runner.run method. To initialize an optuna object, you need to pass in following arguments: on : telling plkit which metric to optimize on. You need to log that metric inside validation_epoch_end for the object to fetch it. n_trials : How many trials you want to run **kwargs : Other arguments for optuna.create_study (see optuna.study.create_study ). OptunaSuggest Defines how the parameters should be tuning. You can specify an OptunaSuggest object to a parameter in configuration like this: configuration = { # default, type, low, high 'learning_rate' : OptunaSuggest ( 1e-3 , 'float' , 1e-5 , 5e-2 ) } This way, yon don't have to modify the configuration when you want to enable or disable optuna integration. When optuna is enabled, a value will be generated for learning_rate using trial.suggest_float('learning_rate', 1e-5, 5e-2) and the default value is ignored. While it's disabled, it will fall back to the default value. See all avaiable suggest types here and corresponding names (aliases) for the types as the second argument of OptunaSuggest Name/Alias optuna.trial.Trial.suggest_xxx cat suggest_categorical categorical suggest_categorical distuni suggest_discrete_uniform dist_uni suggest_discrete_uniform discrete_uniform suggest_discrete_uniform float suggest_float int suggest_int loguni suggest_loguniform log_uni suggest_loguniform uni suggest_uniform","title":"Optuna integration"},{"location":"optuna/#optunasuggest","text":"Defines how the parameters should be tuning. You can specify an OptunaSuggest object to a parameter in configuration like this: configuration = { # default, type, low, high 'learning_rate' : OptunaSuggest ( 1e-3 , 'float' , 1e-5 , 5e-2 ) } This way, yon don't have to modify the configuration when you want to enable or disable optuna integration. When optuna is enabled, a value will be generated for learning_rate using trial.suggest_float('learning_rate', 1e-5, 5e-2) and the default value is ignored. While it's disabled, it will fall back to the default value. See all avaiable suggest types here and corresponding names (aliases) for the types as the second argument of OptunaSuggest Name/Alias optuna.trial.Trial.suggest_xxx cat suggest_categorical categorical suggest_categorical distuni suggest_discrete_uniform dist_uni suggest_discrete_uniform discrete_uniform suggest_discrete_uniform float suggest_float int suggest_int loguni suggest_loguniform log_uni suggest_loguniform uni suggest_uniform","title":"OptunaSuggest"},{"location":"runner/","text":"There are two runners supported: LocalRunner and SGERunner . The first one submits the job locally and uses local CPUs/GPUs, and the second one uses grid CPUs/GPUs. Local runner It is easy to initialize a local runner: runner = LocalRunner() SGE runner But to initialize an sge runner, you need pass in a couple of arguments, which will be translated as command line arguments for qsub . For example: from plkit import SGERunner sge = SGERunner ( o = '/path/to/stdout' ) # will be calling qsub like this: # $ qsub -o /path/to/stdout You can also specify the path to the qsub executable and the path to a workdir for the runner to save the outputs, errors and scripts for each jobs: Note The job is submitted in background, and the main process will be then streaming the stdandard output from the job. You may have to end the program yourself after it's done. sge = SGERunner ( qsub = '/path/to/qsub' , workdir = '/path/to/workdir' )","title":"Runner"},{"location":"runner/#local-runner","text":"It is easy to initialize a local runner: runner = LocalRunner()","title":"Local runner"},{"location":"runner/#sge-runner","text":"But to initialize an sge runner, you need pass in a couple of arguments, which will be translated as command line arguments for qsub . For example: from plkit import SGERunner sge = SGERunner ( o = '/path/to/stdout' ) # will be calling qsub like this: # $ qsub -o /path/to/stdout You can also specify the path to the qsub executable and the path to a workdir for the runner to save the outputs, errors and scripts for each jobs: Note The job is submitted in background, and the main process will be then streaming the stdandard output from the job. You may have to end the program yourself after it's done. sge = SGERunner ( qsub = '/path/to/qsub' , workdir = '/path/to/workdir' )","title":"SGE runner"},{"location":"trainer/","text":"The Trainer class is also a subclass of Trainer from pytorch-lightning , with an addition class method from_config (aka from_dict ), which initializes a trainer object. So that you can do: configuration = { 'gpus' : 1 , 'data_tvt' : . 05 , # use a small proportion for training 'batch_size' : 32 , 'max_epochs' : 11 } trainer = Trainer . from_config ( configuration ) # or if you have some extra arguments trainer = Trainer . from_config ( configuration , ** kwargs )","title":"Trainer"},{"location":"api/plkit.data/","text":"module plkit . data </> Data module for plkit Classes Dataset \u2014 The dataset that used internally by Data class </> IterDataset \u2014 Iterable dataset </> DataModule \u2014 Data module for plkit </> class plkit.data . Dataset ( data , ids=None ) </> Bases torch.utils.data.dataset.Dataset The dataset that used internally by Data class Examples >>> ds = Dataset ( data = [( 'a' , 'x' ), ( 'b' , 'y' ), ( 'c' , 'z' )], ids = [ 1 , 2 ]) >>> len ( ds ) == 2 >>> ds [ 0 ] == ( 'b' , 'y' ) >>> ds [ 1 ] == ( 'c' , 'z' ) >>> # The features are what you get by >>> # x, y = batch Parameters data (iterable of tuple) \u2014 The data for the dataset. It could be a tuple of features. Each one should be an iterable, which could be accessed by index ids (list of int, optional) \u2014 The ids or keys of the data, which should be in the same order of each feature in the iterable. class plkit.data . IterDataset ( data , length ) </> Bases torch.utils.data.dataset.IterableDataset torch.utils.data.dataset.Dataset Iterable dataset The iterable dataset where each feature of the data is an iterable Examples >>> feat1 = ( x for x in range ( 10 ) >>> feat2 = ( x for x in range ( 10 ) >>> ds = IterDataset ( zip ( feat1 , feat2 ), ids = [ 4 , 3 ]) >>> next ( ds ) == ( 0 , 0 ) Parameters data (iterable of tuple) \u2014 a tuple of iterable features length (int) \u2014 The length of the iterables class plkit.data . DataModule ( *args , **kwargs ) </> Bases pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks Data module for plkit Attributes dims \u2014 A tuple describing the shape of your data. Extra functionality exposed in size . </> has_prepared_data \u2014 Return bool letting you know if datamodule.prepare_data() has been called or not. </> has_setup_fit \u2014 Return bool letting you know if datamodule.setup('fit') has been called or not. </> has_setup_test \u2014 Return bool letting you know if datamodule.setup('test') has been called or not. </> length \u2014 The length of the data This is required when self.data_reader() yields (it is a generator) </> test_transforms \u2014 Optional transforms (or collection of transforms) you can apply to test dataset </> train_transforms \u2014 Optional transforms (or collection of transforms) you can apply to train dataset </> val_transforms \u2014 Optional transforms (or collection of transforms) you can apply to validation dataset </> Classes _DataModuleWrapper \u2014 type(object_or_name, bases, dict) type(object) -> the object's type type(name, bases, dict) -> a new type </> Methods add_argparse_args ( parent_parser ) (ArgumentParser) \u2014 Extends existing argparse by default LightningDataModule attributes. </> data_reader ( ) (Union(iterable of any, (iterator of any))) \u2014 Read the data </> data_splits ( data , stage ) (dict(str: Union(dataset, iterabledataset, list of dataset or iterabledataset))) \u2014 Split data from data_source for each dataloader </> from_argparse_args ( args , **kwargs ) \u2014 Create an instance from CLI arguments. </> get_init_arguments_and_types ( ) (List with tuples of 3 values) \u2014 Scans the DataModule signature and returns argument names, types and default values. </> on_load_checkpoint ( checkpoint ) \u2014 Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. </> on_save_checkpoint ( checkpoint ) \u2014 Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. </> prepare_data ( *args , **kwargs ) \u2014 Prepare data </> setup ( stage ) \u2014 Setup data </> size ( dim ) (tuple or int) \u2014 Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. </> test_dataloader ( *args , **kwargs ) (Union(dataloader, list of dataloader)) \u2014 Test data loaders </> train_dataloader ( *args , **kwargs ) (DataLoader) \u2014 Train data loaders </> transfer_batch_to_device ( batch , device ) (any) \u2014 Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. </> val_dataloader ( *args , **kwargs ) (Union(dataloader, list of dataloader)) \u2014 Validation data loaders </> method on_load_checkpoint ( checkpoint ) </> Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Parameters checkpoint (dict(str: any)) \u2014 Loaded checkpoint Example .. code-block:: python def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. method on_save_checkpoint ( checkpoint ) </> Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Parameters checkpoint (dict(str: any)) \u2014 Checkpoint to be saved Example .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. class pytorch_lightning.core.datamodule. _DataModuleWrapper ( *args , **kwargs ) </> type(object_or_name, bases, dict) type(object) -> the object's type type(name, bases, dict) -> a new type Methods __call__ ( cls , *args , **kwargs ) \u2014 A wrapper for LightningDataModule that: </> staticmethod __call__ ( cls , *args , **kwargs ) </> A wrapper for LightningDataModule that: Runs user defined subclass's init Assures prepare_data() runs on rank 0 Lets you check prepare_data and setup to see if they've been called method size ( dim=None ) \u2192 tuple or int </> Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. abstract method transfer_batch_to_device ( batch , device ) </> Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Parameters batch (any) \u2014 A batch of data that needs to be transferred to a new device. device (device) \u2014 The target device as defined in PyTorch. Returns (any) A reference to the data on the new device. Note This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). Note This hook only runs on single GPU training (no data-parallel). If you need multi-GPU support for your custom batch objects, you need to define your custom :class: ~torch.nn.parallel.DistributedDataParallel or :class: ~pytorch_lightning.overrides.data_parallel.LightningDistributedDataParallel and override :meth: ~pytorch_lightning.core.lightning.LightningModule.configure_ddp . See Also :func: ~pytorch_lightning.utilities.apply_func.move_data_to_device :func: ~pytorch_lightning.utilities.apply_func.apply_to_collection classmethod add_argparse_args ( parent_parser ) \u2192 ArgumentParser </> Extends existing argparse by default LightningDataModule attributes. classmethod from_argparse_args ( args , **kwargs ) </> Create an instance from CLI arguments. Parameters args (Namespace or ArgumentParser) \u2014 The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: LightningDataModule . **kwargs \u2014 Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args) classmethod get_init_arguments_and_types ( ) </> Scans the DataModule signature and returns argument names, types and default values. Returns (List with tuples of 3 values) . method data_reader ( ) </> Read the data Returns (Union(iterable of any, (iterator of any))) A tuple of iterables of features. Or it yields the following Yields (Union(iterable of any, (iterator of any))) An iterable of tuple of features. In such a case, self.length property is required to be defined. method data_splits ( data=None , stage=None ) </> Split data from data_source for each dataloader Parameters data (iterable of tuple, optional) \u2014 The data read by self.data_reader() stage (str, optional) \u2014 The stage argument same as the one from LightningDataModule.setup(...) Returns (dict(str: Union(dataset, iterabledataset, list of dataset or iterabledataset))) A dictionary with keys train , val and test , and values a Dataset or an IterDataset (config.data_tvt will be ignored) Or if config.data_tvt is specified, one could just return an iterable of features, then the dataset will be automatically split by config.data_tvt method prepare_data ( *args , **kwargs ) </> Prepare data method setup ( stage=None ) </> Setup data method train_dataloader ( *args , **kwargs ) \u2192 DataLoader </> Train data loaders method val_dataloader ( *args , **kwargs ) \u2192 Union(dataloader, list of dataloader) </> Validation data loaders method test_dataloader ( *args , **kwargs ) \u2192 Union(dataloader, list of dataloader) </> Test data loaders","title":"plkit.data"},{"location":"api/plkit.data/#plkitdata","text":"</> Data module for plkit Classes Dataset \u2014 The dataset that used internally by Data class </> IterDataset \u2014 Iterable dataset </> DataModule \u2014 Data module for plkit </> class","title":"plkit.data"},{"location":"api/plkit.data/#plkitdatadataset","text":"</> Bases torch.utils.data.dataset.Dataset The dataset that used internally by Data class Examples >>> ds = Dataset ( data = [( 'a' , 'x' ), ( 'b' , 'y' ), ( 'c' , 'z' )], ids = [ 1 , 2 ]) >>> len ( ds ) == 2 >>> ds [ 0 ] == ( 'b' , 'y' ) >>> ds [ 1 ] == ( 'c' , 'z' ) >>> # The features are what you get by >>> # x, y = batch Parameters data (iterable of tuple) \u2014 The data for the dataset. It could be a tuple of features. Each one should be an iterable, which could be accessed by index ids (list of int, optional) \u2014 The ids or keys of the data, which should be in the same order of each feature in the iterable. class","title":"plkit.data.Dataset"},{"location":"api/plkit.data/#plkitdataiterdataset","text":"</> Bases torch.utils.data.dataset.IterableDataset torch.utils.data.dataset.Dataset Iterable dataset The iterable dataset where each feature of the data is an iterable Examples >>> feat1 = ( x for x in range ( 10 ) >>> feat2 = ( x for x in range ( 10 ) >>> ds = IterDataset ( zip ( feat1 , feat2 ), ids = [ 4 , 3 ]) >>> next ( ds ) == ( 0 , 0 ) Parameters data (iterable of tuple) \u2014 a tuple of iterable features length (int) \u2014 The length of the iterables class","title":"plkit.data.IterDataset"},{"location":"api/plkit.data/#plkitdatadatamodule","text":"</> Bases pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks Data module for plkit Attributes dims \u2014 A tuple describing the shape of your data. Extra functionality exposed in size . </> has_prepared_data \u2014 Return bool letting you know if datamodule.prepare_data() has been called or not. </> has_setup_fit \u2014 Return bool letting you know if datamodule.setup('fit') has been called or not. </> has_setup_test \u2014 Return bool letting you know if datamodule.setup('test') has been called or not. </> length \u2014 The length of the data This is required when self.data_reader() yields (it is a generator) </> test_transforms \u2014 Optional transforms (or collection of transforms) you can apply to test dataset </> train_transforms \u2014 Optional transforms (or collection of transforms) you can apply to train dataset </> val_transforms \u2014 Optional transforms (or collection of transforms) you can apply to validation dataset </> Classes _DataModuleWrapper \u2014 type(object_or_name, bases, dict) type(object) -> the object's type type(name, bases, dict) -> a new type </> Methods add_argparse_args ( parent_parser ) (ArgumentParser) \u2014 Extends existing argparse by default LightningDataModule attributes. </> data_reader ( ) (Union(iterable of any, (iterator of any))) \u2014 Read the data </> data_splits ( data , stage ) (dict(str: Union(dataset, iterabledataset, list of dataset or iterabledataset))) \u2014 Split data from data_source for each dataloader </> from_argparse_args ( args , **kwargs ) \u2014 Create an instance from CLI arguments. </> get_init_arguments_and_types ( ) (List with tuples of 3 values) \u2014 Scans the DataModule signature and returns argument names, types and default values. </> on_load_checkpoint ( checkpoint ) \u2014 Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. </> on_save_checkpoint ( checkpoint ) \u2014 Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. </> prepare_data ( *args , **kwargs ) \u2014 Prepare data </> setup ( stage ) \u2014 Setup data </> size ( dim ) (tuple or int) \u2014 Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. </> test_dataloader ( *args , **kwargs ) (Union(dataloader, list of dataloader)) \u2014 Test data loaders </> train_dataloader ( *args , **kwargs ) (DataLoader) \u2014 Train data loaders </> transfer_batch_to_device ( batch , device ) (any) \u2014 Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. </> val_dataloader ( *args , **kwargs ) (Union(dataloader, list of dataloader)) \u2014 Validation data loaders </> method","title":"plkit.data.DataModule"},{"location":"api/plkit.data/#pytorch_lightningcorehookscheckpointhookson_load_checkpoint","text":"</> Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Parameters checkpoint (dict(str: any)) \u2014 Loaded checkpoint Example .. code-block:: python def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. method","title":"pytorch_lightning.core.hooks.CheckpointHooks.on_load_checkpoint"},{"location":"api/plkit.data/#pytorch_lightningcorehookscheckpointhookson_save_checkpoint","text":"</> Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Parameters checkpoint (dict(str: any)) \u2014 Checkpoint to be saved Example .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. class","title":"pytorch_lightning.core.hooks.CheckpointHooks.on_save_checkpoint"},{"location":"api/plkit.data/#pytorch_lightningcoredatamodule_datamodulewrapper","text":"</> type(object_or_name, bases, dict) type(object) -> the object's type type(name, bases, dict) -> a new type Methods __call__ ( cls , *args , **kwargs ) \u2014 A wrapper for LightningDataModule that: </> staticmethod __call__ ( cls , *args , **kwargs ) </> A wrapper for LightningDataModule that: Runs user defined subclass's init Assures prepare_data() runs on rank 0 Lets you check prepare_data and setup to see if they've been called method","title":"pytorch_lightning.core.datamodule._DataModuleWrapper"},{"location":"api/plkit.data/#pytorch_lightningcoredatamodulelightningdatamodulesize","text":"</> Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. abstract method","title":"pytorch_lightning.core.datamodule.LightningDataModule.size"},{"location":"api/plkit.data/#pytorch_lightningcoredatamodulelightningdatamoduletransfer_batch_to_device","text":"</> Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Parameters batch (any) \u2014 A batch of data that needs to be transferred to a new device. device (device) \u2014 The target device as defined in PyTorch. Returns (any) A reference to the data on the new device. Note This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). Note This hook only runs on single GPU training (no data-parallel). If you need multi-GPU support for your custom batch objects, you need to define your custom :class: ~torch.nn.parallel.DistributedDataParallel or :class: ~pytorch_lightning.overrides.data_parallel.LightningDistributedDataParallel and override :meth: ~pytorch_lightning.core.lightning.LightningModule.configure_ddp . See Also :func: ~pytorch_lightning.utilities.apply_func.move_data_to_device :func: ~pytorch_lightning.utilities.apply_func.apply_to_collection classmethod","title":"pytorch_lightning.core.datamodule.LightningDataModule.transfer_batch_to_device"},{"location":"api/plkit.data/#pytorch_lightningcoredatamodulelightningdatamoduleadd_argparse_args","text":"</> Extends existing argparse by default LightningDataModule attributes. classmethod","title":"pytorch_lightning.core.datamodule.LightningDataModule.add_argparse_args"},{"location":"api/plkit.data/#pytorch_lightningcoredatamodulelightningdatamodulefrom_argparse_args","text":"</> Create an instance from CLI arguments. Parameters args (Namespace or ArgumentParser) \u2014 The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: LightningDataModule . **kwargs \u2014 Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args) classmethod","title":"pytorch_lightning.core.datamodule.LightningDataModule.from_argparse_args"},{"location":"api/plkit.data/#pytorch_lightningcoredatamodulelightningdatamoduleget_init_arguments_and_types","text":"</> Scans the DataModule signature and returns argument names, types and default values. Returns (List with tuples of 3 values) . method","title":"pytorch_lightning.core.datamodule.LightningDataModule.get_init_arguments_and_types"},{"location":"api/plkit.data/#plkitdatadatamoduledata_reader","text":"</> Read the data Returns (Union(iterable of any, (iterator of any))) A tuple of iterables of features. Or it yields the following Yields (Union(iterable of any, (iterator of any))) An iterable of tuple of features. In such a case, self.length property is required to be defined. method","title":"plkit.data.DataModule.data_reader"},{"location":"api/plkit.data/#plkitdatadatamoduledata_splits","text":"</> Split data from data_source for each dataloader Parameters data (iterable of tuple, optional) \u2014 The data read by self.data_reader() stage (str, optional) \u2014 The stage argument same as the one from LightningDataModule.setup(...) Returns (dict(str: Union(dataset, iterabledataset, list of dataset or iterabledataset))) A dictionary with keys train , val and test , and values a Dataset or an IterDataset (config.data_tvt will be ignored) Or if config.data_tvt is specified, one could just return an iterable of features, then the dataset will be automatically split by config.data_tvt method","title":"plkit.data.DataModule.data_splits"},{"location":"api/plkit.data/#plkitdatadatamoduleprepare_data","text":"</> Prepare data method","title":"plkit.data.DataModule.prepare_data"},{"location":"api/plkit.data/#plkitdatadatamodulesetup","text":"</> Setup data method","title":"plkit.data.DataModule.setup"},{"location":"api/plkit.data/#plkitdatadatamoduletrain_dataloader","text":"</> Train data loaders method","title":"plkit.data.DataModule.train_dataloader"},{"location":"api/plkit.data/#plkitdatadatamoduleval_dataloader","text":"</> Validation data loaders method","title":"plkit.data.DataModule.val_dataloader"},{"location":"api/plkit.data/#plkitdatadatamoduletest_dataloader","text":"</> Test data loaders","title":"plkit.data.DataModule.test_dataloader"},{"location":"api/plkit.exceptions/","text":"module plkit . exceptions </> Exceptions for plkit Classes PlkitException \u2014 Base exception class for plkit </> PlkitDataException \u2014 Something wrong when preparing data </> PlkitConfigException \u2014 When certain config items are missing </> class plkit.exceptions . PlkitException ( ) </> Bases Exception BaseException Base exception class for plkit class plkit.exceptions . PlkitDataException ( ) </> Bases plkit.exceptions.PlkitException Exception BaseException Something wrong when preparing data class plkit.exceptions . PlkitConfigException ( ) </> Bases plkit.exceptions.PlkitException Exception BaseException When certain config items are missing","title":"plkit.exceptions"},{"location":"api/plkit.exceptions/#plkitexceptions","text":"</> Exceptions for plkit Classes PlkitException \u2014 Base exception class for plkit </> PlkitDataException \u2014 Something wrong when preparing data </> PlkitConfigException \u2014 When certain config items are missing </> class","title":"plkit.exceptions"},{"location":"api/plkit.exceptions/#plkitexceptionsplkitexception","text":"</> Bases Exception BaseException Base exception class for plkit class","title":"plkit.exceptions.PlkitException"},{"location":"api/plkit.exceptions/#plkitexceptionsplkitdataexception","text":"</> Bases plkit.exceptions.PlkitException Exception BaseException Something wrong when preparing data class","title":"plkit.exceptions.PlkitDataException"},{"location":"api/plkit.exceptions/#plkitexceptionsplkitconfigexception","text":"</> Bases plkit.exceptions.PlkitException Exception BaseException When certain config items are missing","title":"plkit.exceptions.PlkitConfigException"},{"location":"api/plkit/","text":"package plkit </> Superset of pytorch-lightning Functions run ( config , data_class , model_class , optuna , runner ) ( Trainer ) \u2014 Run the pipeline by give configuration, model_class, data class, optuna and runner </> module plkit . trainer </> Wrapper of the Trainer class Classes ProgressBar \u2014 Align the Epoch in progress bar </> Trainer \u2014 The Trainner class </> module plkit . runner </> Run jobs via non-local runners. Classes Runner ( ) \u2014 The base class for runner </> LocalRunner \u2014 The local runner for the pipeline </> SGERunner \u2014 The SGE runner for the pipeline </> module plkit . exceptions </> Exceptions for plkit Classes PlkitException \u2014 Base exception class for plkit </> PlkitDataException \u2014 Something wrong when preparing data </> PlkitConfigException \u2014 When certain config items are missing </> module plkit . module </> The core base module class based on pytorch_lightning.LightningModule Classes Module \u2014 The Module class </> module plkit . data </> Data module for plkit Classes Dataset \u2014 The dataset that used internally by Data class </> IterDataset \u2014 Iterable dataset </> DataModule \u2014 Data module for plkit </> module plkit . optuna </> Optuna wrapper for plkit Classes OptunaSuggest \u2014 Optuna suggests for configuration items </> Optuna \u2014 The class uses optuna to automate hyperparameter tuning </> module plkit . utils </> Utility functions for plkit Functions capture_stderr ( ) \u2014 Capture the stderr </> capture_stdout ( ) \u2014 Capture the stdout </> check_config ( config , item , how , msg ) \u2014 Check configuration items </> collapse_suggest_config ( config ) (dict) \u2014 Use the default value of OptunaSuggest for config items. So that the configs can be used in the case that optuna is opted out. </> log_config ( config , title , items_per_row ) \u2014 Log the configurations in a table in terminal </> normalize_tvt_ratio ( tvt_ratio ) (int or float, list of int or float, list of int or float), optional \u2014 Normalize the train-val-test data ratio into a format of (.7, [.1, .1], [.05, .05]). </> output_to_logging ( stdout_level , stderr_level ) \u2014 Capture the stdout or stderr to logging </> plkit_seed_everything ( config ) \u2014 Try to seed everything and set deterministic to True if seed in config has been set </> warning_to_logging ( ) \u2014 Patch the warning message formatting to only show the message </>","title":"plkit"},{"location":"api/plkit/#plkit","text":"</> Superset of pytorch-lightning Functions run ( config , data_class , model_class , optuna , runner ) ( Trainer ) \u2014 Run the pipeline by give configuration, model_class, data class, optuna and runner </> module","title":"plkit"},{"location":"api/plkit/#plkittrainer","text":"</> Wrapper of the Trainer class Classes ProgressBar \u2014 Align the Epoch in progress bar </> Trainer \u2014 The Trainner class </> module","title":"plkit.trainer"},{"location":"api/plkit/#plkitrunner","text":"</> Run jobs via non-local runners. Classes Runner ( ) \u2014 The base class for runner </> LocalRunner \u2014 The local runner for the pipeline </> SGERunner \u2014 The SGE runner for the pipeline </> module","title":"plkit.runner"},{"location":"api/plkit/#plkitexceptions","text":"</> Exceptions for plkit Classes PlkitException \u2014 Base exception class for plkit </> PlkitDataException \u2014 Something wrong when preparing data </> PlkitConfigException \u2014 When certain config items are missing </> module","title":"plkit.exceptions"},{"location":"api/plkit/#plkitmodule","text":"</> The core base module class based on pytorch_lightning.LightningModule Classes Module \u2014 The Module class </> module","title":"plkit.module"},{"location":"api/plkit/#plkitdata","text":"</> Data module for plkit Classes Dataset \u2014 The dataset that used internally by Data class </> IterDataset \u2014 Iterable dataset </> DataModule \u2014 Data module for plkit </> module","title":"plkit.data"},{"location":"api/plkit/#plkitoptuna","text":"</> Optuna wrapper for plkit Classes OptunaSuggest \u2014 Optuna suggests for configuration items </> Optuna \u2014 The class uses optuna to automate hyperparameter tuning </> module","title":"plkit.optuna"},{"location":"api/plkit/#plkitutils","text":"</> Utility functions for plkit Functions capture_stderr ( ) \u2014 Capture the stderr </> capture_stdout ( ) \u2014 Capture the stdout </> check_config ( config , item , how , msg ) \u2014 Check configuration items </> collapse_suggest_config ( config ) (dict) \u2014 Use the default value of OptunaSuggest for config items. So that the configs can be used in the case that optuna is opted out. </> log_config ( config , title , items_per_row ) \u2014 Log the configurations in a table in terminal </> normalize_tvt_ratio ( tvt_ratio ) (int or float, list of int or float, list of int or float), optional \u2014 Normalize the train-val-test data ratio into a format of (.7, [.1, .1], [.05, .05]). </> output_to_logging ( stdout_level , stderr_level ) \u2014 Capture the stdout or stderr to logging </> plkit_seed_everything ( config ) \u2014 Try to seed everything and set deterministic to True if seed in config has been set </> warning_to_logging ( ) \u2014 Patch the warning message formatting to only show the message </>","title":"plkit.utils"},{"location":"api/plkit.module/","text":"module plkit . module </> The core base module class based on pytorch_lightning.LightningModule Classes Module \u2014 The Module class </> class plkit.module . Module ( config ) </> Bases pytorch_lightning.core.lightning.LightningModule abc.ABC pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.grads.GradInformation pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module The Module class on_epoch_end is added to print a newline to keep the progress bar and the stats on it for each epoch. If you don't want this, just overwrite it with: >>> def on_epoch_end ( self ): >>> pass If you have other stuff to do in on_epoch_end , make sure to you call: >>> super () . on_epoch_end () You may or may not need to write loss_function , as it will be inferred from config item loss and num_classes . Basically, MSELoss will be used for regression and CrossEntropyLoss for classification. measure added for convinience to get some metrics between logits and targets. Parameters config \u2014 The configuration dictionary Attributes _loss_func \u2014 The loss function automatic_optimization (bool) \u2014 If False you are responsible for calling .backward, .step, zero_grad. </> config \u2014 The configs current_epoch (int) \u2014 The current epoch </> global_step (int) \u2014 Total training batches seen across all epochs </> num_classes \u2014 Number of classes to predict. 1 for regression on_gpu \u2014 True if your model is currently running on GPUs. Useful to set flags around the LightningModule for different CPU vs GPU behavior. </> optim \u2014 The optimizer name. currently only adam and sgd are supported. With this, of course you can, but you don't need to write configure_optimizers . Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> all_gather ( tensor , group , sync_grads ) \u2014 Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> backward ( loss , optimizer , optimizer_idx , *args , **kwargs ) \u2014 Override backward with your own implementation if you need to. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> configure_optimizers ( ) \u2014 Configure the optimizers </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( *args , **kwargs ) \u2014 Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). </> freeze ( ) \u2014 Freeze all params for inference. </> get_progress_bar_dict ( ) (dict(str: int or str)) \u2014 Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. </> grad_norm ( norm_type ) (dict(str: float)) \u2014 Compute each parameter's gradient's norm and their overall norm. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_from_checkpoint ( checkpoint_path , map_location , hparams_file , strict , **kwargs ) \u2014 Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> log ( name , value , prog_bar , logger , on_step , on_epoch , reduce_fx , tbptt_reduce_fx , tbptt_pad_token , enable_graph , sync_dist , sync_dist_op , sync_dist_group ) \u2014 Log a key, value </> log_dict ( dictionary , prog_bar , logger , on_step , on_epoch , reduce_fx , tbptt_reduce_fx , tbptt_pad_token , enable_graph , sync_dist , sync_dist_op , sync_dist_group ) \u2014 Log a dictonary of values at once </> loss_function ( logits , labels ) \u2014 Calculate the loss </> manual_backward ( loss , optimizer , *args , **kwargs ) \u2014 Call this directly from your training_step when doing optimizations manually. By using this we can ensure that all the proper scaling when using 16-bit etc has been done for you </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> on_after_backward ( ) \u2014 Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. </> on_before_zero_grad ( optimizer ) \u2014 Called after optimizer.step() and before optimizer.zero_grad(). </> on_epoch_end ( ) \u2014 Keep the epoch progress bar This is not documented but working. </> on_epoch_start ( ) \u2014 Called in the training loop at the very beginning of the epoch. </> on_fit_end ( ) \u2014 Called at the very end of fit. If on DDP it is called on every process </> on_fit_start ( ) \u2014 Called at the very beginning of fit. If on DDP it is called on every process </> on_hpc_load ( checkpoint ) \u2014 Hook to do whatever you need right before Slurm manager loads the model. </> on_hpc_save ( checkpoint ) \u2014 Hook to do whatever you need right before Slurm manager saves the model. </> on_load_checkpoint ( checkpoint ) \u2014 Do something with the checkpoint. Gives model a chance to load something before state_dict is restored. </> on_pretrain_routine_end ( ) \u2014 Called at the end of the pretrain routine (between fit and train start). </> on_pretrain_routine_start ( ) \u2014 Called at the beginning of the pretrain routine (between fit and train start). </> on_save_checkpoint ( checkpoint ) \u2014 Give the model a chance to add something to the checkpoint. state_dict is already there. </> on_test_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called in the test loop after the batch. </> on_test_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called in the test loop before anything happens for that batch. </> on_test_epoch_end ( ) \u2014 Called in the test loop at the very end of the epoch. </> on_test_epoch_start ( ) \u2014 Called in the test loop at the very beginning of the epoch. </> on_test_model_eval ( ) \u2014 Sets the model to eval during the test loop </> on_test_model_train ( ) \u2014 Sets the model to train during the test loop </> on_train_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called in the training loop after the batch. </> on_train_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called in the training loop before anything happens for that batch. </> on_train_end ( ) \u2014 Called at the end of training before logger experiment is closed. </> on_train_epoch_end ( outputs ) \u2014 Called in the training loop at the very end of the epoch. </> on_train_epoch_start ( ) \u2014 Called in the training loop at the very beginning of the epoch. </> on_train_start ( ) \u2014 Called at the beginning of training before sanity check. </> on_validation_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called in the validation loop after the batch. </> on_validation_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called in the validation loop before anything happens for that batch. </> on_validation_epoch_end ( ) \u2014 Called in the validation loop at the very end of the epoch. </> on_validation_epoch_start ( ) \u2014 Called in the validation loop at the very beginning of the epoch. </> on_validation_model_eval ( ) \u2014 Sets the model to eval during the val loop </> on_validation_model_train ( ) \u2014 Sets the model to train during the val loop </> optimizer_step ( epoch , batch_idx , optimizer , optimizer_idx , optimizer_closure , on_tpu , using_native_amp , using_lbfgs ) \u2014 Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> prepare_data ( ) \u2014 Use this to download and prepare data. </> print ( *args , **kwargs ) \u2014 Prints only from process 0. Use this in any distributed mode to log only once. </> register_backward_hook ( hook ) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor ) \u2014 Adds a persistent buffer to the module. </> register_forward_hook ( hook ) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> save_hyperparameters ( *args , frame ) \u2014 Save all model arguments. </> setup ( stage ) \u2014 Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> tbptt_split_batch ( batch , split_size ) (list) \u2014 When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. </> teardown ( stage ) \u2014 Called at the end of fit and test. </> test_dataloader ( ) (Union(dataloader, list of dataloader)) \u2014 Implement one or multiple PyTorch DataLoaders for testing. </> test_epoch_end ( outputs ) \u2014 Called at the end of a test epoch with the output of all test steps. </> test_step ( *args , **kwargs ) \u2014 Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. </> test_step_end ( *args , **kwargs ) \u2014 Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> to_onnx ( file_path , input_sample , **kwargs ) \u2014 Saves the model in ONNX format </> to_torchscript ( file_path , method , example_inputs , **kwargs ) (Union(scriptmodule, dict(str: scriptmodule))) \u2014 By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has self.example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. </> toggle_optimizer ( optimizer , optimizer_idx ) \u2014 Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> train_dataloader ( ) (DataLoader) \u2014 Implement a PyTorch DataLoader for training. </> training_epoch_end ( outputs ) \u2014 Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs for every training_step. </> training_step ( *args , **kwargs ) \u2014 Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. </> training_step_end ( *args , **kwargs ) \u2014 Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. </> transfer_batch_to_device ( batch , device ) (any) \u2014 Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> unfreeze ( ) \u2014 Unfreeze all parameters for training. </> val_dataloader ( ) (Union(dataloader, list of dataloader)) \u2014 Implement one or multiple PyTorch DataLoaders for validation. </> validation_epoch_end ( outputs ) \u2014 Called at the end of the validation epoch with the outputs of all validation steps. </> validation_step ( *args , **kwargs ) \u2014 Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. </> validation_step_end ( *args , **kwargs ) \u2014 Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. </> zero_grad ( ) \u2014 Sets gradients of all model parameters to zero. </> method register_buffer ( name , tensor ) </> Adds a persistent buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the persistent state. Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method register_parameter ( name , param ) </> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method add_module ( name , module ) </> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method apply ( fn ) </> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters fn ( \u2014 class: Module -> None): function to be applied to each submodule Returns (Module) self Example:: >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . data . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method register_backward_hook ( hook ) </> Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. Returns class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. method register_forward_pre_hook ( hook ) </> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method register_forward_hook ( hook ) </> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method state_dict ( destination=None , prefix='' , keep_vars=False ) </> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method load_state_dict ( state_dict , strict=True ) </> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator parameters ( recurse=True ) </> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param . data ), param . size ()) < class ' torch . FloatTensor '> (20L,) < class ' torch . FloatTensor '> (20L, 1L, 5L, 5L) generator named_parameters ( prefix='' , recurse=True ) </> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator buffers ( recurse=True ) </> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf . data ), buf . size ()) < class ' torch . FloatTensor '> (20L,) < class ' torch . FloatTensor '> (20L, 1L, 5L, 5L) generator named_buffers ( prefix='' , recurse=True ) </> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator children ( ) </> Returns an iterator over immediate children modules. Yields (Module) a child module generator named_children ( ) </> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator modules ( ) </> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator named_modules ( memo=None , prefix='' ) </> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method train ( mode=True ) </> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method eval ( ) </> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method requires_grad_ ( requires_grad=True ) </> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method zero_grad ( ) </> Sets gradients of all model parameters to zero. method extra_repr ( ) </> Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. method setup ( stage ) </> Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters stage (str) \u2014 either 'fit' or 'test' Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) method teardown ( stage ) </> Called at the end of fit and test. Parameters stage (str) \u2014 either 'fit' or 'test' method on_fit_start ( ) </> Called at the very beginning of fit. If on DDP it is called on every process method on_fit_end ( ) </> Called at the very end of fit. If on DDP it is called on every process method on_train_start ( ) </> Called at the beginning of training before sanity check. method on_train_end ( ) </> Called at the end of training before logger experiment is closed. method on_pretrain_routine_start ( ) </> Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start method on_pretrain_routine_end ( ) </> Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start method on_train_batch_start ( batch , batch_idx , dataloader_idx ) </> Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters batch (any) \u2014 The batched data as it is returned by the training DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method on_train_batch_end ( outputs , batch , batch_idx , dataloader_idx ) </> Called in the training loop after the batch. Parameters outputs (any) \u2014 The outputs of training_step_end(training_step(x)) batch (any) \u2014 The batched data as it is returned by the training DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method on_validation_model_eval ( ) </> Sets the model to eval during the val loop method on_validation_model_train ( ) </> Sets the model to train during the val loop method on_validation_batch_start ( batch , batch_idx , dataloader_idx ) </> Called in the validation loop before anything happens for that batch. Parameters batch (any) \u2014 The batched data as it is returned by the validation DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method on_validation_batch_end ( outputs , batch , batch_idx , dataloader_idx ) </> Called in the validation loop after the batch. Parameters outputs (any) \u2014 The outputs of validation_step_end(validation_step(x)) batch (any) \u2014 The batched data as it is returned by the validation DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method on_test_batch_start ( batch , batch_idx , dataloader_idx ) </> Called in the test loop before anything happens for that batch. Parameters batch (any) \u2014 The batched data as it is returned by the test DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method on_test_batch_end ( outputs , batch , batch_idx , dataloader_idx ) </> Called in the test loop after the batch. Parameters outputs (any) \u2014 The outputs of test_step_end(test_step(x)) batch (any) \u2014 The batched data as it is returned by the test DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method on_test_model_eval ( ) </> Sets the model to eval during the test loop method on_test_model_train ( ) </> Sets the model to train during the test loop method on_epoch_start ( ) </> Called in the training loop at the very beginning of the epoch. method on_train_epoch_start ( ) </> Called in the training loop at the very beginning of the epoch. method on_train_epoch_end ( outputs ) </> Called in the training loop at the very end of the epoch. method on_validation_epoch_start ( ) </> Called in the validation loop at the very beginning of the epoch. method on_validation_epoch_end ( ) </> Called in the validation loop at the very end of the epoch. method on_test_epoch_start ( ) </> Called in the test loop at the very beginning of the epoch. method on_test_epoch_end ( ) </> Called in the test loop at the very end of the epoch. method on_before_zero_grad ( optimizer ) </> Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() Parameters optimizer (Optimizer) \u2014 The optimizer for which grads should be zeroed. method on_after_backward ( ) </> Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) method prepare_data ( ) </> Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() if ddp/tpu: init() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() method train_dataloader ( ) \u2192 DataLoader </> Implement a PyTorch DataLoader for training. Return: Single PyTorch :class: ~torch.utils.data.DataLoader . The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example .. code-block:: python def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader method test_dataloader ( ) \u2192 Union(dataloader, list of dataloader) </> Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: Single or multiple PyTorch DataLoaders. Example .. code-block:: python def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. method val_dataloader ( ) \u2192 Union(dataloader, list of dataloader) </> Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: Single or multiple PyTorch DataLoaders. Examples .. code-block:: python def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. method transfer_batch_to_device ( batch , device=None ) </> Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Parameters batch (any) \u2014 A batch of data that needs to be transferred to a new device. device (device, optional) \u2014 The target device as defined in PyTorch. Returns (any) A reference to the data on the new device. Note This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). Note This hook only runs on single GPU training (no data-parallel). If you need multi-GPU support for your custom batch objects, you need to define your custom :class: ~torch.nn.parallel.DistributedDataParallel or :class: ~pytorch_lightning.overrides.data_parallel.LightningDistributedDataParallel and override :meth: ~pytorch_lightning.core.lightning.LightningModule.configure_ddp . See Also :func: ~pytorch_lightning.utilities.apply_func.move_data_to_device :func: ~pytorch_lightning.utilities.apply_func.apply_to_collection classmethod load_from_checkpoint ( checkpoint_path , map_location=None , hparams_file=None , strict=True , **kwargs ) </> Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Parameters checkpoint_path (str or IO) \u2014 Path to checkpoint. This can also be a URL, or file-like object map_location (Union(dict(str: str), str, device, int, callable, nonetype), optional) \u2014 If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file (str, optional) \u2014 Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class: ~dict and passed into your :class: LightningModule for use. If your model's hparams argument is :class: ~argparse.Namespace and .yaml file has hierarchical structure, you need to refactor your model to treat hparams as :class: ~dict . strict (bool, optional) \u2014 Whether to strictly enforce that the keys in :attr: checkpoint_path match the keys returned by this module's state dict. Default: True . kwargs \u2014 Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example .. code-block:: python # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path: NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) method on_load_checkpoint ( checkpoint ) </> Do something with the checkpoint. Gives model a chance to load something before state_dict is restored. Parameters checkpoint (dict(str: any)) \u2014 A dictionary with variables from the checkpoint. method on_save_checkpoint ( checkpoint ) </> Give the model a chance to add something to the checkpoint. state_dict is already there. Parameters checkpoint (dict(str: any)) \u2014 A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. method on_hpc_save ( checkpoint ) </> Hook to do whatever you need right before Slurm manager saves the model. Parameters checkpoint (dict(str: any)) \u2014 A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. method on_hpc_load ( checkpoint ) </> Hook to do whatever you need right before Slurm manager loads the model. Parameters checkpoint (dict(str: any)) \u2014 A dictionary with variables from the checkpoint. method grad_norm ( norm_type ) \u2192 dict(str: float) </> Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Parameters norm_type (float, int, or str) \u2014 The type of the used p-norm, cast to float if necessary. Can be 'inf' for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. method to ( *args , **kwargs ) </> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note This method modifies the module in-place. Parameters device \u2014 the desired device of the parameters and buffers in this module dtype \u2014 the desired floating point type of the floating point parameters and buffers in this module tensor \u2014 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns (Module) self Example:: >>> class ExampleModule ( DeviceDtypeModuleMixin ): ... def __init__ ( self , weight : torch . Tensor ): ... super () . __init__ () ... self . register_buffer ( 'weight' , weight ) >>> _ = torch . manual_seed ( 0 ) >>> module = ExampleModule ( torch . rand ( 3 , 4 )) >>> module . weight #doctest: +ELLIPSIS tensor ([[ ... ]]) >>> module . to ( torch . double ) ExampleModule () >>> module . weight #doctest: +ELLIPSIS tensor ([[ ... ]], dtype = torch . float64 ) >>> cpu = torch . device ( 'cpu' ) >>> module . to ( cpu , dtype = torch . half , non_blocking = True ) ExampleModule () >>> module . weight #doctest: +ELLIPSIS tensor ([[ ... ]], dtype = torch . float16 ) >>> module . to ( cpu ) ExampleModule () >>> module . weight #doctest: +ELLIPSIS tensor ([[ ... ]], dtype = torch . float16 ) >>> module . device device ( type = 'cpu' ) >>> module . dtype torch . float16 method cuda ( device=None ) </> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method cpu ( ) </> Moves all model parameters and buffers to the CPU. Returns (Module) self method type ( dst_type ) </> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method float ( ) </> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method double ( ) </> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method half ( ) </> Casts all floating point parameters and buffers to half datatype. Returns (Module) self class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method print ( *args , **kwargs ) </> Prints only from process 0. Use this in any distributed mode to log only once. Parameters *args \u2014 The thing to print. Will be passed to Python's built-in print function. **kwargs \u2014 Will be passed to Python's built-in print function. Example n : ) method log ( name , value , prog_bar=False , logger=True , on_step=None , on_epoch=None , reduce_fx=<built-in method mean of type object at 0x7f7ebfbd7860> , tbptt_reduce_fx=<built-in method mean of type object at 0x7f7ebfbd7860> , tbptt_pad_token=0 , enable_graph=False , sync_dist=False , sync_dist_op='mean' , sync_dist_group=None ) </> Log a key, value Example:: self.log('train_loss', loss) The default behavior per hook is as follows .. csv-table:: * also applies to the test loop :header: \"LightningMoule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters name (str) \u2014 key name value (any) \u2014 value name prog_bar (bool, optional) \u2014 if True logs to the progress bar logger (bool, optional) \u2014 if True logs to the logger on_step (bool, optional) \u2014 if True logs at this step. None auto-logs at the training_step but not validation/test_step on_epoch (bool, optional) \u2014 if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step reduce_fx (callable, optional) \u2014 reduction function over step values for end of epoch. Torch.mean by default tbptt_reduce_fx (callable, optional) \u2014 function to reduce on truncated back prop tbptt_pad_token (int, optional) \u2014 token to use for padding enable_graph (bool, optional) \u2014 if True, will not auto detach the graph sync_dist (bool, optional) \u2014 if True, reduces the metric across GPUs/TPUs sync_dist_op (any or str, optional) \u2014 the op to sync across GPUs/TPUs sync_dist_group (any, optional) \u2014 the ddp group method log_dict ( dictionary , prog_bar=False , logger=True , on_step=None , on_epoch=None , reduce_fx=<built-in method mean of type object at 0x7f7ebfbd7860> , tbptt_reduce_fx=<built-in method mean of type object at 0x7f7ebfbd7860> , tbptt_pad_token=0 , enable_graph=False , sync_dist=False , sync_dist_op='mean' , sync_dist_group=None ) </> Log a dictonary of values at once Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters dictionary (dict) \u2014 key value pairs (str, tensors) prog_bar (bool, optional) \u2014 if True logs to the progress base logger (bool, optional) \u2014 if True logs to the logger on_step (bool, optional) \u2014 if True logs at this step. None auto-logs for training_step but not validation/test_step on_epoch (bool, optional) \u2014 if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step reduce_fx (callable, optional) \u2014 reduction function over step values for end of epoch. Torch.mean by default tbptt_reduce_fx (callable, optional) \u2014 function to reduce on truncated back prop tbptt_pad_token (int, optional) \u2014 token to use for padding enable_graph (bool, optional) \u2014 if True, will not auto detach the graph sync_dist (bool, optional) \u2014 if True, reduces the metric across GPUs/TPUs sync_dist_op (any or str, optional) \u2014 the op to sync across GPUs/TPUs sync_dist_group (any, optional) \u2014 the ddp group: method all_gather ( tensor , group=None , sync_grads=False ) </> Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes Parameters tensor (Tensor) \u2014 tensor of shape (batch, ...) group (any, optional) \u2014 the process group to gather results from. Defaults to all processes (world) sync_grads (bool, optional) \u2014 flag that allows users to synchronize gradients for all_gather op Return: A tensor of shape (world_size, batch, ...) method forward ( *args , **kwargs ) </> Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). Normally you'd call self() from your :meth: training_step method. This makes it easy to write a complex system for training with the outputs you'd want in a prediction setting. You may also find the :func: ~pytorch_lightning.core.decorators.auto_move_data decorator useful when using the module outside Lightning in a production setting. Parameters *args \u2014 Whatever you decide to pass into the forward method. **kwargs \u2014 Keyword arguments are also possible. Return: Predicted output Examples .. code-block:: python # example if we were using this model as a feature extractor def forward(self, x): feature_maps = self.convnet(x) return feature_maps def training_step(self, batch, batch_idx): x, y = batch feature_maps = self(x) logits = self.classifier(feature_maps) # ... return loss # splitting it this way allows model to be used a feature extractor model = MyModelAbove() inputs = server.get_request() results = model(inputs) server.write_results(results) # ------------- # This is in stark contrast to torch.nn.Module where normally you would have this: def forward(self, batch): x, y = batch feature_maps = self.convnet(x) logits = self.classifier(feature_maps) return logits method training_step ( *args , **kwargs ) </> Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters batch ( \u2014 class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int) \u2014 Integer displaying index of this batch optimizer_idx (int) \u2014 When using multiple optimizers, this argument will also be present. hiddens( \u2014 class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - `dict` - A dictionary. Can include any keys, but must include the key 'loss' - `None` - Training will skip to the next batch In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder if optimizer_idx == 1: # do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step ... out, hiddens = self.lstm(data, hiddens) ... return {'loss': loss, 'hiddens': hiddens} Note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. method training_step_end ( *args , **kwargs ) </> Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters batch_parts_outputs \u2014 What you return in training_step for each batch part. Return: Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denomintaor loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {'pred': out} def training_step_end(self, training_step_outputs): gpu_0_pred = training_step_outputs[0]['pred'] gpu_1_pred = training_step_outputs[1]['pred'] gpu_n_pred = training_step_outputs[n]['pred'] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also See the :ref: multi_gpu guide for more details. method training_epoch_end ( outputs ) </> Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs for every training_step. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters outputs (list of any) \u2014 List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note If this method is not overridden, this won't be called. Example:: def training_epoch_end(self, training_step_outputs): # do something with all training_step outputs return result With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, training_step_outputs): for out in training_step_outputs: # do something here method validation_step ( *args , **kwargs ) </> Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters batch ( \u2014 class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int) \u2014 The index of this batch dataloader_idx (int) \u2014 The index of the dataloader that produced this batch (only if multiple val datasets used) Return: Any of. - Any object or value - `None` - Validation will skip to the next batch .. code-block:: python # pseudocode of order out = validation_step() if defined('validation_step_end'): out = validation_step_end(out) out = validation_epoch_end(out) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx) # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples .. code-block:: python # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val datasets, validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation datasets def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. Note If you don't need to validate you don't need to implement this method. Note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. method validation_step_end ( *args , **kwargs ) </> Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Parameters batch_parts_outputs \u2014 What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log('val_loss', loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs: # do something with these See Also See the :ref: multi_gpu guide for more details. method validation_epoch_end ( outputs ) </> Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters outputs (list of any) \u2014 List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note If you didn't define a :meth: validation_step , this won't be called. Examples With a single dataloader: .. code-block:: python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs: # do something With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader. .. code-block:: python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value) method test_step ( *args , **kwargs ) </> Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters batch ( \u2014 class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int) \u2014 The index of this batch. dataloader_idx (int) \u2014 The index of the dataloader that produced this batch (only if multiple test datasets used). Return: Any of. - Any object or value - `None` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx) # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx) Examples .. code-block:: python # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple validation datasets, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test datasets def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. Note If you don't need to validate you don't need to implement this method. Note When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. method test_step_end ( *args , **kwargs ) </> Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Parameters batch_parts_outputs \u2014 What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log('test_loss', loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_epoch_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log('test_loss', loss) See Also See the :ref: multi_gpu guide for more details. method test_epoch_end ( outputs ) </> Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters outputs (list of any) \u2014 List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: None Note If you didn't define a :meth: test_step , this won't be called. Examples With a single dataloader: .. code-block:: python def test_epoch_end(self, outputs): # do something with the outputs of all test batches all_test_preds = test_step_outputs.predictions some_result = calc_all_results(all_test_preds) self.log(some_result) With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each test step for that dataloader. .. code-block:: python def test_epoch_end(self, outputs): final_value = 0 for dataloader_outputs in outputs: for test_step_out in dataloader_outputs: # do something final_value += test_step_out self.log('final_metric', final_value) method manual_backward ( loss , optimizer , *args , **kwargs ) </> Call this directly from your training_step when doing optimizations manually. By using this we can ensure that all the proper scaling when using 16-bit etc has been done for you This function forwards all args to the .backward() call as well. .. tip:: In manual mode we still automatically clip grads if Trainer(gradient_clip_val=x) is set .. tip:: In manual mode we still automatically accumulate grad over batches if Trainer(accumulate_grad_batches=x) is set and you use optimizer.step() Example:: def training_step(...): (opt_a, opt_b) = self.optimizers() loss = ... # automatically applies scaling, etc... self.manual_backward(loss, opt_a) opt_a.step() method backward ( loss , optimizer , optimizer_idx , *args , **kwargs ) </> Override backward with your own implementation if you need to. Parameters loss (Tensor) \u2014 Loss is already scaled by accumulated grads optimizer (Optimizer) \u2014 Current optimizer being used optimizer_idx (int) \u2014 Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, loss, optimizer, optimizer_idx): loss.backward() method toggle_optimizer ( optimizer , optimizer_idx ) </> Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. .. note:: Only called when using multiple optimizers Override for your own behavior method optimizer_step ( epoch=None , batch_idx=None , optimizer=None , optimizer_idx=None , optimizer_closure=None , on_tpu=None , using_native_amp=None , using_lbfgs=None ) </> Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. .. tip:: With Trainer(enable_pl_optimizer=True) , you can user optimizer.step() directly and it will handle zero_grad, accumulated gradients, AMP, TPU and more automatically for you. Warning If you are overriding this method, make sure that you pass the optimizer_closure parameter to optimizer.step() function as shown in the examples. This ensures that train_step_and_backward_closure is called within :meth: ~pytorch_lightning.trainer.training_loop.TrainLoop.run_training_batch . Parameters epoch (int, optional) \u2014 Current epoch batch_idx (int, optional) \u2014 Index of current batch optimizer (Optimizer, optional) \u2014 A PyTorch optimizer optimizer_idx (int, optional) \u2014 If you used multiple optimizers this indexes into that list. optimizer_closure (callable, optional) \u2014 closure for all optimizers on_tpu (bool, optional) \u2014 true if TPU backward is required using_native_amp (bool, optional) \u2014 True if using native amp using_lbfgs (bool, optional) \u2014 True if the matching optimizer is lbfgs Examples .. code-block:: python # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every 2 steps if optimizer_idx == 0: if batch_idx % 2 == 0 : optimizer.step(closure=optimizer_closure) optimizer.zero_grad() # update discriminator opt every 4 steps if optimizer_idx == 1: if batch_idx % 4 == 0 : optimizer.step(closure=optimizer_closure) optimizer.zero_grad() # ... # add as many optimizers as you want s : n p , : r : ) : e s ) ) method tbptt_split_batch ( batch , split_size ) \u2192 list </> When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Parameters batch (Tensor) \u2014 Current batch split_size (int) \u2014 The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples .. code-block:: python def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.trainer.Trainer.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . method freeze ( ) </> Freeze all params for inference. Example .. code-block:: python model = MyLightningModule(...) model.freeze() method unfreeze ( ) </> Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() method get_progress_bar_dict ( ) \u2192 dict(str: int or str) </> Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. .. code-block:: Epoch 1: 4%|\u258e | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10] Here is an example how to override the defaults: .. code-block:: python def get_progress_bar_dict(self): # don't show the version number items = super().get_progress_bar_dict() items.pop(\"v_num\", None) return items Return: Dictionary with the items to be displayed in the progress bar. method save_hyperparameters ( *args , frame=None ) </> Save all model arguments. Parameters args \u2014 single object of dict , NameSpace or OmegaConf or string names or argumenst from class __init__ >>> from collections import OrderedDict >>> class ManuallyArgsModel ( LightningModule ): ... def __init__ ( self , arg1 , arg2 , arg3 ): ... super () . __init__ () ... # manually assign arguments ... self . save_hyperparameters ( 'arg1' , 'arg3' ) ... def forward ( self , * args , ** kwargs ): ... ... >>> model = ManuallyArgsModel ( 1 , 'abc' , 3.14 ) >>> model . hparams \"arg1\" : 1 \"arg3\" : 3.14 >>> class AutomaticArgsModel ( LightningModule ): ... def __init__ ( self , arg1 , arg2 , arg3 ): ... super () . __init__ () ... # equivalent automatic ... self . save_hyperparameters () ... def forward ( self , * args , ** kwargs ): ... ... >>> model = AutomaticArgsModel ( 1 , 'abc' , 3.14 ) >>> model . hparams \"arg1\" : 1 \"arg2\" : abc \"arg3\" : 3.14 >>> class SingleArgModel ( LightningModule ): ... def __init__ ( self , params ): ... super () . __init__ () ... # manually assign single argument ... self . save_hyperparameters ( params ) ... def forward ( self , * args , ** kwargs ): ... ... >>> model = SingleArgModel ( Namespace ( p1 = 1 , p2 = 'abc' , p3 = 3.14 )) >>> model . hparams \"p1\" : 1 \"p2\" : abc \"p3\" : 3.14 method to_onnx ( file_path , input_sample=None , **kwargs ) </> Saves the model in ONNX format Parameters file_path (str or Path) \u2014 The path of the file the onnx model should be saved to. input_sample (any, optional) \u2014 An input for tracing. Default: None (Use self.example_input_array) **kwargs \u2014 Will be passed to torch.onnx.export function. Example >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) >>> with tempfile . NamedTemporaryFile ( suffix = '.onnx' , delete = False ) as tmpfile : ... model = SimpleModel () ... input_sample = torch . randn (( 1 , 64 )) ... model . to_onnx ( tmpfile . name , input_sample , export_params = True ) ... os . path . isfile ( tmpfile . name ) True method to_torchscript ( file_path=None , method='script' , example_inputs=None , **kwargs ) \u2192 Union(scriptmodule, dict(str: scriptmodule)) </> By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has self.example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Parameters file_path (str, Path, or NoneType, optional) \u2014 Path where to save the torchscript. Default: None (no file saved). method (str, optional) \u2014 Whether to use TorchScript's script or trace method. Default: 'script' example_inputs (any, optional) \u2014 An input to be used to do tracing when method is set to 'trace'. Default: None (Use self.example_input_array) **kwargs \u2014 Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. The exported script will be set to evaluation mode. It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) ... >>> model = SimpleModel () >>> torch . jit . save ( model . to_torchscript (), \"model.pt\" ) # doctest: +SKIP >>> os . path . isfile ( \"model.pt\" ) # doctest: +SKIP >>> torch . jit . save ( model . to_torchscript ( file_path = \"model_trace.pt\" , method = 'trace' , # doctest: +SKIP ... example_inputs = torch . randn ( 1 , 64 ))) # doctest: +SKIP >>> os . path . isfile ( \"model_trace.pt\" ) # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not. method on_epoch_end ( ) </> Keep the epoch progress bar This is not documented but working. method loss_function ( logits , labels ) </> Calculate the loss method configure_optimizers ( ) </> Configure the optimizers","title":"plkit.module"},{"location":"api/plkit.module/#plkitmodule","text":"</> The core base module class based on pytorch_lightning.LightningModule Classes Module \u2014 The Module class </> class","title":"plkit.module"},{"location":"api/plkit.module/#plkitmodulemodule","text":"</> Bases pytorch_lightning.core.lightning.LightningModule abc.ABC pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.grads.GradInformation pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module The Module class on_epoch_end is added to print a newline to keep the progress bar and the stats on it for each epoch. If you don't want this, just overwrite it with: >>> def on_epoch_end ( self ): >>> pass If you have other stuff to do in on_epoch_end , make sure to you call: >>> super () . on_epoch_end () You may or may not need to write loss_function , as it will be inferred from config item loss and num_classes . Basically, MSELoss will be used for regression and CrossEntropyLoss for classification. measure added for convinience to get some metrics between logits and targets. Parameters config \u2014 The configuration dictionary Attributes _loss_func \u2014 The loss function automatic_optimization (bool) \u2014 If False you are responsible for calling .backward, .step, zero_grad. </> config \u2014 The configs current_epoch (int) \u2014 The current epoch </> global_step (int) \u2014 Total training batches seen across all epochs </> num_classes \u2014 Number of classes to predict. 1 for regression on_gpu \u2014 True if your model is currently running on GPUs. Useful to set flags around the LightningModule for different CPU vs GPU behavior. </> optim \u2014 The optimizer name. currently only adam and sgd are supported. With this, of course you can, but you don't need to write configure_optimizers . Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods add_module ( name , module ) \u2014 Adds a child module to the current module. </> all_gather ( tensor , group , sync_grads ) \u2014 Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. </> apply ( fn ) (Module) \u2014 Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). </> backward ( loss , optimizer , optimizer_idx , *args , **kwargs ) \u2014 Override backward with your own implementation if you need to. </> buffers ( recurse ) (torch.Tensor) \u2014 Returns an iterator over module buffers. </> children ( ) (Module) \u2014 Returns an iterator over immediate children modules. </> configure_optimizers ( ) \u2014 Configure the optimizers </> cpu ( ) (Module) \u2014 Moves all model parameters and buffers to the CPU. </> cuda ( device ) (Module) \u2014 Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. </> double ( ) (Module) \u2014 Casts all floating point parameters and buffers to double datatype. </> eval ( ) (Module) \u2014 Sets the module in evaluation mode. </> extra_repr ( ) \u2014 Set the extra representation of the module </> float ( ) (Module) \u2014 Casts all floating point parameters and buffers to float datatype. </> forward ( *args , **kwargs ) \u2014 Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). </> freeze ( ) \u2014 Freeze all params for inference. </> get_progress_bar_dict ( ) (dict(str: int or str)) \u2014 Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. </> grad_norm ( norm_type ) (dict(str: float)) \u2014 Compute each parameter's gradient's norm and their overall norm. </> half ( ) (Module) \u2014 Casts all floating point parameters and buffers to half datatype. </> load_from_checkpoint ( checkpoint_path , map_location , hparams_file , strict , **kwargs ) \u2014 Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters </> load_state_dict ( state_dict , strict ) (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) \u2014 Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. </> log ( name , value , prog_bar , logger , on_step , on_epoch , reduce_fx , tbptt_reduce_fx , tbptt_pad_token , enable_graph , sync_dist , sync_dist_op , sync_dist_group ) \u2014 Log a key, value </> log_dict ( dictionary , prog_bar , logger , on_step , on_epoch , reduce_fx , tbptt_reduce_fx , tbptt_pad_token , enable_graph , sync_dist , sync_dist_op , sync_dist_group ) \u2014 Log a dictonary of values at once </> loss_function ( logits , labels ) \u2014 Calculate the loss </> manual_backward ( loss , optimizer , *args , **kwargs ) \u2014 Call this directly from your training_step when doing optimizations manually. By using this we can ensure that all the proper scaling when using 16-bit etc has been done for you </> modules ( ) (Module) \u2014 Returns an iterator over all modules in the network. </> named_buffers ( prefix , recurse ) (string, torch.Tensor) \u2014 Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. </> named_children ( ) (string, Module) \u2014 Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. </> named_modules ( memo , prefix ) (string, Module) \u2014 Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. </> named_parameters ( prefix , recurse ) (string, Parameter) \u2014 Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. </> on_after_backward ( ) \u2014 Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. </> on_before_zero_grad ( optimizer ) \u2014 Called after optimizer.step() and before optimizer.zero_grad(). </> on_epoch_end ( ) \u2014 Keep the epoch progress bar This is not documented but working. </> on_epoch_start ( ) \u2014 Called in the training loop at the very beginning of the epoch. </> on_fit_end ( ) \u2014 Called at the very end of fit. If on DDP it is called on every process </> on_fit_start ( ) \u2014 Called at the very beginning of fit. If on DDP it is called on every process </> on_hpc_load ( checkpoint ) \u2014 Hook to do whatever you need right before Slurm manager loads the model. </> on_hpc_save ( checkpoint ) \u2014 Hook to do whatever you need right before Slurm manager saves the model. </> on_load_checkpoint ( checkpoint ) \u2014 Do something with the checkpoint. Gives model a chance to load something before state_dict is restored. </> on_pretrain_routine_end ( ) \u2014 Called at the end of the pretrain routine (between fit and train start). </> on_pretrain_routine_start ( ) \u2014 Called at the beginning of the pretrain routine (between fit and train start). </> on_save_checkpoint ( checkpoint ) \u2014 Give the model a chance to add something to the checkpoint. state_dict is already there. </> on_test_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called in the test loop after the batch. </> on_test_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called in the test loop before anything happens for that batch. </> on_test_epoch_end ( ) \u2014 Called in the test loop at the very end of the epoch. </> on_test_epoch_start ( ) \u2014 Called in the test loop at the very beginning of the epoch. </> on_test_model_eval ( ) \u2014 Sets the model to eval during the test loop </> on_test_model_train ( ) \u2014 Sets the model to train during the test loop </> on_train_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called in the training loop after the batch. </> on_train_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called in the training loop before anything happens for that batch. </> on_train_end ( ) \u2014 Called at the end of training before logger experiment is closed. </> on_train_epoch_end ( outputs ) \u2014 Called in the training loop at the very end of the epoch. </> on_train_epoch_start ( ) \u2014 Called in the training loop at the very beginning of the epoch. </> on_train_start ( ) \u2014 Called at the beginning of training before sanity check. </> on_validation_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called in the validation loop after the batch. </> on_validation_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called in the validation loop before anything happens for that batch. </> on_validation_epoch_end ( ) \u2014 Called in the validation loop at the very end of the epoch. </> on_validation_epoch_start ( ) \u2014 Called in the validation loop at the very beginning of the epoch. </> on_validation_model_eval ( ) \u2014 Sets the model to eval during the val loop </> on_validation_model_train ( ) \u2014 Sets the model to train during the val loop </> optimizer_step ( epoch , batch_idx , optimizer , optimizer_idx , optimizer_closure , on_tpu , using_native_amp , using_lbfgs ) \u2014 Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. </> parameters ( recurse ) (Parameter) \u2014 Returns an iterator over module parameters. </> prepare_data ( ) \u2014 Use this to download and prepare data. </> print ( *args , **kwargs ) \u2014 Prints only from process 0. Use this in any distributed mode to log only once. </> register_backward_hook ( hook ) \u2014 Registers a backward hook on the module. </> register_buffer ( name , tensor ) \u2014 Adds a persistent buffer to the module. </> register_forward_hook ( hook ) \u2014 Registers a forward hook on the module. </> register_forward_pre_hook ( hook ) \u2014 Registers a forward pre-hook on the module. </> register_parameter ( name , param ) \u2014 Adds a parameter to the module. </> requires_grad_ ( requires_grad ) (Module) \u2014 Change if autograd should record operations on parameters in this module. </> save_hyperparameters ( *args , frame ) \u2014 Save all model arguments. </> setup ( stage ) \u2014 Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. </> state_dict ( destination , prefix , keep_vars ) (dict) \u2014 Returns a dictionary containing a whole state of the module. </> tbptt_split_batch ( batch , split_size ) (list) \u2014 When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. </> teardown ( stage ) \u2014 Called at the end of fit and test. </> test_dataloader ( ) (Union(dataloader, list of dataloader)) \u2014 Implement one or multiple PyTorch DataLoaders for testing. </> test_epoch_end ( outputs ) \u2014 Called at the end of a test epoch with the output of all test steps. </> test_step ( *args , **kwargs ) \u2014 Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. </> test_step_end ( *args , **kwargs ) \u2014 Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. </> to ( *args , **kwargs ) (Module) \u2014 Moves and/or casts the parameters and buffers. </> to_onnx ( file_path , input_sample , **kwargs ) \u2014 Saves the model in ONNX format </> to_torchscript ( file_path , method , example_inputs , **kwargs ) (Union(scriptmodule, dict(str: scriptmodule))) \u2014 By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has self.example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. </> toggle_optimizer ( optimizer , optimizer_idx ) \u2014 Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. </> train ( mode ) (Module) \u2014 Sets the module in training mode. </> train_dataloader ( ) (DataLoader) \u2014 Implement a PyTorch DataLoader for training. </> training_epoch_end ( outputs ) \u2014 Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs for every training_step. </> training_step ( *args , **kwargs ) \u2014 Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. </> training_step_end ( *args , **kwargs ) \u2014 Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. </> transfer_batch_to_device ( batch , device ) (any) \u2014 Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. </> type ( dst_type ) (Module) \u2014 Casts all parameters and buffers to :attr: dst_type . </> unfreeze ( ) \u2014 Unfreeze all parameters for training. </> val_dataloader ( ) (Union(dataloader, list of dataloader)) \u2014 Implement one or multiple PyTorch DataLoaders for validation. </> validation_epoch_end ( outputs ) \u2014 Called at the end of the validation epoch with the outputs of all validation steps. </> validation_step ( *args , **kwargs ) \u2014 Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. </> validation_step_end ( *args , **kwargs ) \u2014 Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. </> zero_grad ( ) \u2014 Sets gradients of all model parameters to zero. </> method","title":"plkit.module.Module"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleregister_buffer","text":"</> Adds a persistent buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the persistent state. Buffers can be accessed as attributes using given names. Parameters name (string) \u2014 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2014 buffer to be registered. Example:: >>> self . register_buffer ( 'running_mean' , torch . zeros ( num_features )) method","title":"torch.nn.modules.module.Module.register_buffer"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleregister_parameter","text":"</> Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name (string) \u2014 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2014 parameter to be added to the module. method","title":"torch.nn.modules.module.Module.register_parameter"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleadd_module","text":"</> Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name (string) \u2014 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2014 child module to be added to the module. method","title":"torch.nn.modules.module.Module.add_module"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleapply","text":"</> Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters fn ( \u2014 class: Module -> None): function to be applied to each submodule Returns (Module) self Example:: >>> def init_weights ( m ): >>> print ( m ) >>> if type ( m ) == nn . Linear : >>> m . weight . data . fill_ ( 1.0 ) >>> print ( m . weight ) >>> net = nn . Sequential ( nn . Linear ( 2 , 2 ), nn . Linear ( 2 , 2 )) >>> net . apply ( init_weights ) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Linear ( in_features = 2 , out_features = 2 , bias = True ) Parameter containing : tensor ([[ 1. , 1. ], [ 1. , 1. ]]) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) method","title":"torch.nn.modules.module.Module.apply"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleregister_backward_hook","text":"</> Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. Returns class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. method","title":"torch.nn.modules.module.Module.register_backward_hook"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleregister_forward_pre_hook","text":"</> Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_pre_hook"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleregister_forward_hook","text":"</> Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() method","title":"torch.nn.modules.module.Module.register_forward_hook"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulestate_dict","text":"</> Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns (dict) e Example:: >>> module . state_dict () . keys () [ 'bias' , 'weight' ] method","title":"torch.nn.modules.module.Module.state_dict"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleload_state_dict","text":"</> Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters state_dict (dict) \u2014 a dict containing parameters and persistent buffers. strict (bool, optional) \u2014 whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns (``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields) s s generator","title":"torch.nn.modules.module.Module.load_state_dict"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleparameters","text":"</> Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (Parameter) module parameter Example:: >>> for param in model . parameters (): >>> print ( type ( param . data ), param . size ()) < class ' torch . FloatTensor '> (20L,) < class ' torch . FloatTensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.parameters"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulenamed_parameters","text":"</> Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix (str) \u2014 prefix to prepend to all parameter names. recurse (bool) \u2014 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields (string, Parameter) Tuple containing the name and parameter Example:: >>> for name , param in self . named_parameters (): >>> if name in [ 'bias' ]: >>> print ( param . size ()) generator","title":"torch.nn.modules.module.Module.named_parameters"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulebuffers","text":"</> Returns an iterator over module buffers. Parameters recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (torch.Tensor) module buffer Example:: >>> for buf in model . buffers (): >>> print ( type ( buf . data ), buf . size ()) < class ' torch . FloatTensor '> (20L,) < class ' torch . FloatTensor '> (20L, 1L, 5L, 5L) generator","title":"torch.nn.modules.module.Module.buffers"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulenamed_buffers","text":"</> Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix (str) \u2014 prefix to prepend to all buffer names. recurse (bool) \u2014 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields (string, torch.Tensor) Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) generator","title":"torch.nn.modules.module.Module.named_buffers"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulechildren","text":"</> Returns an iterator over immediate children modules. Yields (Module) a child module generator","title":"torch.nn.modules.module.Module.children"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulenamed_children","text":"</> Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple containing a name and child module Example:: >>> for name , module in model . named_children (): >>> if name in [ 'conv4' , 'conv5' ]: >>> print ( module ) generator","title":"torch.nn.modules.module.Module.named_children"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulemodules","text":"</> Returns an iterator over all modules in the network. Yields (Module) a module in the network Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) generator","title":"torch.nn.modules.module.Module.modules"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulenamed_modules","text":"</> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields (string, Module) Tuple of name and module Note Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) method","title":"torch.nn.modules.module.Module.named_modules"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduletrain","text":"</> Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters mode (bool) \u2014 whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.train"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleeval","text":"</> Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns (Module) self method","title":"torch.nn.modules.module.Module.eval"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulerequires_grad_","text":"</> Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Parameters requires_grad (bool) \u2014 whether autograd should record operations on parameters in this module. Default: True . Returns (Module) self method","title":"torch.nn.modules.module.Module.requires_grad_"},{"location":"api/plkit.module/#torchnnmodulesmodulemodulezero_grad","text":"</> Sets gradients of all model parameters to zero. method","title":"torch.nn.modules.module.Module.zero_grad"},{"location":"api/plkit.module/#torchnnmodulesmodulemoduleextra_repr","text":"</> Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. method","title":"torch.nn.modules.module.Module.extra_repr"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookssetup","text":"</> Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters stage (str) \u2014 either 'fit' or 'test' Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) method","title":"pytorch_lightning.core.hooks.ModelHooks.setup"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhooksteardown","text":"</> Called at the end of fit and test. Parameters stage (str) \u2014 either 'fit' or 'test' method","title":"pytorch_lightning.core.hooks.ModelHooks.teardown"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_fit_start","text":"</> Called at the very beginning of fit. If on DDP it is called on every process method","title":"pytorch_lightning.core.hooks.ModelHooks.on_fit_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_fit_end","text":"</> Called at the very end of fit. If on DDP it is called on every process method","title":"pytorch_lightning.core.hooks.ModelHooks.on_fit_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_train_start","text":"</> Called at the beginning of training before sanity check. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_train_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_train_end","text":"</> Called at the end of training before logger experiment is closed. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_train_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_pretrain_routine_start","text":"</> Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start method","title":"pytorch_lightning.core.hooks.ModelHooks.on_pretrain_routine_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_pretrain_routine_end","text":"</> Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start method","title":"pytorch_lightning.core.hooks.ModelHooks.on_pretrain_routine_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_train_batch_start","text":"</> Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters batch (any) \u2014 The batched data as it is returned by the training DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method","title":"pytorch_lightning.core.hooks.ModelHooks.on_train_batch_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_train_batch_end","text":"</> Called in the training loop after the batch. Parameters outputs (any) \u2014 The outputs of training_step_end(training_step(x)) batch (any) \u2014 The batched data as it is returned by the training DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method","title":"pytorch_lightning.core.hooks.ModelHooks.on_train_batch_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_validation_model_eval","text":"</> Sets the model to eval during the val loop method","title":"pytorch_lightning.core.hooks.ModelHooks.on_validation_model_eval"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_validation_model_train","text":"</> Sets the model to train during the val loop method","title":"pytorch_lightning.core.hooks.ModelHooks.on_validation_model_train"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_validation_batch_start","text":"</> Called in the validation loop before anything happens for that batch. Parameters batch (any) \u2014 The batched data as it is returned by the validation DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method","title":"pytorch_lightning.core.hooks.ModelHooks.on_validation_batch_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_validation_batch_end","text":"</> Called in the validation loop after the batch. Parameters outputs (any) \u2014 The outputs of validation_step_end(validation_step(x)) batch (any) \u2014 The batched data as it is returned by the validation DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method","title":"pytorch_lightning.core.hooks.ModelHooks.on_validation_batch_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_test_batch_start","text":"</> Called in the test loop before anything happens for that batch. Parameters batch (any) \u2014 The batched data as it is returned by the test DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method","title":"pytorch_lightning.core.hooks.ModelHooks.on_test_batch_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_test_batch_end","text":"</> Called in the test loop after the batch. Parameters outputs (any) \u2014 The outputs of test_step_end(test_step(x)) batch (any) \u2014 The batched data as it is returned by the test DataLoader. batch_idx (int) \u2014 the index of the batch dataloader_idx (int) \u2014 the index of the dataloader method","title":"pytorch_lightning.core.hooks.ModelHooks.on_test_batch_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_test_model_eval","text":"</> Sets the model to eval during the test loop method","title":"pytorch_lightning.core.hooks.ModelHooks.on_test_model_eval"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_test_model_train","text":"</> Sets the model to train during the test loop method","title":"pytorch_lightning.core.hooks.ModelHooks.on_test_model_train"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_epoch_start","text":"</> Called in the training loop at the very beginning of the epoch. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_epoch_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_train_epoch_start","text":"</> Called in the training loop at the very beginning of the epoch. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_train_epoch_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_train_epoch_end","text":"</> Called in the training loop at the very end of the epoch. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_train_epoch_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_validation_epoch_start","text":"</> Called in the validation loop at the very beginning of the epoch. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_validation_epoch_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_validation_epoch_end","text":"</> Called in the validation loop at the very end of the epoch. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_validation_epoch_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_test_epoch_start","text":"</> Called in the test loop at the very beginning of the epoch. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_test_epoch_start"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_test_epoch_end","text":"</> Called in the test loop at the very end of the epoch. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_test_epoch_end"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_before_zero_grad","text":"</> Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() Parameters optimizer (Optimizer) \u2014 The optimizer for which grads should be zeroed. method","title":"pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad"},{"location":"api/plkit.module/#pytorch_lightningcorehooksmodelhookson_after_backward","text":"</> Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) method","title":"pytorch_lightning.core.hooks.ModelHooks.on_after_backward"},{"location":"api/plkit.module/#pytorch_lightningcorehooksdatahooksprepare_data","text":"</> Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() if ddp/tpu: init() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() method","title":"pytorch_lightning.core.hooks.DataHooks.prepare_data"},{"location":"api/plkit.module/#pytorch_lightningcorehooksdatahookstrain_dataloader","text":"</> Implement a PyTorch DataLoader for training. Return: Single PyTorch :class: ~torch.utils.data.DataLoader . The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example .. code-block:: python def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader method","title":"pytorch_lightning.core.hooks.DataHooks.train_dataloader"},{"location":"api/plkit.module/#pytorch_lightningcorehooksdatahookstest_dataloader","text":"</> Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: Single or multiple PyTorch DataLoaders. Example .. code-block:: python def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. method","title":"pytorch_lightning.core.hooks.DataHooks.test_dataloader"},{"location":"api/plkit.module/#pytorch_lightningcorehooksdatahooksval_dataloader","text":"</> Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: Single or multiple PyTorch DataLoaders. Examples .. code-block:: python def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. method","title":"pytorch_lightning.core.hooks.DataHooks.val_dataloader"},{"location":"api/plkit.module/#pytorch_lightningcorehooksdatahookstransfer_batch_to_device","text":"</> Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Parameters batch (any) \u2014 A batch of data that needs to be transferred to a new device. device (device, optional) \u2014 The target device as defined in PyTorch. Returns (any) A reference to the data on the new device. Note This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). Note This hook only runs on single GPU training (no data-parallel). If you need multi-GPU support for your custom batch objects, you need to define your custom :class: ~torch.nn.parallel.DistributedDataParallel or :class: ~pytorch_lightning.overrides.data_parallel.LightningDistributedDataParallel and override :meth: ~pytorch_lightning.core.lightning.LightningModule.configure_ddp . See Also :func: ~pytorch_lightning.utilities.apply_func.move_data_to_device :func: ~pytorch_lightning.utilities.apply_func.apply_to_collection classmethod","title":"pytorch_lightning.core.hooks.DataHooks.transfer_batch_to_device"},{"location":"api/plkit.module/#pytorch_lightningcoresavingmodelioload_from_checkpoint","text":"</> Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Parameters checkpoint_path (str or IO) \u2014 Path to checkpoint. This can also be a URL, or file-like object map_location (Union(dict(str: str), str, device, int, callable, nonetype), optional) \u2014 If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file (str, optional) \u2014 Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class: ~dict and passed into your :class: LightningModule for use. If your model's hparams argument is :class: ~argparse.Namespace and .yaml file has hierarchical structure, you need to refactor your model to treat hparams as :class: ~dict . strict (bool, optional) \u2014 Whether to strictly enforce that the keys in :attr: checkpoint_path match the keys returned by this module's state dict. Default: True . kwargs \u2014 Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example .. code-block:: python # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path: NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) method","title":"pytorch_lightning.core.saving.ModelIO.load_from_checkpoint"},{"location":"api/plkit.module/#pytorch_lightningcoresavingmodelioon_load_checkpoint","text":"</> Do something with the checkpoint. Gives model a chance to load something before state_dict is restored. Parameters checkpoint (dict(str: any)) \u2014 A dictionary with variables from the checkpoint. method","title":"pytorch_lightning.core.saving.ModelIO.on_load_checkpoint"},{"location":"api/plkit.module/#pytorch_lightningcoresavingmodelioon_save_checkpoint","text":"</> Give the model a chance to add something to the checkpoint. state_dict is already there. Parameters checkpoint (dict(str: any)) \u2014 A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. method","title":"pytorch_lightning.core.saving.ModelIO.on_save_checkpoint"},{"location":"api/plkit.module/#pytorch_lightningcoresavingmodelioon_hpc_save","text":"</> Hook to do whatever you need right before Slurm manager saves the model. Parameters checkpoint (dict(str: any)) \u2014 A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. method","title":"pytorch_lightning.core.saving.ModelIO.on_hpc_save"},{"location":"api/plkit.module/#pytorch_lightningcoresavingmodelioon_hpc_load","text":"</> Hook to do whatever you need right before Slurm manager loads the model. Parameters checkpoint (dict(str: any)) \u2014 A dictionary with variables from the checkpoint. method","title":"pytorch_lightning.core.saving.ModelIO.on_hpc_load"},{"location":"api/plkit.module/#pytorch_lightningcoregradsgradinformationgrad_norm","text":"</> Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Parameters norm_type (float, int, or str) \u2014 The type of the used p-norm, cast to float if necessary. Can be 'inf' for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. method","title":"pytorch_lightning.core.grads.GradInformation.grad_norm"},{"location":"api/plkit.module/#pytorch_lightningutilitiesdevice_dtype_mixindevicedtypemodulemixinto","text":"</> Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note This method modifies the module in-place. Parameters device \u2014 the desired device of the parameters and buffers in this module dtype \u2014 the desired floating point type of the floating point parameters and buffers in this module tensor \u2014 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns (Module) self Example:: >>> class ExampleModule ( DeviceDtypeModuleMixin ): ... def __init__ ( self , weight : torch . Tensor ): ... super () . __init__ () ... self . register_buffer ( 'weight' , weight ) >>> _ = torch . manual_seed ( 0 ) >>> module = ExampleModule ( torch . rand ( 3 , 4 )) >>> module . weight #doctest: +ELLIPSIS tensor ([[ ... ]]) >>> module . to ( torch . double ) ExampleModule () >>> module . weight #doctest: +ELLIPSIS tensor ([[ ... ]], dtype = torch . float64 ) >>> cpu = torch . device ( 'cpu' ) >>> module . to ( cpu , dtype = torch . half , non_blocking = True ) ExampleModule () >>> module . weight #doctest: +ELLIPSIS tensor ([[ ... ]], dtype = torch . float16 ) >>> module . to ( cpu ) ExampleModule () >>> module . weight #doctest: +ELLIPSIS tensor ([[ ... ]], dtype = torch . float16 ) >>> module . device device ( type = 'cpu' ) >>> module . dtype torch . float16 method","title":"pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.to"},{"location":"api/plkit.module/#pytorch_lightningutilitiesdevice_dtype_mixindevicedtypemodulemixincuda","text":"</> Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters device (int, optional) \u2014 if specified, all parameters will be copied to that device Returns (Module) self method","title":"pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cuda"},{"location":"api/plkit.module/#pytorch_lightningutilitiesdevice_dtype_mixindevicedtypemodulemixincpu","text":"</> Moves all model parameters and buffers to the CPU. Returns (Module) self method","title":"pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.cpu"},{"location":"api/plkit.module/#pytorch_lightningutilitiesdevice_dtype_mixindevicedtypemodulemixintype","text":"</> Casts all parameters and buffers to :attr: dst_type . Parameters dst_type (type or string) \u2014 the desired type Returns (Module) self method","title":"pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.type"},{"location":"api/plkit.module/#pytorch_lightningutilitiesdevice_dtype_mixindevicedtypemodulemixinfloat","text":"</> Casts all floating point parameters and buffers to float datatype. Returns (Module) self method","title":"pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.float"},{"location":"api/plkit.module/#pytorch_lightningutilitiesdevice_dtype_mixindevicedtypemodulemixindouble","text":"</> Casts all floating point parameters and buffers to double datatype. Returns (Module) self method","title":"pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.double"},{"location":"api/plkit.module/#pytorch_lightningutilitiesdevice_dtype_mixindevicedtypemodulemixinhalf","text":"</> Casts all floating point parameters and buffers to half datatype. Returns (Module) self class","title":"pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin.half"},{"location":"api/plkit.module/#abcabcmeta","text":"</> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method","title":"abc.ABCMeta"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduleprint","text":"</> Prints only from process 0. Use this in any distributed mode to log only once. Parameters *args \u2014 The thing to print. Will be passed to Python's built-in print function. **kwargs \u2014 Will be passed to Python's built-in print function. Example n : ) method","title":"pytorch_lightning.core.lightning.LightningModule.print"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulelog","text":"</> Log a key, value Example:: self.log('train_loss', loss) The default behavior per hook is as follows .. csv-table:: * also applies to the test loop :header: \"LightningMoule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters name (str) \u2014 key name value (any) \u2014 value name prog_bar (bool, optional) \u2014 if True logs to the progress bar logger (bool, optional) \u2014 if True logs to the logger on_step (bool, optional) \u2014 if True logs at this step. None auto-logs at the training_step but not validation/test_step on_epoch (bool, optional) \u2014 if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step reduce_fx (callable, optional) \u2014 reduction function over step values for end of epoch. Torch.mean by default tbptt_reduce_fx (callable, optional) \u2014 function to reduce on truncated back prop tbptt_pad_token (int, optional) \u2014 token to use for padding enable_graph (bool, optional) \u2014 if True, will not auto detach the graph sync_dist (bool, optional) \u2014 if True, reduces the metric across GPUs/TPUs sync_dist_op (any or str, optional) \u2014 the op to sync across GPUs/TPUs sync_dist_group (any, optional) \u2014 the ddp group method","title":"pytorch_lightning.core.lightning.LightningModule.log"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulelog_dict","text":"</> Log a dictonary of values at once Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters dictionary (dict) \u2014 key value pairs (str, tensors) prog_bar (bool, optional) \u2014 if True logs to the progress base logger (bool, optional) \u2014 if True logs to the logger on_step (bool, optional) \u2014 if True logs at this step. None auto-logs for training_step but not validation/test_step on_epoch (bool, optional) \u2014 if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step reduce_fx (callable, optional) \u2014 reduction function over step values for end of epoch. Torch.mean by default tbptt_reduce_fx (callable, optional) \u2014 function to reduce on truncated back prop tbptt_pad_token (int, optional) \u2014 token to use for padding enable_graph (bool, optional) \u2014 if True, will not auto detach the graph sync_dist (bool, optional) \u2014 if True, reduces the metric across GPUs/TPUs sync_dist_op (any or str, optional) \u2014 the op to sync across GPUs/TPUs sync_dist_group (any, optional) \u2014 the ddp group: method","title":"pytorch_lightning.core.lightning.LightningModule.log_dict"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduleall_gather","text":"</> Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes Parameters tensor (Tensor) \u2014 tensor of shape (batch, ...) group (any, optional) \u2014 the process group to gather results from. Defaults to all processes (world) sync_grads (bool, optional) \u2014 flag that allows users to synchronize gradients for all_gather op Return: A tensor of shape (world_size, batch, ...) method","title":"pytorch_lightning.core.lightning.LightningModule.all_gather"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduleforward","text":"</> Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). Normally you'd call self() from your :meth: training_step method. This makes it easy to write a complex system for training with the outputs you'd want in a prediction setting. You may also find the :func: ~pytorch_lightning.core.decorators.auto_move_data decorator useful when using the module outside Lightning in a production setting. Parameters *args \u2014 Whatever you decide to pass into the forward method. **kwargs \u2014 Keyword arguments are also possible. Return: Predicted output Examples .. code-block:: python # example if we were using this model as a feature extractor def forward(self, x): feature_maps = self.convnet(x) return feature_maps def training_step(self, batch, batch_idx): x, y = batch feature_maps = self(x) logits = self.classifier(feature_maps) # ... return loss # splitting it this way allows model to be used a feature extractor model = MyModelAbove() inputs = server.get_request() results = model(inputs) server.write_results(results) # ------------- # This is in stark contrast to torch.nn.Module where normally you would have this: def forward(self, batch): x, y = batch feature_maps = self.convnet(x) logits = self.classifier(feature_maps) return logits method","title":"pytorch_lightning.core.lightning.LightningModule.forward"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduletraining_step","text":"</> Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters batch ( \u2014 class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int) \u2014 Integer displaying index of this batch optimizer_idx (int) \u2014 When using multiple optimizers, this argument will also be present. hiddens( \u2014 class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - `dict` - A dictionary. Can include any keys, but must include the key 'loss' - `None` - Training will skip to the next batch In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder if optimizer_idx == 1: # do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step ... out, hiddens = self.lstm(data, hiddens) ... return {'loss': loss, 'hiddens': hiddens} Note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. method","title":"pytorch_lightning.core.lightning.LightningModule.training_step"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduletraining_step_end","text":"</> Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters batch_parts_outputs \u2014 What you return in training_step for each batch part. Return: Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denomintaor loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {'pred': out} def training_step_end(self, training_step_outputs): gpu_0_pred = training_step_outputs[0]['pred'] gpu_1_pred = training_step_outputs[1]['pred'] gpu_n_pred = training_step_outputs[n]['pred'] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also See the :ref: multi_gpu guide for more details. method","title":"pytorch_lightning.core.lightning.LightningModule.training_step_end"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduletraining_epoch_end","text":"</> Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs for every training_step. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters outputs (list of any) \u2014 List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note If this method is not overridden, this won't be called. Example:: def training_epoch_end(self, training_step_outputs): # do something with all training_step outputs return result With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, training_step_outputs): for out in training_step_outputs: # do something here method","title":"pytorch_lightning.core.lightning.LightningModule.training_epoch_end"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulevalidation_step","text":"</> Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters batch ( \u2014 class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int) \u2014 The index of this batch dataloader_idx (int) \u2014 The index of the dataloader that produced this batch (only if multiple val datasets used) Return: Any of. - Any object or value - `None` - Validation will skip to the next batch .. code-block:: python # pseudocode of order out = validation_step() if defined('validation_step_end'): out = validation_step_end(out) out = validation_epoch_end(out) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx) # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples .. code-block:: python # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val datasets, validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation datasets def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. Note If you don't need to validate you don't need to implement this method. Note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. method","title":"pytorch_lightning.core.lightning.LightningModule.validation_step"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulevalidation_step_end","text":"</> Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Parameters batch_parts_outputs \u2014 What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log('val_loss', loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs: # do something with these See Also See the :ref: multi_gpu guide for more details. method","title":"pytorch_lightning.core.lightning.LightningModule.validation_step_end"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulevalidation_epoch_end","text":"</> Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters outputs (list of any) \u2014 List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note If you didn't define a :meth: validation_step , this won't be called. Examples With a single dataloader: .. code-block:: python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs: # do something With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader. .. code-block:: python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value) method","title":"pytorch_lightning.core.lightning.LightningModule.validation_epoch_end"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduletest_step","text":"</> Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters batch ( \u2014 class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int) \u2014 The index of this batch. dataloader_idx (int) \u2014 The index of the dataloader that produced this batch (only if multiple test datasets used). Return: Any of. - Any object or value - `None` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx) # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx) Examples .. code-block:: python # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple validation datasets, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test datasets def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. Note If you don't need to validate you don't need to implement this method. Note When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. method","title":"pytorch_lightning.core.lightning.LightningModule.test_step"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduletest_step_end","text":"</> Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Parameters batch_parts_outputs \u2014 What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log('test_loss', loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_epoch_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log('test_loss', loss) See Also See the :ref: multi_gpu guide for more details. method","title":"pytorch_lightning.core.lightning.LightningModule.test_step_end"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduletest_epoch_end","text":"</> Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters outputs (list of any) \u2014 List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: None Note If you didn't define a :meth: test_step , this won't be called. Examples With a single dataloader: .. code-block:: python def test_epoch_end(self, outputs): # do something with the outputs of all test batches all_test_preds = test_step_outputs.predictions some_result = calc_all_results(all_test_preds) self.log(some_result) With multiple dataloaders, outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each test step for that dataloader. .. code-block:: python def test_epoch_end(self, outputs): final_value = 0 for dataloader_outputs in outputs: for test_step_out in dataloader_outputs: # do something final_value += test_step_out self.log('final_metric', final_value) method","title":"pytorch_lightning.core.lightning.LightningModule.test_epoch_end"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulemanual_backward","text":"</> Call this directly from your training_step when doing optimizations manually. By using this we can ensure that all the proper scaling when using 16-bit etc has been done for you This function forwards all args to the .backward() call as well. .. tip:: In manual mode we still automatically clip grads if Trainer(gradient_clip_val=x) is set .. tip:: In manual mode we still automatically accumulate grad over batches if Trainer(accumulate_grad_batches=x) is set and you use optimizer.step() Example:: def training_step(...): (opt_a, opt_b) = self.optimizers() loss = ... # automatically applies scaling, etc... self.manual_backward(loss, opt_a) opt_a.step() method","title":"pytorch_lightning.core.lightning.LightningModule.manual_backward"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulebackward","text":"</> Override backward with your own implementation if you need to. Parameters loss (Tensor) \u2014 Loss is already scaled by accumulated grads optimizer (Optimizer) \u2014 Current optimizer being used optimizer_idx (int) \u2014 Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, loss, optimizer, optimizer_idx): loss.backward() method","title":"pytorch_lightning.core.lightning.LightningModule.backward"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduletoggle_optimizer","text":"</> Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. .. note:: Only called when using multiple optimizers Override for your own behavior method","title":"pytorch_lightning.core.lightning.LightningModule.toggle_optimizer"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduleoptimizer_step","text":"</> Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. .. tip:: With Trainer(enable_pl_optimizer=True) , you can user optimizer.step() directly and it will handle zero_grad, accumulated gradients, AMP, TPU and more automatically for you. Warning If you are overriding this method, make sure that you pass the optimizer_closure parameter to optimizer.step() function as shown in the examples. This ensures that train_step_and_backward_closure is called within :meth: ~pytorch_lightning.trainer.training_loop.TrainLoop.run_training_batch . Parameters epoch (int, optional) \u2014 Current epoch batch_idx (int, optional) \u2014 Index of current batch optimizer (Optimizer, optional) \u2014 A PyTorch optimizer optimizer_idx (int, optional) \u2014 If you used multiple optimizers this indexes into that list. optimizer_closure (callable, optional) \u2014 closure for all optimizers on_tpu (bool, optional) \u2014 true if TPU backward is required using_native_amp (bool, optional) \u2014 True if using native amp using_lbfgs (bool, optional) \u2014 True if the matching optimizer is lbfgs Examples .. code-block:: python # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every 2 steps if optimizer_idx == 0: if batch_idx % 2 == 0 : optimizer.step(closure=optimizer_closure) optimizer.zero_grad() # update discriminator opt every 4 steps if optimizer_idx == 1: if batch_idx % 4 == 0 : optimizer.step(closure=optimizer_closure) optimizer.zero_grad() # ... # add as many optimizers as you want s : n p , : r : ) : e s ) ) method","title":"pytorch_lightning.core.lightning.LightningModule.optimizer_step"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduletbptt_split_batch","text":"</> When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Parameters batch (Tensor) \u2014 Current batch split_size (int) \u2014 The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples .. code-block:: python def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.trainer.Trainer.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . method","title":"pytorch_lightning.core.lightning.LightningModule.tbptt_split_batch"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulefreeze","text":"</> Freeze all params for inference. Example .. code-block:: python model = MyLightningModule(...) model.freeze() method","title":"pytorch_lightning.core.lightning.LightningModule.freeze"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduleunfreeze","text":"</> Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() method","title":"pytorch_lightning.core.lightning.LightningModule.unfreeze"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduleget_progress_bar_dict","text":"</> Implement this to override the default items displayed in the progress bar. By default it includes the average loss value, split index of BPTT (if used) and the version of the experiment when using a logger. .. code-block:: Epoch 1: 4%|\u258e | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10] Here is an example how to override the defaults: .. code-block:: python def get_progress_bar_dict(self): # don't show the version number items = super().get_progress_bar_dict() items.pop(\"v_num\", None) return items Return: Dictionary with the items to be displayed in the progress bar. method","title":"pytorch_lightning.core.lightning.LightningModule.get_progress_bar_dict"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmodulesave_hyperparameters","text":"</> Save all model arguments. Parameters args \u2014 single object of dict , NameSpace or OmegaConf or string names or argumenst from class __init__ >>> from collections import OrderedDict >>> class ManuallyArgsModel ( LightningModule ): ... def __init__ ( self , arg1 , arg2 , arg3 ): ... super () . __init__ () ... # manually assign arguments ... self . save_hyperparameters ( 'arg1' , 'arg3' ) ... def forward ( self , * args , ** kwargs ): ... ... >>> model = ManuallyArgsModel ( 1 , 'abc' , 3.14 ) >>> model . hparams \"arg1\" : 1 \"arg3\" : 3.14 >>> class AutomaticArgsModel ( LightningModule ): ... def __init__ ( self , arg1 , arg2 , arg3 ): ... super () . __init__ () ... # equivalent automatic ... self . save_hyperparameters () ... def forward ( self , * args , ** kwargs ): ... ... >>> model = AutomaticArgsModel ( 1 , 'abc' , 3.14 ) >>> model . hparams \"arg1\" : 1 \"arg2\" : abc \"arg3\" : 3.14 >>> class SingleArgModel ( LightningModule ): ... def __init__ ( self , params ): ... super () . __init__ () ... # manually assign single argument ... self . save_hyperparameters ( params ) ... def forward ( self , * args , ** kwargs ): ... ... >>> model = SingleArgModel ( Namespace ( p1 = 1 , p2 = 'abc' , p3 = 3.14 )) >>> model . hparams \"p1\" : 1 \"p2\" : abc \"p3\" : 3.14 method","title":"pytorch_lightning.core.lightning.LightningModule.save_hyperparameters"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduleto_onnx","text":"</> Saves the model in ONNX format Parameters file_path (str or Path) \u2014 The path of the file the onnx model should be saved to. input_sample (any, optional) \u2014 An input for tracing. Default: None (Use self.example_input_array) **kwargs \u2014 Will be passed to torch.onnx.export function. Example >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) >>> with tempfile . NamedTemporaryFile ( suffix = '.onnx' , delete = False ) as tmpfile : ... model = SimpleModel () ... input_sample = torch . randn (( 1 , 64 )) ... model . to_onnx ( tmpfile . name , input_sample , export_params = True ) ... os . path . isfile ( tmpfile . name ) True method","title":"pytorch_lightning.core.lightning.LightningModule.to_onnx"},{"location":"api/plkit.module/#pytorch_lightningcorelightninglightningmoduleto_torchscript","text":"</> By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has self.example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Parameters file_path (str, Path, or NoneType, optional) \u2014 Path where to save the torchscript. Default: None (no file saved). method (str, optional) \u2014 Whether to use TorchScript's script or trace method. Default: 'script' example_inputs (any, optional) \u2014 An input to be used to do tracing when method is set to 'trace'. Default: None (Use self.example_input_array) **kwargs \u2014 Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. The exported script will be set to evaluation mode. It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example >>> class SimpleModel ( LightningModule ): ... def __init__ ( self ): ... super () . __init__ () ... self . l1 = torch . nn . Linear ( in_features = 64 , out_features = 4 ) ... ... def forward ( self , x ): ... return torch . relu ( self . l1 ( x . view ( x . size ( 0 ), - 1 ))) ... >>> model = SimpleModel () >>> torch . jit . save ( model . to_torchscript (), \"model.pt\" ) # doctest: +SKIP >>> os . path . isfile ( \"model.pt\" ) # doctest: +SKIP >>> torch . jit . save ( model . to_torchscript ( file_path = \"model_trace.pt\" , method = 'trace' , # doctest: +SKIP ... example_inputs = torch . randn ( 1 , 64 ))) # doctest: +SKIP >>> os . path . isfile ( \"model_trace.pt\" ) # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not. method","title":"pytorch_lightning.core.lightning.LightningModule.to_torchscript"},{"location":"api/plkit.module/#plkitmodulemoduleon_epoch_end","text":"</> Keep the epoch progress bar This is not documented but working. method","title":"plkit.module.Module.on_epoch_end"},{"location":"api/plkit.module/#plkitmodulemoduleloss_function","text":"</> Calculate the loss method","title":"plkit.module.Module.loss_function"},{"location":"api/plkit.module/#plkitmodulemoduleconfigure_optimizers","text":"</> Configure the optimizers","title":"plkit.module.Module.configure_optimizers"},{"location":"api/plkit.optuna/","text":"module plkit . optuna </> Optuna wrapper for plkit Classes OptunaSuggest \u2014 Optuna suggests for configuration items </> Optuna \u2014 The class uses optuna to automate hyperparameter tuning </> class plkit.optuna . OptunaSuggest ( default , suggtype , *args , **kwargs ) </> Optuna suggests for configuration items Parameters default (any) \u2014 The default value, which the value will be collapsed to when optuna is opted out. So that you don't have to change your code if you don't run optuna. suggtype (str) \u2014 The type of suggestion For example, cat refers to trial.suggest_categorical The mappings are: cat -> 'suggest_categorical', categorical -> 'suggest_categorical', distuni -> 'suggest_discrete_uniform', dist_uni -> 'suggest_discrete_uniform', discrete_uniform -> 'suggest_discrete_uniform', float -> 'suggest_float', int -> 'suggest_int', loguni -> 'suggest_loguniform', log_uni -> 'suggest_loguniform', uni -> 'suggest_uniform' *args \u2014 The args used in trial.suggest_xxx(name, *args, **kwargs) **kwargs \u2014 The kwargs used in trial.suggest_xxx(name, *args, **kwargs) Attributes args \u2014 *args from Args default (any) \u2014 The default from Args kwargs \u2014 **kwargs from Args suggfunc (str) \u2014 The transformed suggestion name according to suggtype Methods suggest ( name , trial ) (Any) \u2014 Get the suggested value </> method suggest ( name , trial ) </> Get the suggested value This is used in Optuna class, you don't have to call this Parameters name (str) \u2014 The name of the parameter trial (optuna.Trial) \u2014 The trial to get the suggested value from Returns (Any) The suggested value class plkit.optuna . Optuna ( on , n_trials , **kwargs ) </> The class uses optuna to automate hyperparameter tuning Example >>> from plkit import Data , Module , Optuna >>> class MyData ( Data ): >>> ... >>> class MyModel ( Module ): >>> ... >>> class MyOptuna ( Optuna ): >>> def suggests ( self , config ): >>> ... >>> return new_config >>> optuna = MyOptuna ( 'val_loss' , 100 ) >>> optuna . run ( config , model_class , data_class ) Parameters on (str) \u2014 On which value to optimize. Should be one of the keys of dictionary that is returned from validation_epoch_end . val_acc or val_loss for example. n_trials (int) \u2014 Number of trials **kwargs \u2014 Other keyword arguments for optuna.create_study Attributes best_model \u2014 Get the model from best trainer </> best_params \u2014 The best parameters from the study </> best_trainer \u2014 Get the best trainer </> best_trial \u2014 The best trial from the study </> n_trials (int) \u2014 n_trials from Args on (str) \u2014 on from Args study (optuna.Study) \u2014 study object created from kwargs trainers (list) \u2014 list of trainers to keep track of the best one trials \u2014 The trials </> Methods run ( config , data_class , model_class , **kwargs ) \u2014 Run the optimization </> run ( config , data_class , model_class , **kwargs ) \u2014 Run the optimization </> suggests ( trial , conf ) (dict) \u2014 Collect the hyperparameters from the trial suggestions if any configuration item is an OptunaSuggest object </> method suggests ( trial , conf ) </> Collect the hyperparameters from the trial suggestions if any configuration item is an OptunaSuggest object Parameters trial (optuna.Trial) \u2014 the trial object conf (dict) \u2014 The configuration dictionary Returns (dict) A dictionary of suggested parameters method run ( config , data_class , model_class , **kwargs ) </> Run the optimization The optimization is running on fit of the trainer. If test data is provided. Test will be performed as well. Parameters config (dict) \u2014 The configuation dictionary data_class (class) \u2014 The data class subclassed from plkit.Data Note that this is the class itself, not the instantized object. model_class (class) \u2014 The data class subclassed from plkit.Module Note that this is the class itself, not the instantized object. **kwargs \u2014 Other arguments for study.optimize other than func and n_trials . See: https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize method run ( config , data_class , model_class , **kwargs ) </> Run the optimization The optimization is running on fit of the trainer. If test data is provided. Test will be performed as well. Parameters config (dict) \u2014 The configuation dictionary data_class (class) \u2014 The data class subclassed from plkit.Data Note that this is the class itself, not the instantized object. model_class (class) \u2014 The data class subclassed from plkit.Module Note that this is the class itself, not the instantized object. **kwargs \u2014 Other arguments for study.optimize other than func and n_trials . See: https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize","title":"plkit.optuna"},{"location":"api/plkit.optuna/#plkitoptuna","text":"</> Optuna wrapper for plkit Classes OptunaSuggest \u2014 Optuna suggests for configuration items </> Optuna \u2014 The class uses optuna to automate hyperparameter tuning </> class","title":"plkit.optuna"},{"location":"api/plkit.optuna/#plkitoptunaoptunasuggest","text":"</> Optuna suggests for configuration items Parameters default (any) \u2014 The default value, which the value will be collapsed to when optuna is opted out. So that you don't have to change your code if you don't run optuna. suggtype (str) \u2014 The type of suggestion For example, cat refers to trial.suggest_categorical The mappings are: cat -> 'suggest_categorical', categorical -> 'suggest_categorical', distuni -> 'suggest_discrete_uniform', dist_uni -> 'suggest_discrete_uniform', discrete_uniform -> 'suggest_discrete_uniform', float -> 'suggest_float', int -> 'suggest_int', loguni -> 'suggest_loguniform', log_uni -> 'suggest_loguniform', uni -> 'suggest_uniform' *args \u2014 The args used in trial.suggest_xxx(name, *args, **kwargs) **kwargs \u2014 The kwargs used in trial.suggest_xxx(name, *args, **kwargs) Attributes args \u2014 *args from Args default (any) \u2014 The default from Args kwargs \u2014 **kwargs from Args suggfunc (str) \u2014 The transformed suggestion name according to suggtype Methods suggest ( name , trial ) (Any) \u2014 Get the suggested value </> method","title":"plkit.optuna.OptunaSuggest"},{"location":"api/plkit.optuna/#plkitoptunaoptunasuggestsuggest","text":"</> Get the suggested value This is used in Optuna class, you don't have to call this Parameters name (str) \u2014 The name of the parameter trial (optuna.Trial) \u2014 The trial to get the suggested value from Returns (Any) The suggested value class","title":"plkit.optuna.OptunaSuggest.suggest"},{"location":"api/plkit.optuna/#plkitoptunaoptuna","text":"</> The class uses optuna to automate hyperparameter tuning Example >>> from plkit import Data , Module , Optuna >>> class MyData ( Data ): >>> ... >>> class MyModel ( Module ): >>> ... >>> class MyOptuna ( Optuna ): >>> def suggests ( self , config ): >>> ... >>> return new_config >>> optuna = MyOptuna ( 'val_loss' , 100 ) >>> optuna . run ( config , model_class , data_class ) Parameters on (str) \u2014 On which value to optimize. Should be one of the keys of dictionary that is returned from validation_epoch_end . val_acc or val_loss for example. n_trials (int) \u2014 Number of trials **kwargs \u2014 Other keyword arguments for optuna.create_study Attributes best_model \u2014 Get the model from best trainer </> best_params \u2014 The best parameters from the study </> best_trainer \u2014 Get the best trainer </> best_trial \u2014 The best trial from the study </> n_trials (int) \u2014 n_trials from Args on (str) \u2014 on from Args study (optuna.Study) \u2014 study object created from kwargs trainers (list) \u2014 list of trainers to keep track of the best one trials \u2014 The trials </> Methods run ( config , data_class , model_class , **kwargs ) \u2014 Run the optimization </> run ( config , data_class , model_class , **kwargs ) \u2014 Run the optimization </> suggests ( trial , conf ) (dict) \u2014 Collect the hyperparameters from the trial suggestions if any configuration item is an OptunaSuggest object </> method","title":"plkit.optuna.Optuna"},{"location":"api/plkit.optuna/#plkitoptunaoptunasuggests","text":"</> Collect the hyperparameters from the trial suggestions if any configuration item is an OptunaSuggest object Parameters trial (optuna.Trial) \u2014 the trial object conf (dict) \u2014 The configuration dictionary Returns (dict) A dictionary of suggested parameters method","title":"plkit.optuna.Optuna.suggests"},{"location":"api/plkit.optuna/#plkitoptunaoptunarun","text":"</> Run the optimization The optimization is running on fit of the trainer. If test data is provided. Test will be performed as well. Parameters config (dict) \u2014 The configuation dictionary data_class (class) \u2014 The data class subclassed from plkit.Data Note that this is the class itself, not the instantized object. model_class (class) \u2014 The data class subclassed from plkit.Module Note that this is the class itself, not the instantized object. **kwargs \u2014 Other arguments for study.optimize other than func and n_trials . See: https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize method","title":"plkit.optuna.Optuna.run"},{"location":"api/plkit.optuna/#plkitoptunaoptunarun_1","text":"</> Run the optimization The optimization is running on fit of the trainer. If test data is provided. Test will be performed as well. Parameters config (dict) \u2014 The configuation dictionary data_class (class) \u2014 The data class subclassed from plkit.Data Note that this is the class itself, not the instantized object. model_class (class) \u2014 The data class subclassed from plkit.Module Note that this is the class itself, not the instantized object. **kwargs \u2014 Other arguments for study.optimize other than func and n_trials . See: https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize","title":"plkit.optuna.Optuna.run"},{"location":"api/plkit.runner/","text":"module plkit . runner </> Run jobs via non-local runners. Classes Runner ( ) \u2014 The base class for runner </> LocalRunner \u2014 The local runner for the pipeline </> SGERunner \u2014 The SGE runner for the pipeline </> abstract class plkit.runner . Runner ( ) </> The base class for runner Methods run ( config , data_class , model_class , optuna ) ( Trainer ) \u2014 Run the whole pipeline using the runner </> abstract method run ( config , data_class , model_class , optuna=None ) </> Run the whole pipeline using the runner Parameters config (dict(str: any)) \u2014 A dictionary of configuration, must have following items: - batch_size: The batch size - num_classes: The number of classes for classification 1 means regression data_class (type of DataModule ) \u2014 The data class subclassed from Data model_class (type of Module ) \u2014 The model class subclassed from Module optuna ( Optuna , optional) \u2014 The optuna object runner \u2014 The runner object Returns ( Trainer ) The trainer object class plkit.runner . LocalRunner ( ) </> Bases plkit.runner.Runner The local runner for the pipeline Methods run ( config , data_class , model_class , optuna ) ( Trainer ) \u2014 Run the pipeline locally </> method run ( config , data_class , model_class , optuna=None ) \u2192 Trainer </> Run the pipeline locally class plkit.runner . SGERunner ( *args , **opts ) </> Bases plkit.runner.LocalRunner plkit.runner.Runner The SGE runner for the pipeline Parameters opts \u2014 The options for SGE runner, which will be translated as arguments for qsub . For example opts={'notify': True} will be translated as qsub --notify ... from command line. there are two special options qsub and workdir . qsub specified the path to qsub executable and workdir specifies a location to save outputs, errors and scripts of each job. Attributes qsub \u2014 The path to qsub executable workdir \u2014 The path to the workdir Methods run ( config , data_class , model_class , optuna ) ( Trainer ) \u2014 Run the job depending on the env flag </> method run ( config , data_class , model_class , optuna=None ) \u2192 Trainer </> Run the job depending on the env flag","title":"plkit.runner"},{"location":"api/plkit.runner/#plkitrunner","text":"</> Run jobs via non-local runners. Classes Runner ( ) \u2014 The base class for runner </> LocalRunner \u2014 The local runner for the pipeline </> SGERunner \u2014 The SGE runner for the pipeline </> abstract class","title":"plkit.runner"},{"location":"api/plkit.runner/#plkitrunnerrunner","text":"</> The base class for runner Methods run ( config , data_class , model_class , optuna ) ( Trainer ) \u2014 Run the whole pipeline using the runner </> abstract method","title":"plkit.runner.Runner"},{"location":"api/plkit.runner/#plkitrunnerrunnerrun","text":"</> Run the whole pipeline using the runner Parameters config (dict(str: any)) \u2014 A dictionary of configuration, must have following items: - batch_size: The batch size - num_classes: The number of classes for classification 1 means regression data_class (type of DataModule ) \u2014 The data class subclassed from Data model_class (type of Module ) \u2014 The model class subclassed from Module optuna ( Optuna , optional) \u2014 The optuna object runner \u2014 The runner object Returns ( Trainer ) The trainer object class","title":"plkit.runner.Runner.run"},{"location":"api/plkit.runner/#plkitrunnerlocalrunner","text":"</> Bases plkit.runner.Runner The local runner for the pipeline Methods run ( config , data_class , model_class , optuna ) ( Trainer ) \u2014 Run the pipeline locally </> method","title":"plkit.runner.LocalRunner"},{"location":"api/plkit.runner/#plkitrunnerlocalrunnerrun","text":"</> Run the pipeline locally class","title":"plkit.runner.LocalRunner.run"},{"location":"api/plkit.runner/#plkitrunnersgerunner","text":"</> Bases plkit.runner.LocalRunner plkit.runner.Runner The SGE runner for the pipeline Parameters opts \u2014 The options for SGE runner, which will be translated as arguments for qsub . For example opts={'notify': True} will be translated as qsub --notify ... from command line. there are two special options qsub and workdir . qsub specified the path to qsub executable and workdir specifies a location to save outputs, errors and scripts of each job. Attributes qsub \u2014 The path to qsub executable workdir \u2014 The path to the workdir Methods run ( config , data_class , model_class , optuna ) ( Trainer ) \u2014 Run the job depending on the env flag </> method","title":"plkit.runner.SGERunner"},{"location":"api/plkit.runner/#plkitrunnersgerunnerrun","text":"</> Run the job depending on the env flag","title":"plkit.runner.SGERunner.run"},{"location":"api/plkit.trainer/","text":"module plkit . trainer </> Wrapper of the Trainer class Classes ProgressBar \u2014 Align the Epoch in progress bar </> Trainer \u2014 The Trainner class </> class plkit.trainer . ProgressBar ( refresh_rate=1 , process_position=0 ) </> Bases pytorch_lightning.callbacks.progress.ProgressBar pytorch_lightning.callbacks.progress.ProgressBarBase pytorch_lightning.callbacks.base.Callback Align the Epoch in progress bar Attributes test_batch_idx (int) \u2014 The current batch index being processed during testing. Use this to update your progress bar. </> total_test_batches (int) \u2014 The total number of training batches during testing, which may change from epoch to epoch. Use this to set the total number of iterations in the progress bar. Can return inf if the test dataloader is of infinite size. </> total_train_batches (int) \u2014 The total number of training batches during training, which may change from epoch to epoch. Use this to set the total number of iterations in the progress bar. Can return inf if the training dataloader is of infinite size. </> total_val_batches (int) \u2014 The total number of training batches during validation, which may change from epoch to epoch. Use this to set the total number of iterations in the progress bar. Can return inf if the validation dataloader is of infinite size. </> train_batch_idx (int) \u2014 The current batch index being processed during training. Use this to update your progress bar. </> val_batch_idx (int) \u2014 The current batch index being processed during validation. Use this to update your progress bar. </> Methods disable ( ) \u2014 You should provide a way to disable the progress bar. The :class: ~pytorch_lightning.trainer.trainer.Trainer will call this to disable the output on processes that have a rank different from 0, e.g., in multi-node training. </> enable ( ) \u2014 You should provide a way to enable the progress bar. The :class: ~pytorch_lightning.trainer.trainer.Trainer will call this in e.g. pre-training routines like the :ref: learning rate finder <lr_finder> to temporarily enable and disable the main progress bar. </> init_sanity_tqdm ( ) (tqdm) \u2014 Override this to customize the tqdm bar for the validation sanity run. </> init_test_tqdm ( ) (tqdm) \u2014 Override this to customize the tqdm bar for testing. </> init_train_tqdm ( ) (tqdm) \u2014 Override this to customize the tqdm bar for training. </> init_validation_tqdm ( ) (tqdm) \u2014 Override this to customize the tqdm bar for validation. </> on_after_backward ( trainer , pl_module ) \u2014 Called after loss.backward() and before optimizers do anything. </> on_batch_end ( trainer , pl_module ) \u2014 Called when the training batch ends. </> on_batch_start ( trainer , pl_module ) \u2014 Called when the training batch begins. </> on_before_zero_grad ( trainer , pl_module , optimizer ) \u2014 Called after optimizer.step() and before optimizer.zero_grad(). </> on_epoch_end ( trainer , pl_module ) \u2014 Called when the epoch ends. </> on_epoch_start ( trainer , pl_module ) \u2014 Try to align the epoch number </> on_fit_end ( trainer , pl_module ) \u2014 Called when fit ends </> on_fit_start ( trainer , pl_module ) \u2014 Called when fit begins </> on_init_end ( trainer ) \u2014 Called when the trainer initialization ends, model has not yet been set. </> on_init_start ( trainer ) \u2014 Called when the trainer initialization begins, model has not yet been set. </> on_keyboard_interrupt ( trainer , pl_module ) \u2014 Called when the training is interrupted by KeyboardInterrupt. </> on_load_checkpoint ( checkpointed_state ) \u2014 Called when loading a model checkpoint, use to reload state. </> on_pretrain_routine_end ( trainer , pl_module ) \u2014 Called when the pretrain routine ends. </> on_pretrain_routine_start ( trainer , pl_module ) \u2014 Called when the pretrain routine begins. </> on_sanity_check_end ( trainer , pl_module ) \u2014 Called when the validation sanity check ends. </> on_sanity_check_start ( trainer , pl_module ) \u2014 Called when the validation sanity check starts. </> on_save_checkpoint ( trainer , pl_module ) \u2014 Called when saving a model checkpoint, use to persist state. </> on_test_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the test batch ends. </> on_test_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) \u2014 Called when the test batch begins. </> on_test_end ( trainer , pl_module ) \u2014 Called when the test ends. </> on_test_epoch_end ( trainer , pl_module ) \u2014 Called when the test epoch ends. </> on_test_epoch_start ( trainer , pl_module ) \u2014 Called when the test epoch begins. </> on_test_start ( trainer , pl_module ) \u2014 Called when the test begins. </> on_train_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the train batch ends. </> on_train_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) \u2014 Called when the train batch begins. </> on_train_end ( trainer , pl_module ) \u2014 Called when the train ends. </> on_train_epoch_end ( trainer , pl_module , outputs ) \u2014 Called when the train epoch ends. </> on_train_epoch_start ( trainer , pl_module ) \u2014 Called when the train epoch begins. </> on_train_start ( trainer , pl_module ) \u2014 Called when the train begins. </> on_validation_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the validation batch ends. </> on_validation_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) \u2014 Called when the validation batch begins. </> on_validation_end ( trainer , pl_module ) \u2014 Called when the validation loop ends. </> on_validation_epoch_end ( trainer , pl_module ) \u2014 Called when the val epoch ends. </> on_validation_epoch_start ( trainer , pl_module ) \u2014 Called when the val epoch begins. </> on_validation_start ( trainer , pl_module ) \u2014 Called when the validation loop begins. </> setup ( trainer , pl_module , stage ) \u2014 Called when fit or test begins </> teardown ( trainer , pl_module , stage ) \u2014 Called when fit or test ends </> method setup ( trainer , pl_module , stage ) </> Called when fit or test begins method teardown ( trainer , pl_module , stage ) </> Called when fit or test ends method on_init_start ( trainer ) </> Called when the trainer initialization begins, model has not yet been set. method on_fit_start ( trainer , pl_module ) </> Called when fit begins method on_fit_end ( trainer , pl_module ) </> Called when fit ends method on_train_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) </> Called when the train batch begins. method on_train_epoch_start ( trainer , pl_module ) </> Called when the train epoch begins. method on_train_epoch_end ( trainer , pl_module , outputs ) </> Called when the train epoch ends. method on_validation_epoch_start ( trainer , pl_module ) </> Called when the val epoch begins. method on_validation_epoch_end ( trainer , pl_module ) </> Called when the val epoch ends. method on_test_epoch_start ( trainer , pl_module ) </> Called when the test epoch begins. method on_test_epoch_end ( trainer , pl_module ) </> Called when the test epoch ends. method on_epoch_end ( trainer , pl_module ) </> Called when the epoch ends. method on_batch_start ( trainer , pl_module ) </> Called when the training batch begins. method on_validation_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) </> Called when the validation batch begins. method on_test_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) </> Called when the test batch begins. method on_batch_end ( trainer , pl_module ) </> Called when the training batch ends. method on_pretrain_routine_start ( trainer , pl_module ) </> Called when the pretrain routine begins. method on_pretrain_routine_end ( trainer , pl_module ) </> Called when the pretrain routine ends. method on_keyboard_interrupt ( trainer , pl_module ) </> Called when the training is interrupted by KeyboardInterrupt. method on_save_checkpoint ( trainer , pl_module ) </> Called when saving a model checkpoint, use to persist state. method on_load_checkpoint ( checkpointed_state ) </> Called when loading a model checkpoint, use to reload state. method on_after_backward ( trainer , pl_module ) </> Called after loss.backward() and before optimizers do anything. method on_before_zero_grad ( trainer , pl_module , optimizer ) </> Called after optimizer.step() and before optimizer.zero_grad(). method on_init_end ( trainer ) </> Called when the trainer initialization ends, model has not yet been set. method disable ( ) </> You should provide a way to disable the progress bar. The :class: ~pytorch_lightning.trainer.trainer.Trainer will call this to disable the output on processes that have a rank different from 0, e.g., in multi-node training. method enable ( ) </> You should provide a way to enable the progress bar. The :class: ~pytorch_lightning.trainer.trainer.Trainer will call this in e.g. pre-training routines like the :ref: learning rate finder <lr_finder> to temporarily enable and disable the main progress bar. method init_sanity_tqdm ( ) \u2192 tqdm </> Override this to customize the tqdm bar for the validation sanity run. method init_train_tqdm ( ) \u2192 tqdm </> Override this to customize the tqdm bar for training. method init_validation_tqdm ( ) \u2192 tqdm </> Override this to customize the tqdm bar for validation. method init_test_tqdm ( ) \u2192 tqdm </> Override this to customize the tqdm bar for testing. method on_sanity_check_start ( trainer , pl_module ) </> Called when the validation sanity check starts. method on_sanity_check_end ( trainer , pl_module ) </> Called when the validation sanity check ends. method on_train_start ( trainer , pl_module ) </> Called when the train begins. method on_train_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) </> Called when the train batch ends. method on_validation_start ( trainer , pl_module ) </> Called when the validation loop begins. method on_validation_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) </> Called when the validation batch ends. method on_validation_end ( trainer , pl_module ) </> Called when the validation loop ends. method on_train_end ( trainer , pl_module ) </> Called when the train ends. method on_test_start ( trainer , pl_module ) </> Called when the test begins. method on_test_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) </> Called when the test batch ends. method on_test_end ( trainer , pl_module ) </> Called when the test ends. method on_epoch_start ( trainer , pl_module ) </> Try to align the epoch number class plkit.trainer . Trainer ( *args , **kwargs ) </> Bases pytorch_lightning.trainer.trainer.Trainer pytorch_lightning.trainer.properties.TrainerProperties pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin pytorch_lightning.trainer.logging.TrainerLoggingMixin pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin abc.ABC pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes The Trainner class from_config (aka from_dict ) added as classmethod to instantiate trainer from configuration dictionaries. Attributes checkpoint_callback (ModelCheckpoint, optional) \u2014 The first checkpoint callback in the Trainer.callbacks list, or None if no checkpoint callbacks exist. </> checkpoint_callbacks (list of ModelCheckpoint) \u2014 A list of all instances of ModelCheckpoint found in the Trainer.callbacks list. </> default_root_dir (str) \u2014 The default location to save artifacts of loggers, checkpoints etc. It is used as a fallback if logger or checkpoint callback do not define specific save paths. </> disable_validation (bool) \u2014 Check if validation is disabled during training. </> enable_validation (bool) \u2014 Check if we should run validation during training. </> progress_bar_dict (dict) \u2014 Format progress bar metrics. </> weights_save_path (str) \u2014 The default root location to save weights (checkpoints), e.g., when the :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint does not define a file path. </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods available_plugins ( ) \u2014 List of all available plugins that can be string arguments to the trainer. Returns: List of all available plugins that are supported as string arguments. </> fit ( *args , **kwargs ) \u2014 Train and validate the model </> from_config ( config , **kwargs ) \u2014 Create an instance from CLI arguments. </> from_config ( config , **kwargs ) \u2014 Create an instance from CLI arguments. </> get_deprecated_arg_names ( ) (list) \u2014 Returns a list with deprecated Trainer arguments. </> on_after_backward ( ) \u2014 Called after loss.backward() and before optimizers do anything. </> on_batch_end ( ) \u2014 Called when the training batch ends. </> on_batch_start ( ) \u2014 Called when the training batch begins. </> on_before_zero_grad ( optimizer ) \u2014 Called after optimizer.step() and before optimizer.zero_grad(). </> on_epoch_end ( ) \u2014 Called when the epoch ends. </> on_epoch_start ( ) \u2014 Called when the epoch begins. </> on_fit_end ( ) \u2014 Called when the trainer initialization begins, model has not yet been set. </> on_fit_start ( ) \u2014 Called when the trainer initialization begins, model has not yet been set. </> on_init_end ( ) \u2014 Called when the trainer initialization ends, model has not yet been set. </> on_init_start ( ) \u2014 Called when the trainer initialization begins, model has not yet been set. </> on_keyboard_interrupt ( ) \u2014 Called when the training is interrupted by KeyboardInterrupt. </> on_load_checkpoint ( checkpoint ) \u2014 Called when loading a model checkpoint. </> on_pretrain_routine_end ( model ) \u2014 Called when the train ends. </> on_pretrain_routine_start ( model ) \u2014 Called when the train begins. </> on_sanity_check_end ( ) \u2014 Called when the validation sanity check ends. </> on_sanity_check_start ( ) \u2014 Called when the validation sanity check starts. </> on_save_checkpoint ( ) \u2014 Called when saving a model checkpoint. </> on_test_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the test batch ends. </> on_test_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called when the test batch begins. </> on_test_end ( ) \u2014 Called when the test ends. </> on_test_epoch_end ( ) \u2014 Called when the epoch ends. </> on_test_epoch_start ( ) \u2014 Called when the epoch begins. </> on_test_start ( ) \u2014 Called when the test begins. </> on_train_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the training batch ends. </> on_train_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called when the training batch begins. </> on_train_end ( ) \u2014 Called when the train ends. </> on_train_epoch_end ( outputs ) \u2014 Called when the epoch ends. </> on_train_epoch_start ( ) \u2014 Called when the epoch begins. </> on_train_start ( ) \u2014 Called when the train begins. </> on_validation_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the validation batch ends. </> on_validation_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called when the validation batch begins. </> on_validation_end ( ) \u2014 Called when the validation loop ends. </> on_validation_epoch_end ( ) \u2014 Called when the epoch ends. </> on_validation_epoch_start ( ) \u2014 Called when the epoch begins. </> on_validation_start ( ) \u2014 Called when the validation loop begins. </> process_dict_result ( output , train ) \u2014 Reduces output according to the training mode. </> request_dataloader ( dataloader_fx ) (DataLoader) \u2014 Handles downloading data in the GPU or TPU case. </> reset_test_dataloader ( model ) \u2014 Resets the validation dataloader and determines the number of batches. </> reset_train_dataloader ( model ) \u2014 Resets the train dataloader and initialises required variables (number of batches, when to validate, etc.). </> reset_val_dataloader ( model ) \u2014 Resets the validation dataloader and determines the number of batches. </> setup ( model , stage ) \u2014 Called in the beginning of fit and test </> teardown ( stage ) \u2014 Called at the end of fit and test </> test ( *args , **kwargs ) \u2014 Test the model </> tune ( model , train_dataloader , val_dataloaders , datamodule ) \u2014 Runs routines to tune hyperparameters before training. </> class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method reset_train_dataloader ( model ) </> Resets the train dataloader and initialises required variables (number of batches, when to validate, etc.). Parameters model (LightningModule) \u2014 The current LightningModule method reset_val_dataloader ( model ) </> Resets the validation dataloader and determines the number of batches. Parameters model (LightningModule) \u2014 The current LightningModule method reset_test_dataloader ( model ) </> Resets the validation dataloader and determines the number of batches. Parameters model \u2014 The current LightningModule method request_dataloader ( dataloader_fx ) </> Handles downloading data in the GPU or TPU case. Parameters dataloader_fx (callable) \u2014 The bound dataloader getter Returns (DataLoader) The dataloader method process_dict_result ( output , train=False ) </> Reduces output according to the training mode. Separates loss from logging and progress bar metrics method setup ( model , stage ) </> Called in the beginning of fit and test method teardown ( stage ) </> Called at the end of fit and test method on_init_start ( ) </> Called when the trainer initialization begins, model has not yet been set. method on_init_end ( ) </> Called when the trainer initialization ends, model has not yet been set. method on_fit_start ( ) </> Called when the trainer initialization begins, model has not yet been set. method on_fit_end ( ) </> Called when the trainer initialization begins, model has not yet been set. method on_sanity_check_start ( ) </> Called when the validation sanity check starts. method on_sanity_check_end ( ) </> Called when the validation sanity check ends. method on_train_epoch_start ( ) </> Called when the epoch begins. method on_train_epoch_end ( outputs ) </> Called when the epoch ends. method on_validation_epoch_start ( ) </> Called when the epoch begins. method on_validation_epoch_end ( ) </> Called when the epoch ends. method on_test_epoch_start ( ) </> Called when the epoch begins. method on_test_epoch_end ( ) </> Called when the epoch ends. method on_epoch_start ( ) </> Called when the epoch begins. method on_epoch_end ( ) </> Called when the epoch ends. method on_train_start ( ) </> Called when the train begins. method on_train_end ( ) </> Called when the train ends. method on_pretrain_routine_start ( model ) </> Called when the train begins. method on_pretrain_routine_end ( model ) </> Called when the train ends. method on_batch_start ( ) </> Called when the training batch begins. method on_batch_end ( ) </> Called when the training batch ends. method on_train_batch_start ( batch , batch_idx , dataloader_idx ) </> Called when the training batch begins. method on_train_batch_end ( outputs , batch , batch_idx , dataloader_idx ) </> Called when the training batch ends. method on_validation_batch_start ( batch , batch_idx , dataloader_idx ) </> Called when the validation batch begins. method on_validation_batch_end ( outputs , batch , batch_idx , dataloader_idx ) </> Called when the validation batch ends. method on_test_batch_start ( batch , batch_idx , dataloader_idx ) </> Called when the test batch begins. method on_test_batch_end ( outputs , batch , batch_idx , dataloader_idx ) </> Called when the test batch ends. method on_validation_start ( ) </> Called when the validation loop begins. method on_validation_end ( ) </> Called when the validation loop ends. method on_test_start ( ) </> Called when the test begins. method on_test_end ( ) </> Called when the test ends. method on_keyboard_interrupt ( ) </> Called when the training is interrupted by KeyboardInterrupt. method on_save_checkpoint ( ) </> Called when saving a model checkpoint. method on_load_checkpoint ( checkpoint ) </> Called when loading a model checkpoint. method on_after_backward ( ) </> Called after loss.backward() and before optimizers do anything. method on_before_zero_grad ( optimizer ) </> Called after optimizer.step() and before optimizer.zero_grad(). classmethod get_deprecated_arg_names ( ) \u2192 list </> Returns a list with deprecated Trainer arguments. method tune ( model , train_dataloader=None , val_dataloaders=None , datamodule=None ) </> Runs routines to tune hyperparameters before training. Parameters model (LightningModule) \u2014 Model to tune. train_dataloader (DataLoader, optional) \u2014 A Pytorch DataLoader with training samples. If the model has a predefined train_dataloader method this will be skipped. val_dataloaders (Union(dataloader, list of dataloader, nonetype), optional) \u2014 Either a single Pytorch Dataloader or a list of them, specifying validation samples. If the model has a predefined val_dataloaders method this will be skipped datamodule (LightningDataModule, optional) \u2014 A instance of :class: LightningDataModule . staticmethod available_plugins ( ) </> List of all available plugins that can be string arguments to the trainer. Returns: List of all available plugins that are supported as string arguments. classmethod from_config ( config , **kwargs ) </> Create an instance from CLI arguments. Examples >>> config = { 'my_custom_arg' : 'something' } >>> trainer = Trainer . from_dict ( config , logger = False ) Parameters config \u2014 The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: Trainer . **kwargs \u2014 Additional keyword arguments that may override ones in the parser or namespace. These must be valid Trainer arguments. classmethod from_config ( config , **kwargs ) </> Create an instance from CLI arguments. Examples >>> config = { 'my_custom_arg' : 'something' } >>> trainer = Trainer . from_dict ( config , logger = False ) Parameters config \u2014 The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: Trainer . **kwargs \u2014 Additional keyword arguments that may override ones in the parser or namespace. These must be valid Trainer arguments. method fit ( *args , **kwargs ) </> Train and validate the model method test ( *args , **kwargs ) </> Test the model","title":"plkit.trainer"},{"location":"api/plkit.trainer/#plkittrainer","text":"</> Wrapper of the Trainer class Classes ProgressBar \u2014 Align the Epoch in progress bar </> Trainer \u2014 The Trainner class </> class","title":"plkit.trainer"},{"location":"api/plkit.trainer/#plkittrainerprogressbar","text":"</> Bases pytorch_lightning.callbacks.progress.ProgressBar pytorch_lightning.callbacks.progress.ProgressBarBase pytorch_lightning.callbacks.base.Callback Align the Epoch in progress bar Attributes test_batch_idx (int) \u2014 The current batch index being processed during testing. Use this to update your progress bar. </> total_test_batches (int) \u2014 The total number of training batches during testing, which may change from epoch to epoch. Use this to set the total number of iterations in the progress bar. Can return inf if the test dataloader is of infinite size. </> total_train_batches (int) \u2014 The total number of training batches during training, which may change from epoch to epoch. Use this to set the total number of iterations in the progress bar. Can return inf if the training dataloader is of infinite size. </> total_val_batches (int) \u2014 The total number of training batches during validation, which may change from epoch to epoch. Use this to set the total number of iterations in the progress bar. Can return inf if the validation dataloader is of infinite size. </> train_batch_idx (int) \u2014 The current batch index being processed during training. Use this to update your progress bar. </> val_batch_idx (int) \u2014 The current batch index being processed during validation. Use this to update your progress bar. </> Methods disable ( ) \u2014 You should provide a way to disable the progress bar. The :class: ~pytorch_lightning.trainer.trainer.Trainer will call this to disable the output on processes that have a rank different from 0, e.g., in multi-node training. </> enable ( ) \u2014 You should provide a way to enable the progress bar. The :class: ~pytorch_lightning.trainer.trainer.Trainer will call this in e.g. pre-training routines like the :ref: learning rate finder <lr_finder> to temporarily enable and disable the main progress bar. </> init_sanity_tqdm ( ) (tqdm) \u2014 Override this to customize the tqdm bar for the validation sanity run. </> init_test_tqdm ( ) (tqdm) \u2014 Override this to customize the tqdm bar for testing. </> init_train_tqdm ( ) (tqdm) \u2014 Override this to customize the tqdm bar for training. </> init_validation_tqdm ( ) (tqdm) \u2014 Override this to customize the tqdm bar for validation. </> on_after_backward ( trainer , pl_module ) \u2014 Called after loss.backward() and before optimizers do anything. </> on_batch_end ( trainer , pl_module ) \u2014 Called when the training batch ends. </> on_batch_start ( trainer , pl_module ) \u2014 Called when the training batch begins. </> on_before_zero_grad ( trainer , pl_module , optimizer ) \u2014 Called after optimizer.step() and before optimizer.zero_grad(). </> on_epoch_end ( trainer , pl_module ) \u2014 Called when the epoch ends. </> on_epoch_start ( trainer , pl_module ) \u2014 Try to align the epoch number </> on_fit_end ( trainer , pl_module ) \u2014 Called when fit ends </> on_fit_start ( trainer , pl_module ) \u2014 Called when fit begins </> on_init_end ( trainer ) \u2014 Called when the trainer initialization ends, model has not yet been set. </> on_init_start ( trainer ) \u2014 Called when the trainer initialization begins, model has not yet been set. </> on_keyboard_interrupt ( trainer , pl_module ) \u2014 Called when the training is interrupted by KeyboardInterrupt. </> on_load_checkpoint ( checkpointed_state ) \u2014 Called when loading a model checkpoint, use to reload state. </> on_pretrain_routine_end ( trainer , pl_module ) \u2014 Called when the pretrain routine ends. </> on_pretrain_routine_start ( trainer , pl_module ) \u2014 Called when the pretrain routine begins. </> on_sanity_check_end ( trainer , pl_module ) \u2014 Called when the validation sanity check ends. </> on_sanity_check_start ( trainer , pl_module ) \u2014 Called when the validation sanity check starts. </> on_save_checkpoint ( trainer , pl_module ) \u2014 Called when saving a model checkpoint, use to persist state. </> on_test_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the test batch ends. </> on_test_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) \u2014 Called when the test batch begins. </> on_test_end ( trainer , pl_module ) \u2014 Called when the test ends. </> on_test_epoch_end ( trainer , pl_module ) \u2014 Called when the test epoch ends. </> on_test_epoch_start ( trainer , pl_module ) \u2014 Called when the test epoch begins. </> on_test_start ( trainer , pl_module ) \u2014 Called when the test begins. </> on_train_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the train batch ends. </> on_train_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) \u2014 Called when the train batch begins. </> on_train_end ( trainer , pl_module ) \u2014 Called when the train ends. </> on_train_epoch_end ( trainer , pl_module , outputs ) \u2014 Called when the train epoch ends. </> on_train_epoch_start ( trainer , pl_module ) \u2014 Called when the train epoch begins. </> on_train_start ( trainer , pl_module ) \u2014 Called when the train begins. </> on_validation_batch_end ( trainer , pl_module , outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the validation batch ends. </> on_validation_batch_start ( trainer , pl_module , batch , batch_idx , dataloader_idx ) \u2014 Called when the validation batch begins. </> on_validation_end ( trainer , pl_module ) \u2014 Called when the validation loop ends. </> on_validation_epoch_end ( trainer , pl_module ) \u2014 Called when the val epoch ends. </> on_validation_epoch_start ( trainer , pl_module ) \u2014 Called when the val epoch begins. </> on_validation_start ( trainer , pl_module ) \u2014 Called when the validation loop begins. </> setup ( trainer , pl_module , stage ) \u2014 Called when fit or test begins </> teardown ( trainer , pl_module , stage ) \u2014 Called when fit or test ends </> method","title":"plkit.trainer.ProgressBar"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbacksetup","text":"</> Called when fit or test begins method","title":"pytorch_lightning.callbacks.base.Callback.setup"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackteardown","text":"</> Called when fit or test ends method","title":"pytorch_lightning.callbacks.base.Callback.teardown"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_init_start","text":"</> Called when the trainer initialization begins, model has not yet been set. method","title":"pytorch_lightning.callbacks.base.Callback.on_init_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_fit_start","text":"</> Called when fit begins method","title":"pytorch_lightning.callbacks.base.Callback.on_fit_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_fit_end","text":"</> Called when fit ends method","title":"pytorch_lightning.callbacks.base.Callback.on_fit_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_train_batch_start","text":"</> Called when the train batch begins. method","title":"pytorch_lightning.callbacks.base.Callback.on_train_batch_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_train_epoch_start","text":"</> Called when the train epoch begins. method","title":"pytorch_lightning.callbacks.base.Callback.on_train_epoch_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_train_epoch_end","text":"</> Called when the train epoch ends. method","title":"pytorch_lightning.callbacks.base.Callback.on_train_epoch_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_validation_epoch_start","text":"</> Called when the val epoch begins. method","title":"pytorch_lightning.callbacks.base.Callback.on_validation_epoch_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_validation_epoch_end","text":"</> Called when the val epoch ends. method","title":"pytorch_lightning.callbacks.base.Callback.on_validation_epoch_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_test_epoch_start","text":"</> Called when the test epoch begins. method","title":"pytorch_lightning.callbacks.base.Callback.on_test_epoch_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_test_epoch_end","text":"</> Called when the test epoch ends. method","title":"pytorch_lightning.callbacks.base.Callback.on_test_epoch_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_epoch_end","text":"</> Called when the epoch ends. method","title":"pytorch_lightning.callbacks.base.Callback.on_epoch_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_batch_start","text":"</> Called when the training batch begins. method","title":"pytorch_lightning.callbacks.base.Callback.on_batch_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_validation_batch_start","text":"</> Called when the validation batch begins. method","title":"pytorch_lightning.callbacks.base.Callback.on_validation_batch_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_test_batch_start","text":"</> Called when the test batch begins. method","title":"pytorch_lightning.callbacks.base.Callback.on_test_batch_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_batch_end","text":"</> Called when the training batch ends. method","title":"pytorch_lightning.callbacks.base.Callback.on_batch_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_pretrain_routine_start","text":"</> Called when the pretrain routine begins. method","title":"pytorch_lightning.callbacks.base.Callback.on_pretrain_routine_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_pretrain_routine_end","text":"</> Called when the pretrain routine ends. method","title":"pytorch_lightning.callbacks.base.Callback.on_pretrain_routine_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_keyboard_interrupt","text":"</> Called when the training is interrupted by KeyboardInterrupt. method","title":"pytorch_lightning.callbacks.base.Callback.on_keyboard_interrupt"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_save_checkpoint","text":"</> Called when saving a model checkpoint, use to persist state. method","title":"pytorch_lightning.callbacks.base.Callback.on_save_checkpoint"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_load_checkpoint","text":"</> Called when loading a model checkpoint, use to reload state. method","title":"pytorch_lightning.callbacks.base.Callback.on_load_checkpoint"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_after_backward","text":"</> Called after loss.backward() and before optimizers do anything. method","title":"pytorch_lightning.callbacks.base.Callback.on_after_backward"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksbasecallbackon_before_zero_grad","text":"</> Called after optimizer.step() and before optimizer.zero_grad(). method","title":"pytorch_lightning.callbacks.base.Callback.on_before_zero_grad"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbarbaseon_init_end","text":"</> Called when the trainer initialization ends, model has not yet been set. method","title":"pytorch_lightning.callbacks.progress.ProgressBarBase.on_init_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbardisable","text":"</> You should provide a way to disable the progress bar. The :class: ~pytorch_lightning.trainer.trainer.Trainer will call this to disable the output on processes that have a rank different from 0, e.g., in multi-node training. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.disable"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbarenable","text":"</> You should provide a way to enable the progress bar. The :class: ~pytorch_lightning.trainer.trainer.Trainer will call this in e.g. pre-training routines like the :ref: learning rate finder <lr_finder> to temporarily enable and disable the main progress bar. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.enable"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbarinit_sanity_tqdm","text":"</> Override this to customize the tqdm bar for the validation sanity run. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.init_sanity_tqdm"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbarinit_train_tqdm","text":"</> Override this to customize the tqdm bar for training. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.init_train_tqdm"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbarinit_validation_tqdm","text":"</> Override this to customize the tqdm bar for validation. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.init_validation_tqdm"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbarinit_test_tqdm","text":"</> Override this to customize the tqdm bar for testing. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.init_test_tqdm"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_sanity_check_start","text":"</> Called when the validation sanity check starts. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_sanity_check_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_sanity_check_end","text":"</> Called when the validation sanity check ends. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_sanity_check_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_train_start","text":"</> Called when the train begins. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_train_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_train_batch_end","text":"</> Called when the train batch ends. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_train_batch_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_validation_start","text":"</> Called when the validation loop begins. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_validation_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_validation_batch_end","text":"</> Called when the validation batch ends. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_validation_batch_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_validation_end","text":"</> Called when the validation loop ends. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_validation_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_train_end","text":"</> Called when the train ends. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_train_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_test_start","text":"</> Called when the test begins. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_test_start"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_test_batch_end","text":"</> Called when the test batch ends. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_test_batch_end"},{"location":"api/plkit.trainer/#pytorch_lightningcallbacksprogressprogressbaron_test_end","text":"</> Called when the test ends. method","title":"pytorch_lightning.callbacks.progress.ProgressBar.on_test_end"},{"location":"api/plkit.trainer/#plkittrainerprogressbaron_epoch_start","text":"</> Try to align the epoch number class","title":"plkit.trainer.ProgressBar.on_epoch_start"},{"location":"api/plkit.trainer/#plkittrainertrainer","text":"</> Bases pytorch_lightning.trainer.trainer.Trainer pytorch_lightning.trainer.properties.TrainerProperties pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin pytorch_lightning.trainer.logging.TrainerLoggingMixin pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin abc.ABC pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes The Trainner class from_config (aka from_dict ) added as classmethod to instantiate trainer from configuration dictionaries. Attributes checkpoint_callback (ModelCheckpoint, optional) \u2014 The first checkpoint callback in the Trainer.callbacks list, or None if no checkpoint callbacks exist. </> checkpoint_callbacks (list of ModelCheckpoint) \u2014 A list of all instances of ModelCheckpoint found in the Trainer.callbacks list. </> default_root_dir (str) \u2014 The default location to save artifacts of loggers, checkpoints etc. It is used as a fallback if logger or checkpoint callback do not define specific save paths. </> disable_validation (bool) \u2014 Check if validation is disabled during training. </> enable_validation (bool) \u2014 Check if we should run validation during training. </> progress_bar_dict (dict) \u2014 Format progress bar metrics. </> weights_save_path (str) \u2014 The default root location to save weights (checkpoints), e.g., when the :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint does not define a file path. </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods available_plugins ( ) \u2014 List of all available plugins that can be string arguments to the trainer. Returns: List of all available plugins that are supported as string arguments. </> fit ( *args , **kwargs ) \u2014 Train and validate the model </> from_config ( config , **kwargs ) \u2014 Create an instance from CLI arguments. </> from_config ( config , **kwargs ) \u2014 Create an instance from CLI arguments. </> get_deprecated_arg_names ( ) (list) \u2014 Returns a list with deprecated Trainer arguments. </> on_after_backward ( ) \u2014 Called after loss.backward() and before optimizers do anything. </> on_batch_end ( ) \u2014 Called when the training batch ends. </> on_batch_start ( ) \u2014 Called when the training batch begins. </> on_before_zero_grad ( optimizer ) \u2014 Called after optimizer.step() and before optimizer.zero_grad(). </> on_epoch_end ( ) \u2014 Called when the epoch ends. </> on_epoch_start ( ) \u2014 Called when the epoch begins. </> on_fit_end ( ) \u2014 Called when the trainer initialization begins, model has not yet been set. </> on_fit_start ( ) \u2014 Called when the trainer initialization begins, model has not yet been set. </> on_init_end ( ) \u2014 Called when the trainer initialization ends, model has not yet been set. </> on_init_start ( ) \u2014 Called when the trainer initialization begins, model has not yet been set. </> on_keyboard_interrupt ( ) \u2014 Called when the training is interrupted by KeyboardInterrupt. </> on_load_checkpoint ( checkpoint ) \u2014 Called when loading a model checkpoint. </> on_pretrain_routine_end ( model ) \u2014 Called when the train ends. </> on_pretrain_routine_start ( model ) \u2014 Called when the train begins. </> on_sanity_check_end ( ) \u2014 Called when the validation sanity check ends. </> on_sanity_check_start ( ) \u2014 Called when the validation sanity check starts. </> on_save_checkpoint ( ) \u2014 Called when saving a model checkpoint. </> on_test_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the test batch ends. </> on_test_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called when the test batch begins. </> on_test_end ( ) \u2014 Called when the test ends. </> on_test_epoch_end ( ) \u2014 Called when the epoch ends. </> on_test_epoch_start ( ) \u2014 Called when the epoch begins. </> on_test_start ( ) \u2014 Called when the test begins. </> on_train_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the training batch ends. </> on_train_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called when the training batch begins. </> on_train_end ( ) \u2014 Called when the train ends. </> on_train_epoch_end ( outputs ) \u2014 Called when the epoch ends. </> on_train_epoch_start ( ) \u2014 Called when the epoch begins. </> on_train_start ( ) \u2014 Called when the train begins. </> on_validation_batch_end ( outputs , batch , batch_idx , dataloader_idx ) \u2014 Called when the validation batch ends. </> on_validation_batch_start ( batch , batch_idx , dataloader_idx ) \u2014 Called when the validation batch begins. </> on_validation_end ( ) \u2014 Called when the validation loop ends. </> on_validation_epoch_end ( ) \u2014 Called when the epoch ends. </> on_validation_epoch_start ( ) \u2014 Called when the epoch begins. </> on_validation_start ( ) \u2014 Called when the validation loop begins. </> process_dict_result ( output , train ) \u2014 Reduces output according to the training mode. </> request_dataloader ( dataloader_fx ) (DataLoader) \u2014 Handles downloading data in the GPU or TPU case. </> reset_test_dataloader ( model ) \u2014 Resets the validation dataloader and determines the number of batches. </> reset_train_dataloader ( model ) \u2014 Resets the train dataloader and initialises required variables (number of batches, when to validate, etc.). </> reset_val_dataloader ( model ) \u2014 Resets the validation dataloader and determines the number of batches. </> setup ( model , stage ) \u2014 Called in the beginning of fit and test </> teardown ( stage ) \u2014 Called at the end of fit and test </> test ( *args , **kwargs ) \u2014 Test the model </> tune ( model , train_dataloader , val_dataloaders , datamodule ) \u2014 Runs routines to tune hyperparameters before training. </> class","title":"plkit.trainer.Trainer"},{"location":"api/plkit.trainer/#abcabcmeta","text":"</> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method","title":"abc.ABCMeta"},{"location":"api/plkit.trainer/#pytorch_lightningtrainerdata_loadingtrainerdataloadingmixinreset_train_dataloader","text":"</> Resets the train dataloader and initialises required variables (number of batches, when to validate, etc.). Parameters model (LightningModule) \u2014 The current LightningModule method","title":"pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin.reset_train_dataloader"},{"location":"api/plkit.trainer/#pytorch_lightningtrainerdata_loadingtrainerdataloadingmixinreset_val_dataloader","text":"</> Resets the validation dataloader and determines the number of batches. Parameters model (LightningModule) \u2014 The current LightningModule method","title":"pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin.reset_val_dataloader"},{"location":"api/plkit.trainer/#pytorch_lightningtrainerdata_loadingtrainerdataloadingmixinreset_test_dataloader","text":"</> Resets the validation dataloader and determines the number of batches. Parameters model \u2014 The current LightningModule method","title":"pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin.reset_test_dataloader"},{"location":"api/plkit.trainer/#pytorch_lightningtrainerdata_loadingtrainerdataloadingmixinrequest_dataloader","text":"</> Handles downloading data in the GPU or TPU case. Parameters dataloader_fx (callable) \u2014 The bound dataloader getter Returns (DataLoader) The dataloader method","title":"pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin.request_dataloader"},{"location":"api/plkit.trainer/#pytorch_lightningtrainerloggingtrainerloggingmixinprocess_dict_result","text":"</> Reduces output according to the training mode. Separates loss from logging and progress bar metrics method","title":"pytorch_lightning.trainer.logging.TrainerLoggingMixin.process_dict_result"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinsetup","text":"</> Called in the beginning of fit and test method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.setup"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinteardown","text":"</> Called at the end of fit and test method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.teardown"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_init_start","text":"</> Called when the trainer initialization begins, model has not yet been set. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_init_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_init_end","text":"</> Called when the trainer initialization ends, model has not yet been set. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_init_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_fit_start","text":"</> Called when the trainer initialization begins, model has not yet been set. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_fit_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_fit_end","text":"</> Called when the trainer initialization begins, model has not yet been set. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_fit_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_sanity_check_start","text":"</> Called when the validation sanity check starts. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_sanity_check_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_sanity_check_end","text":"</> Called when the validation sanity check ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_sanity_check_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_train_epoch_start","text":"</> Called when the epoch begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_train_epoch_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_train_epoch_end","text":"</> Called when the epoch ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_train_epoch_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_validation_epoch_start","text":"</> Called when the epoch begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_validation_epoch_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_validation_epoch_end","text":"</> Called when the epoch ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_validation_epoch_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_test_epoch_start","text":"</> Called when the epoch begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_test_epoch_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_test_epoch_end","text":"</> Called when the epoch ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_test_epoch_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_epoch_start","text":"</> Called when the epoch begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_epoch_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_epoch_end","text":"</> Called when the epoch ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_epoch_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_train_start","text":"</> Called when the train begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_train_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_train_end","text":"</> Called when the train ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_train_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_pretrain_routine_start","text":"</> Called when the train begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_pretrain_routine_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_pretrain_routine_end","text":"</> Called when the train ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_pretrain_routine_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_batch_start","text":"</> Called when the training batch begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_batch_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_batch_end","text":"</> Called when the training batch ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_batch_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_train_batch_start","text":"</> Called when the training batch begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_train_batch_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_train_batch_end","text":"</> Called when the training batch ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_train_batch_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_validation_batch_start","text":"</> Called when the validation batch begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_validation_batch_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_validation_batch_end","text":"</> Called when the validation batch ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_validation_batch_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_test_batch_start","text":"</> Called when the test batch begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_test_batch_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_test_batch_end","text":"</> Called when the test batch ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_test_batch_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_validation_start","text":"</> Called when the validation loop begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_validation_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_validation_end","text":"</> Called when the validation loop ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_validation_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_test_start","text":"</> Called when the test begins. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_test_start"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_test_end","text":"</> Called when the test ends. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_test_end"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_keyboard_interrupt","text":"</> Called when the training is interrupted by KeyboardInterrupt. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_keyboard_interrupt"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_save_checkpoint","text":"</> Called when saving a model checkpoint. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_save_checkpoint"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_load_checkpoint","text":"</> Called when loading a model checkpoint. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_load_checkpoint"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_after_backward","text":"</> Called after loss.backward() and before optimizers do anything. method","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_after_backward"},{"location":"api/plkit.trainer/#pytorch_lightningtrainercallback_hooktrainercallbackhookmixinon_before_zero_grad","text":"</> Called after optimizer.step() and before optimizer.zero_grad(). classmethod","title":"pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin.on_before_zero_grad"},{"location":"api/plkit.trainer/#pytorch_lightningtrainerpropertiestrainerpropertiesget_deprecated_arg_names","text":"</> Returns a list with deprecated Trainer arguments. method","title":"pytorch_lightning.trainer.properties.TrainerProperties.get_deprecated_arg_names"},{"location":"api/plkit.trainer/#pytorch_lightningtrainertrainertrainertune","text":"</> Runs routines to tune hyperparameters before training. Parameters model (LightningModule) \u2014 Model to tune. train_dataloader (DataLoader, optional) \u2014 A Pytorch DataLoader with training samples. If the model has a predefined train_dataloader method this will be skipped. val_dataloaders (Union(dataloader, list of dataloader, nonetype), optional) \u2014 Either a single Pytorch Dataloader or a list of them, specifying validation samples. If the model has a predefined val_dataloaders method this will be skipped datamodule (LightningDataModule, optional) \u2014 A instance of :class: LightningDataModule . staticmethod","title":"pytorch_lightning.trainer.trainer.Trainer.tune"},{"location":"api/plkit.trainer/#pytorch_lightningtrainertrainertraineravailable_plugins","text":"</> List of all available plugins that can be string arguments to the trainer. Returns: List of all available plugins that are supported as string arguments. classmethod","title":"pytorch_lightning.trainer.trainer.Trainer.available_plugins"},{"location":"api/plkit.trainer/#plkittrainertrainerfrom_config","text":"</> Create an instance from CLI arguments. Examples >>> config = { 'my_custom_arg' : 'something' } >>> trainer = Trainer . from_dict ( config , logger = False ) Parameters config \u2014 The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: Trainer . **kwargs \u2014 Additional keyword arguments that may override ones in the parser or namespace. These must be valid Trainer arguments. classmethod","title":"plkit.trainer.Trainer.from_config"},{"location":"api/plkit.trainer/#plkittrainertrainerfrom_config_1","text":"</> Create an instance from CLI arguments. Examples >>> config = { 'my_custom_arg' : 'something' } >>> trainer = Trainer . from_dict ( config , logger = False ) Parameters config \u2014 The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: Trainer . **kwargs \u2014 Additional keyword arguments that may override ones in the parser or namespace. These must be valid Trainer arguments. method","title":"plkit.trainer.Trainer.from_config"},{"location":"api/plkit.trainer/#plkittrainertrainerfit","text":"</> Train and validate the model method","title":"plkit.trainer.Trainer.fit"},{"location":"api/plkit.trainer/#plkittrainertrainertest","text":"</> Test the model","title":"plkit.trainer.Trainer.test"},{"location":"api/plkit.utils/","text":"module plkit . utils </> Utility functions for plkit Functions capture_stderr ( ) \u2014 Capture the stderr </> capture_stdout ( ) \u2014 Capture the stdout </> check_config ( config , item , how , msg ) \u2014 Check configuration items </> collapse_suggest_config ( config ) (dict) \u2014 Use the default value of OptunaSuggest for config items. So that the configs can be used in the case that optuna is opted out. </> log_config ( config , title , items_per_row ) \u2014 Log the configurations in a table in terminal </> normalize_tvt_ratio ( tvt_ratio ) (int or float, list of int or float, list of int or float), optional \u2014 Normalize the train-val-test data ratio into a format of (.7, [.1, .1], [.05, .05]). </> output_to_logging ( stdout_level , stderr_level ) \u2014 Capture the stdout or stderr to logging </> plkit_seed_everything ( config ) \u2014 Try to seed everything and set deterministic to True if seed in config has been set </> warning_to_logging ( ) \u2014 Patch the warning message formatting to only show the message </> function plkit.utils . check_config ( config , item , how=<function <lambda> at 0x7f7e732ea040> , msg='Configuration item {key} is required.' ) </> Check configuration items Parameters config (dict) \u2014 The configuration dictionary item (str) \u2014 The configuration key to check how (callable) \u2014 How to check. Return False to fail the check. msg (str) \u2014 The message to show in the exception. {key} is available to refer to the key checked. Raises PlkitConfigException \u2014 When the check fails function plkit.utils . collapse_suggest_config ( config ) </> Use the default value of OptunaSuggest for config items. So that the configs can be used in the case that optuna is opted out. Parameters config (dict) \u2014 The configuration dictionary Returns (dict) The collapsed configuration function plkit.utils . normalize_tvt_ratio ( tvt_ratio ) </> Normalize the train-val-test data ratio into a format of (.7, [.1, .1], [.05, .05]). For config.data_tvt , the first element is required. If val or test ratios are not provided, it will be filled with None All numbers could be absolute numbers (>1) or ratios (<=1) Parameters tvt_ratio (Union(int, float, iterable of int or float, nonetype)) \u2014 The train-val-test ratio Returns (int or float, list of int or float, list of int or float), optional The normalized ratios Raises PlkitConfigException \u2014 When the passed-in tvt_ratio is in malformat generator plkit.utils . warning_to_logging ( ) </> Patch the warning message formatting to only show the message generator plkit.utils . capture_stdout ( ) </> Capture the stdout generator plkit.utils . capture_stderr ( ) </> Capture the stderr generator plkit.utils . output_to_logging ( stdout_level='info' , stderr_level='error' ) </> Capture the stdout or stderr to logging function plkit.utils . log_config ( config , title='Configurations' , items_per_row=1 ) </> Log the configurations in a table in terminal Parameters config (dict) \u2014 The configuration dictionary title (str) \u2014 The title of the table items_per_row (int) \u2014 The number of items to print per row function plkit.utils . plkit_seed_everything ( config ) </> Try to seed everything and set deterministic to True if seed in config has been set Parameters config (FrozenDiot) \u2014 The configurations","title":"plkit.utils"},{"location":"api/plkit.utils/#plkitutils","text":"</> Utility functions for plkit Functions capture_stderr ( ) \u2014 Capture the stderr </> capture_stdout ( ) \u2014 Capture the stdout </> check_config ( config , item , how , msg ) \u2014 Check configuration items </> collapse_suggest_config ( config ) (dict) \u2014 Use the default value of OptunaSuggest for config items. So that the configs can be used in the case that optuna is opted out. </> log_config ( config , title , items_per_row ) \u2014 Log the configurations in a table in terminal </> normalize_tvt_ratio ( tvt_ratio ) (int or float, list of int or float, list of int or float), optional \u2014 Normalize the train-val-test data ratio into a format of (.7, [.1, .1], [.05, .05]). </> output_to_logging ( stdout_level , stderr_level ) \u2014 Capture the stdout or stderr to logging </> plkit_seed_everything ( config ) \u2014 Try to seed everything and set deterministic to True if seed in config has been set </> warning_to_logging ( ) \u2014 Patch the warning message formatting to only show the message </> function","title":"plkit.utils"},{"location":"api/plkit.utils/#plkitutilscheck_config","text":"</> Check configuration items Parameters config (dict) \u2014 The configuration dictionary item (str) \u2014 The configuration key to check how (callable) \u2014 How to check. Return False to fail the check. msg (str) \u2014 The message to show in the exception. {key} is available to refer to the key checked. Raises PlkitConfigException \u2014 When the check fails function","title":"plkit.utils.check_config"},{"location":"api/plkit.utils/#plkitutilscollapse_suggest_config","text":"</> Use the default value of OptunaSuggest for config items. So that the configs can be used in the case that optuna is opted out. Parameters config (dict) \u2014 The configuration dictionary Returns (dict) The collapsed configuration function","title":"plkit.utils.collapse_suggest_config"},{"location":"api/plkit.utils/#plkitutilsnormalize_tvt_ratio","text":"</> Normalize the train-val-test data ratio into a format of (.7, [.1, .1], [.05, .05]). For config.data_tvt , the first element is required. If val or test ratios are not provided, it will be filled with None All numbers could be absolute numbers (>1) or ratios (<=1) Parameters tvt_ratio (Union(int, float, iterable of int or float, nonetype)) \u2014 The train-val-test ratio Returns (int or float, list of int or float, list of int or float), optional The normalized ratios Raises PlkitConfigException \u2014 When the passed-in tvt_ratio is in malformat generator","title":"plkit.utils.normalize_tvt_ratio"},{"location":"api/plkit.utils/#plkitutilswarning_to_logging","text":"</> Patch the warning message formatting to only show the message generator","title":"plkit.utils.warning_to_logging"},{"location":"api/plkit.utils/#plkitutilscapture_stdout","text":"</> Capture the stdout generator","title":"plkit.utils.capture_stdout"},{"location":"api/plkit.utils/#plkitutilscapture_stderr","text":"</> Capture the stderr generator","title":"plkit.utils.capture_stderr"},{"location":"api/plkit.utils/#plkitutilsoutput_to_logging","text":"</> Capture the stdout or stderr to logging function","title":"plkit.utils.output_to_logging"},{"location":"api/plkit.utils/#plkitutilslog_config","text":"</> Log the configurations in a table in terminal Parameters config (dict) \u2014 The configuration dictionary title (str) \u2014 The title of the table items_per_row (int) \u2014 The number of items to print per row function","title":"plkit.utils.log_config"},{"location":"api/plkit.utils/#plkitutilsplkit_seed_everything","text":"</> Try to seed everything and set deterministic to True if seed in config has been set Parameters config (FrozenDiot) \u2014 The configurations","title":"plkit.utils.plkit_seed_everything"},{"location":"api/source/plkit.data/","text":"SOURCE CODE plkit. data DOCS \"\"\"Data module for plkit\"\"\" from types import GeneratorType from typing import Any , Dict , Iterable , Iterator , List , Optional , Tuple , Union from itertools import islice from diot import FrozenDiot from pytorch_lightning import LightningDataModule from torch.utils.data import ( DataLoader , Dataset as TorchDataset , IterableDataset as TorchIterableDataset , random_split ) from .exceptions import PlkitDataException from .utils import ( normalize_tvt_ratio , check_config , logger ) # pylint: disable=unused-argument # The ids or keys for the data DatasetType = Union [ TorchDataset , TorchIterableDataset ] class Dataset ( TorchDataset ): DOCS \"\"\"The dataset that used internally by Data class Examples: >>> ds = Dataset(data=[('a', 'x'), ('b', 'y'), ('c', 'z')], ids=[1, 2]) >>> len(ds) == 2 >>> ds[0] == ('b', 'y') >>> ds[1] == ('c', 'z') >>> # The features are what you get by >>> # x, y = batch Args: data: The data for the dataset. It could be a tuple of features. Each one should be an iterable, which could be accessed by index ids: The ids or keys of the data, which should be in the same order of each feature in the iterable. \"\"\" def __init__ ( self , data : Iterable [ tuple ], ids : Optional [ List [ int ]] = None ) -> None : self . data = data self . ids = ids or list ( range ( len ( data ))) def __len__ ( self ) -> int : return len ( self . ids ) def __getitem__ ( self , idx : Union [ int , str ]) -> Tuple [ Any ]: data_id = self . ids [ idx ] return self . data [ data_id ] class IterDataset ( TorchIterableDataset ): DOCS \"\"\"Iterable dataset The iterable dataset where each feature of the data is an iterable Examples: >>> feat1 = (x for x in range(10) >>> feat2 = (x for x in range(10) >>> ds = IterDataset(zip(feat1, feat2), ids=[4,3]) >>> next(ds) == (0, 0) Args: data: a tuple of iterable features length: The length of the iterables \"\"\" # pylint: disable=abstract-method def __init__ ( self , data : Iterable [ tuple ], length : int ) -> None : self . data = data self . length = length def __iter__ ( self ) -> Iterable [ Any ]: return iter ( self . data ) class DataModule ( LightningDataModule ): DOCS \"\"\"Data module for plkit\"\"\" def __init__ ( self , train_transforms = None , val_transforms = None , test_transforms = None , config : Optional [ FrozenDiot ] = None ) -> None : super () . __init__ ( train_transforms = train_transforms , val_transforms = val_transforms , test_transforms = test_transforms ) self . config = config or FrozenDiot () check_config ( self . config , 'batch_size' ) self . num_workers = self . config . get ( 'data_num_workers' , 0 ) self . tvt_ratio = normalize_tvt_ratio ( self . config . get ( 'data_tvt' )) self . data = None self . splits = None self . _length = None @property DOCS def length ( self ) -> int : \"\"\"The length of the data This is required when `self.data_reader()` yields (it is a generator) Returns: The length of the data. \"\"\" return self . _length def data_reader ( self ) -> Union [ Iterable [ Any ], Tuple [ Iterator [ Any ]]]: DOCS \"\"\"Read the data Returns: A tuple of iterables of features. Or it yields the following Yields: An iterable of tuple of features. In such a case, self.length property is required to be defined. \"\"\" raise NotImplementedError # pragma: no cover def _split_data_generator ( self , data : Iterable [ tuple ] ) -> Dict [ str , Union [ TorchIterableDataset , List [ TorchIterableDataset ]]]: ret = {} is_ratio = self . tvt_ratio [ 0 ] <= 1.0 if is_ratio and self . length is None : raise PlkitDataException ( 'Got generator from `data_reader` and ratios from ' '`config.data_tvt`, `self.length` should be recorded ' 'in `data_reader`.' ) # split using islice start = 0 train_len = ( round ( self . tvt_ratio [ 0 ] * float ( self . length )) if is_ratio else self . tvt_ratio [ 0 ]) ret [ 'train' ] = IterDataset ( islice ( data , start , train_len ), train_len ) start += train_len if self . tvt_ratio [ 1 ]: ret [ 'val' ] = [] for val_ratio in self . tvt_ratio [ 1 ]: val_len = ( round ( val_ratio * float ( self . length )) if is_ratio else val_ratio ) ret [ 'val' ] . append ( IterDataset ( islice ( data , start , start + val_len ), val_len )) start += val_len if self . tvt_ratio [ 2 ]: ret [ 'test' ] = [] for test_ratio in self . tvt_ratio [ 2 ]: test_len = ( round ( test_ratio * float ( self . length )) if is_ratio else test_ratio ) ret [ 'test' ] . append ( IterDataset ( islice ( data , start , start + test_len ), test_len )) start += test_len return ret def _split_data_list ( self , data : List [ Any ] ) -> Dict [ str , Union [ IterDataset , List [ IterDataset ]]]: ret = {} is_ratio = self . tvt_ratio [ 0 ] <= 1.0 self . _length = len ( data ) all_ids = range ( self . length ) train_len = ( round ( self . tvt_ratio [ 0 ] * float ( self . length )) if is_ratio else self . tvt_ratio [ 0 ]) train_ids , rest_ids = random_split ( all_ids , [ train_len , len ( all_ids ) - train_len ] ) ret [ 'train' ] = Dataset ( data , train_ids ) if self . tvt_ratio [ 1 ]: ret [ 'val' ] = [] for val_ratio in self . tvt_ratio [ 1 ]: val_len = ( round ( val_ratio * float ( self . length )) if is_ratio else val_ratio ) val_ids , rest_ids = random_split ( rest_ids , [ val_len , len ( rest_ids ) - val_len ] ) ret [ 'val' ] . append ( Dataset ( data , val_ids )) if self . tvt_ratio [ 2 ]: ret [ 'test' ] = [] for test_ratio in self . tvt_ratio [ 2 ]: test_len = ( round ( test_ratio * float ( self . length )) if is_ratio else test_ratio ) test_ids , rest_ids = random_split ( rest_ids , [ test_len , len ( rest_ids ) - test_len ] ) ret [ 'test' ] . append ( Dataset ( data , test_ids )) return ret def data_splits ( # pylint: disable=unused-argument DOCS self , data : Optional [ Iterable [ tuple ]] = None , stage : Optional [ str ] = None ) -> Dict [ str , Union [ DatasetType , List [ DatasetType ]]]: \"\"\"Split data from data_source for each dataloader Args: data: The data read by self.data_reader() stage: The stage argument same as the one from `LightningDataModule.setup(...)` Returns: A dictionary with keys `train`, `val` and `test`, and values a Dataset or an IterDataset (config.data_tvt will be ignored) Or if config.data_tvt is specified, one could just return an iterable of features, then the dataset will be automatically split by config.data_tvt \"\"\" if not self . tvt_ratio : return None data = data or self . data if isinstance ( data , GeneratorType ): return self . _split_data_generator ( data ) return self . _split_data_list ( data ) def prepare_data ( self , * args , ** kwargs ) -> None : DOCS \"\"\"Prepare data\"\"\" logger . info ( 'Reading data ...' ) self . data = self . data_reader () def setup ( self , stage : Optional [ str ] = None ) -> None : DOCS \"\"\"Setup data\"\"\" if stage == 'fit' : # Only do it once. # If you want it to be separate # redefine this method logger . info ( 'Splitting data ...' ) self . splits = self . data_splits ( self . data , stage ) if not self . tvt_ratio and not self . splits : raise PlkitDataException ( 'No train-val-test ratio (data-tvt) specified in ' 'configuration, then `data_splits` method should be ' 'implemented for DataModule.' ) def train_dataloader ( self , * args , ** kwargs ) -> DataLoader : DOCS \"\"\"Train data loaders\"\"\" if 'train' not in self . splits : return None return DataLoader ( self . splits [ 'train' ], batch_size = self . config . batch_size , num_workers = self . num_workers ) def val_dataloader ( self , DOCS * args , ** kwargs ) -> Union [ DataLoader , List [ DataLoader ]]: \"\"\"Validation data loaders\"\"\" if 'val' not in self . splits : return None ret = [] for val_data in self . splits [ 'val' ]: ret . append ( DataLoader ( val_data , batch_size = self . config . batch_size , num_workers = self . num_workers )) return ret [ 0 ] if len ( ret ) == 1 else ret def test_dataloader ( self , DOCS * args , ** kwargs ) -> Union [ DataLoader , List [ DataLoader ]]: \"\"\"Test data loaders\"\"\" if 'test' not in self . splits : return None ret = [] for test_data in self . splits [ 'test' ]: ret . append ( DataLoader ( test_data , batch_size = self . config . batch_size , num_workers = self . num_workers )) return ret [ 0 ] if len ( ret ) == 1 else ret","title":"plkit.data"},{"location":"api/source/plkit.exceptions/","text":"SOURCE CODE plkit. exceptions DOCS \"\"\"Exceptions for plkit\"\"\" class PlkitException ( Exception ): DOCS \"\"\"Base exception class for plkit\"\"\" class PlkitDataException ( PlkitException ): DOCS \"\"\"Something wrong when preparing data\"\"\" class PlkitConfigException ( PlkitException ): DOCS \"\"\"When certain config items are missing\"\"\"","title":"plkit.exceptions"},{"location":"api/source/plkit/","text":"SOURCE CODE plkit DOCS \"\"\"Superset of pytorch-lightning\"\"\" from typing import Any , Dict , Optional , Type from .data import DataModule from .module import Module from .trainer import Trainer from .optuna import Optuna , OptunaSuggest from .runner import Runner , LocalRunner , SGERunner from .utils import logger __version__ = \"0.0.10\" def run ( config : Dict [ str , Any ], DOCS data_class : Type [ DataModule ], model_class : Type [ Module ], optuna : Optional [ Optuna ] = None , runner : Optional [ Runner ] = None ) -> Trainer : \"\"\"Run the pipeline by give configuration, model_class, data_class, optuna and runner Args: config: A dictionary of configuration, must have following items: - batch_size: The batch size - num_classes: The number of classes for classification 1 means regression data_class: The data class subclassed from `Data` model_class: The model class subclassed from `Module` optuna: The optuna object runner: The runner object Returns: The trainer object \"\"\" runner = runner or LocalRunner () return runner . run ( config , data_class , model_class , optuna )","title":"plkit"},{"location":"api/source/plkit.module/","text":"SOURCE CODE plkit. module DOCS \"\"\"The core base module class based on pytorch_lightning.LightningModule\"\"\" import torch from torch import nn from pytorch_lightning import LightningModule from .utils import collapse_suggest_config class Module ( LightningModule ): # pylint: disable=too-many-ancestors DOCS \"\"\"The Module class `on_epoch_end` is added to print a newline to keep the progress bar and the stats on it for each epoch. If you don't want this, just overwrite it with: >>> def on_epoch_end(self): >>> pass If you have other stuff to do in `on_epoch_end`, make sure to you call: >>> super().on_epoch_end() You may or may not need to write `loss_function`, as it will be inferred from config item `loss` and `num_classes`. Basically, `MSELoss` will be used for regression and `CrossEntropyLoss` for classification. `measure` added for convinience to get some metrics between logits and targets. Args: config: The configuration dictionary Attributes: Apart from attributes of `LightningModule`, following attributes added: config: The configs optim: The optimizer name. currently only `adam` and `sgd` are supported. With this, of course you can, but you don't need to write `configure_optimizers`. num_classes: Number of classes to predict. 1 for regression _loss_func: The loss function \"\"\" def __init__ ( self , config ): super () . __init__ () self . config = collapse_suggest_config ( config ) self . optim = self . config . get ( 'optim' , 'adam' ) self . num_classes = self . config . get ( 'num_classes' ) # We may run test only without measurement. # if not self.num_classes: # raise ValueError('We need `num_classes` from config or passed in ' # 'explictly to check final logits size.') loss = self . config . get ( 'loss' , 'auto' ) if loss == 'auto' : if self . num_classes == 1 : # regression self . _loss_func = nn . MSELoss () else : self . _loss_func = nn . CrossEntropyLoss () else : self . _loss_func = loss # the hyperparameters to be logged to tensorboard self . hparams = {} def on_epoch_end ( self ): DOCS \"\"\"Keep the epoch progress bar This is not documented but working.\"\"\" print () def loss_function ( self , logits , labels ): DOCS \"\"\"Calculate the loss\"\"\" return self . _loss_func ( logits , labels ) # pylint: disable=inconsistent-return-statements def configure_optimizers ( self ): DOCS \"\"\"Configure the optimizers\"\"\" if self . optim == 'adam' : return torch . optim . Adam ( self . parameters (), lr = self . config . get ( 'learning_rate' , 1e-3 )) if self . optim == 'sgd' : return torch . optim . SGD ( self . parameters (), lr = self . config . get ( 'learning_rate' , 1e-3 ), momentum = self . config . get ( 'momentum' , . 9 )) # more to support","title":"plkit.module"},{"location":"api/source/plkit.optuna/","text":"SOURCE CODE plkit. optuna DOCS \"\"\"Optuna wrapper for plkit\"\"\" from diot import FrozenDiot from torch import Tensor from pytorch_lightning.callbacks import ModelCheckpoint import optuna from .trainer import Trainer from .utils import log_config , logger , plkit_seed_everything # supress optuna logging optuna . logging . _get_library_root_logger () . handlers . clear () class OptunaSuggest : DOCS \"\"\"Optuna suggests for configuration items Args: default (any): The default value, which the value will be collapsed to when optuna is opted out. So that you don't have to change your code if you don't run optuna. suggtype (str): The type of suggestion For example, `cat` refers to `trial.suggest_categorical` The mappings are: cat -> 'suggest_categorical', categorical -> 'suggest_categorical', distuni -> 'suggest_discrete_uniform', dist_uni -> 'suggest_discrete_uniform', discrete_uniform -> 'suggest_discrete_uniform', float -> 'suggest_float', int -> 'suggest_int', loguni -> 'suggest_loguniform', log_uni -> 'suggest_loguniform', uni -> 'suggest_uniform' *args: The args used in `trial.suggest_xxx(name, *args, **kwargs)` **kwargs: The kwargs used in `trial.suggest_xxx(name, *args, **kwargs)` Attributes: default (any): The default from Args suggfunc (str): The transformed suggestion name according to `suggtype` args: *args from Args kwargs: **kwargs from Args \"\"\" def __init__ ( self , default , suggtype , * args , ** kwargs ): self . default = default self . suggfunc = dict ( cat = 'suggest_categorical' , categorical = 'suggest_categorical' , distuni = 'suggest_discrete_uniform' , dist_uni = 'suggest_discrete_uniform' , discrete_uniform = 'suggest_discrete_uniform' , float = 'suggest_float' , int = 'suggest_int' , loguni = 'suggest_loguniform' , log_uni = 'suggest_loguniform' , uni = 'suggest_uniform' )[ suggtype ] self . args = args self . kwargs = kwargs def suggest ( self , name , trial ): DOCS \"\"\"Get the suggested value This is used in Optuna class, you don't have to call this Args: name (str): The name of the parameter trial (optuna.Trial): The trial to get the suggested value from Returns: Any: The suggested value \"\"\" return getattr ( trial , self . suggfunc )( name , * self . args , ** self . kwargs ) class Optuna : DOCS \"\"\"The class uses optuna to automate hyperparameter tuning Example: >>> from plkit import Data, Module, Optuna >>> class MyData(Data): >>> ... >>> class MyModel(Module): >>> ... >>> class MyOptuna(Optuna): >>> def suggests(self, config): >>> ... >>> return new_config >>> optuna = MyOptuna('val_loss', 100) >>> optuna.run(config, model_class, data_class) Args: on (str): On which value to optimize. Should be one of the keys of dictionary that is returned from `validation_epoch_end`. `val_acc` or `val_loss` for example. n_trials (int): Number of trials **kwargs: Other keyword arguments for `optuna.create_study` Attributes: on (str): on from Args n_trials (int): n_trials from Args study (optuna.Study): study object created from kwargs trainers (list): list of trainers to keep track of the best one \"\"\" def __init__ ( self , on , n_trials , ** kwargs ): self . on = on self . n_trials = n_trials self . _best_trainer = None self . _best_model = None self . study = optuna . create_study ( ** kwargs ) # trainers, used for retrieve the best one self . trainers = [] def _create_objective ( self , config , data , model_class ): \"\"\"Create objective function for the study to optimize The objective function is built to return the best value from `validation_epoch_end` based on `self.on`. To implement this, a `ModelCheckpoint` callback is used and `best_model_score` is returned from it. Args: config (dict): The configuration dictionary data_class (class): The data class subclassed from `plkit.Data` Note it's the class itself, not instantiated object model_class (class): The model class subclassed from `plkit.Module` Note it's the class itself, not instantiated object Returns: callable: The objective function \"\"\" def _objective ( trial ): logger . info ( '--------------------------------' ) logger . info ( 'Start optuna trial # %s / %s ' , len ( self . trainers ), self . n_trials - 1 ) logger . info ( '--------------------------------' ) suggested = self . suggests ( trial , config ) config_copy = config . copy () with config_copy . thaw (): config_copy . update ( suggested ) log_config ( suggested , \"Tunable parameters\" ) model = model_class ( config_copy ) model . hparams . update ( suggested ) # expose filepath argument? checkpoint_callback = ModelCheckpoint ( monitor = self . on ) trainer = Trainer . from_config ( config_copy , checkpoint_callback = checkpoint_callback ) trainer . fit ( model , data ) best_score = checkpoint_callback . best_model_score self . trainers . append (( checkpoint_callback . best_model_path , config_copy , best_score )) logger . info ( '' ) logger . info ( \"'Optuna': trial # %s done with %s = %s \" \"and parameters: %s \" , trial . number , self . on , best_score , trial . params ) number , score , params = self . _current_best params = { key : val for key , val in params . items () if key in suggested } logger . info ( \"'Optuna': the best is # %s with %s = %s \" \"and parameters: %s \" , number , self . on , score , params ) logger . info ( '' ) return best_score return _objective @property def _current_best ( self ): \"\"\"Get the current best trial number and parameters when the tuning is incomplete\"\"\" func = ( max if self . study . direction == optuna . study . StudyDirection . MAXIMIZE else min ) values = [ trainer [ 2 ] if not isinstance ( trainer [ 2 ], Tensor ) else trainer [ 2 ] . cpu () for trainer in self . trainers ] number = values . index ( func ( values )) trainer = self . trainers [ number ] return number , trainer [ 2 ], trainer [ 1 ] def suggests ( self , trial , conf ): DOCS \"\"\"Collect the hyperparameters from the trial suggestions if any configuration item is an `OptunaSuggest` object Args: trial (optuna.Trial): the trial object conf (dict): The configuration dictionary Returns: dict: A dictionary of suggested parameters \"\"\" return { key : val . suggest ( key , trial ) for key , val in conf . items () if isinstance ( val , OptunaSuggest )} def run ( self , config , data_class , model_class , ** kwargs ): DOCS # pylint: disable=line-too-long \"\"\"Run the optimization The optimization is running on fit of the trainer. If test data is provided. Test will be performed as well. Args: config (dict): The configuation dictionary data_class (class): The data class subclassed from `plkit.Data` Note that this is the class itself, not the instantized object. model_class (class): The data class subclassed from `plkit.Module` Note that this is the class itself, not the instantized object. **kwargs: Other arguments for `study.optimize` other than `func` and `n_trials`. See: https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize \"\"\" # pylint: enable=line-too-long if not isinstance ( config , FrozenDiot ): config = FrozenDiot ( config ) plkit_seed_everything ( config ) data = data_class ( config = config ) objective = self . _create_objective ( config , data , model_class ) self . study . optimize ( objective , self . n_trials , ** kwargs ) self . _best_trainer = Trainer . from_config ( self . trainers [ self . best_trial . number ][ 1 ] ) self . _best_model = model_class . load_from_checkpoint ( self . trainers [ self . best_trial . number ][ 0 ], # https://github.com/PyTorchLightning/pytorch-lightning/issues/2550 config = self . trainers [ self . best_trial . number ][ 1 ] ) if hasattr ( data , 'test_dataloader' ): test_dataloaders = data . test_dataloader () else : # pragma: no cover test_dataloaders = None if test_dataloaders : logger . info ( '' ) logger . info ( '---------------------------------' ) logger . info ( 'Testing using best trial: # %s ' , self . best_trial . number ) logger . info ( '---------------------------------' ) self . best_trainer . test ( self . best_model , test_dataloaders = test_dataloaders ) return self . best_trainer optimize = run @property DOCS def best_params ( self ): \"\"\"The best parameters from the study Returns: dict: A dictionary containing parameters of the best trial. \"\"\" return self . study . best_params @property DOCS def best_trial ( self ): \"\"\"The best trial from the study Returns: optuna.FrozenTrial: A FrozenTrial object of the best trial. \"\"\" return self . study . best_trial @property DOCS def trials ( self ): \"\"\"The trials Returns: list: A list of FrozenTrial objects. \"\"\" return self . study . trials @property DOCS def best_trainer ( self ): \"\"\"Get the best trainer Returns: Trainer: The trainer object of the best trial. \"\"\" return self . _best_trainer @property DOCS def best_model ( self ): \"\"\"Get the model from best trainer Returns: Module: The model of the best trial \"\"\" return self . _best_model","title":"plkit.optuna"},{"location":"api/source/plkit.runner/","text":"SOURCE CODE plkit. runner DOCS \"\"\"Run jobs via non-local runners.\"\"\" import os import sys import uuid from abc import ABC , abstractmethod from typing import Any , Dict , Optional , Type from diot import FrozenDiot import cmdy from .data import DataModule from .module import Module from .optuna import Optuna from .trainer import Trainer from .utils import logger , warning_to_logging , plkit_seed_everything class Runner ( ABC ): DOCS \"\"\"The base class for runner\"\"\" @abstractmethod DOCS def run ( self , config : Dict [ str , Any ], data_class : Type [ DataModule ], model_class : Type [ Module ], optuna : Optional [ Optuna ] = None ) -> Trainer : \"\"\"Run the whole pipeline using the runner Args: config: A dictionary of configuration, must have following items: - batch_size: The batch size - num_classes: The number of classes for classification 1 means regression data_class: The data class subclassed from `Data` model_class: The model class subclassed from `Module` optuna: The optuna object runner: The runner object Returns: The trainer object \"\"\" class LocalRunner ( Runner ): DOCS \"\"\"The local runner for the pipeline\"\"\" def run ( self , DOCS config : Dict [ str , Any ], data_class : Type [ DataModule ], model_class : Type [ Module ], optuna : Optional [ Optuna ] = None ) -> Trainer : \"\"\"Run the pipeline locally\"\"\" if not isinstance ( config , FrozenDiot ): config = FrozenDiot ( config ) if optuna : # pragma: no cover return optuna . run ( config , data_class , model_class ) plkit_seed_everything ( config ) data = data_class ( config = config ) model = model_class ( config ) trainer = Trainer . from_config ( config ) with warning_to_logging (): trainer . fit ( model , data ) if hasattr ( data , 'test_dataloader' ): test_dataloader = data . test_dataloader () else : # pragma: no cover test_dataloader = None if test_dataloader : with warning_to_logging (): trainer . test ( test_dataloaders = test_dataloader ) return trainer class SGERunner ( LocalRunner ): DOCS \"\"\"The SGE runner for the pipeline Args: opts: The options for SGE runner, which will be translated as arguments for `qsub`. For example `opts={'notify': True}` will be translated as `qsub --notify ...` from command line. there are two special options `qsub` and `workdir`. `qsub` specified the path to `qsub` executable and `workdir` specifies a location to save outputs, errors and scripts of each job. Attributes: qsub: The path to qsub executable workdir: The path to the workdir \"\"\" ENV_FLAG_PREFIX = \"PLKIT_SGE_RUNNER_\" def __init__ ( self , * args , ** opts ): self . qsub = opts . pop ( \"qsub\" , \"qsub\" ) # type: str self . workdir = opts . pop ( \"workdir\" , \"./workdir\" ) # type: str os . makedirs ( self . workdir , exist_ok = True ) self . args = args self . opts = opts self . uid = uuid . uuid5 ( uuid . NAMESPACE_DNS , str ( sys . argv )) self . envname = SGERunner . ENV_FLAG_PREFIX + str ( self . uid ) . split ( '-' )[ 0 ] def run ( self , DOCS config : Dict [ str , Any ], data_class : Type [ DataModule ], model_class : Type [ Module ], optuna : Optional [ Optuna ] = None ) -> Trainer : \"\"\"Run the job depending on the env flag\"\"\" if not os . environ . get ( self . envname ): logger . info ( 'Wrapping up the job ...' ) workdir = os . path . join ( self . workdir , f 'plkit- { self . uid } ' ) os . makedirs ( workdir , exist_ok = True ) logger . info ( ' - Workdir: %s ' , workdir ) script = os . path . join ( workdir , 'job.sge.sh' ) logger . info ( ' - Script: %s ' , script ) with open ( script , 'w' ) as fscript : fscript . write ( \"#!/bin/sh \\n\\n \" ) cmd = cmdy . _ ( * sys . argv , _exe = sys . executable ) . h . strcmd fscript . write ( f \" { self . envname } =1 { cmd } \\n \" ) opts = self . opts . copy () opts . setdefault ( 'o' , os . path . join ( workdir , 'job.stdout' )) opts . setdefault ( 'cwd' , True ) opts . setdefault ( 'j' , 'y' ) opts . setdefault ( 'notify' , True ) opts . setdefault ( 'N' , os . path . basename ( workdir )) logger . info ( 'Submitting the job ...' ) cmd = cmdy . qsub ( * self . args , opts , script , cmdy_dupkey = True , cmdy_prefix = '-' , cmdy_exe = self . qsub ) . h () logger . info ( ' - Running: %s ' , cmd . strcmd ) logger . info ( ' - %s ' , cmd . run () . stdout . strip ()) cmdy . touch ( opts [ 'o' ]) logger . info ( 'Streaming content from %s ' , opts [ 'o' ]) cmdy . tail ( f = True , _ = opts [ 'o' ]) . fg () return None # pragma: no cover return super () . run ( config , data_class , model_class , optuna )","title":"plkit.runner"},{"location":"api/source/plkit.trainer/","text":"SOURCE CODE plkit. trainer DOCS \"\"\"Wrapper of the Trainer class\"\"\" import inspect from pytorch_lightning import Trainer as PlTrainer from pytorch_lightning.callbacks.progress import ( ProgressBar as PlProgressBar , ProgressBarBase ) from .utils import collapse_suggest_config , warning_to_logging class ProgressBar ( PlProgressBar ): DOCS \"\"\"Align the Epoch in progress bar\"\"\" def on_epoch_start ( self , trainer , pl_module ): DOCS \"\"\"Try to align the epoch number\"\"\" super () . on_epoch_start ( trainer , pl_module ) if self . max_epochs : nchar = len ( str ( self . max_epochs - 1 )) self . main_progress_bar . set_description ( f 'Epoch { str ( trainer . current_epoch ) . rjust ( nchar ) } ' ) class Trainer ( PlTrainer ): # pylint: disable=too-many-ancestors DOCS \"\"\"The Trainner class `from_config` (aka `from_dict`) added as classmethod to instantiate trainer from configuration dictionaries. \"\"\" # pylint: disable=signature-differs @classmethod DOCS def from_config ( cls , config , ** kwargs ): \"\"\"Create an instance from CLI arguments. Examples: >>> config = {'my_custom_arg': 'something'} >>> trainer = Trainer.from_dict(config, logger=False) Args: config: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class:`Trainer`. **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid Trainer arguments. \"\"\" # we only want to pass in valid Trainer args, # the rest may be user specific valid_kwargs = inspect . signature ( PlTrainer . __init__ ) . parameters trainer_kwargs = dict (( name , config [ name ]) for name in valid_kwargs if name in config ) trainer_kwargs . update ( ** kwargs ) trainer_kwargs = collapse_suggest_config ( trainer_kwargs ) return cls ( ** trainer_kwargs ) from_dict = from_config def __init__ ( self , * args , ** kwargs ): kwargs . setdefault ( 'callbacks' , []) if not any ( isinstance ( callback , ProgressBarBase ) for callback in kwargs [ 'callbacks' ]): pbar_kwargs = { ( 'refresh_rate' if key == 'progress_bar_refresh_rate' else key ) : val for key , val in kwargs . items () if key in ( 'process_position' , 'progress_bar_refresh_rate' ) } pbar = ProgressBar ( ** pbar_kwargs ) pbar . max_epochs = kwargs . get ( 'max_epochs' ) kwargs [ 'callbacks' ] . append ( pbar ) with warning_to_logging (): super () . __init__ ( * args , ** kwargs ) @property DOCS def progress_bar_dict ( self ) -> dict : \"\"\"Format progress bar metrics. \"\"\" metrics = super () . progress_bar_dict metrics = { key : ' %.3f ' % val if isinstance ( val , float ) else val for key , val in metrics . items () } return metrics def fit ( self , * args , ** kwargs ): DOCS \"\"\"Train and validate the model\"\"\" with warning_to_logging (): super () . fit ( * args , ** kwargs ) def test ( self , * args , ** kwargs ): DOCS \"\"\"Test the model\"\"\" with warning_to_logging (): super () . test ( * args , ** kwargs )","title":"plkit.trainer"},{"location":"api/source/plkit.utils/","text":"SOURCE CODE plkit. utils DOCS \"\"\"Utility functions for plkit\"\"\" from typing import Iterable , List , Optional , Tuple , Union import sys import logging import warnings from io import StringIO from contextlib import contextmanager from rich.table import Table from rich.console import Console from rich.logging import RichHandler from diot import FrozenDiot from pytorch_lightning import seed_everything , _logger as logger from .exceptions import PlkitConfigException RatioType = Union [ int , float ] del logger . handlers [:] logger . addHandler ( RichHandler ( show_path = False )) logging . getLogger ( 'py.warnings' ) . addHandler ( RichHandler ( show_path = False )) def check_config ( config , DOCS item , how = lambda conf , key : key in conf , msg = \"Configuration item {key} is required.\" ): \"\"\"Check configuration items Args: config (dict): The configuration dictionary item (str): The configuration key to check how (callable): How to check. Return False to fail the check. msg (str): The message to show in the exception. `{key}` is available to refer to the key checked. Raises: PlkitConfigException: When the check fails \"\"\" checked = how ( config , item ) if not checked : raise PlkitConfigException ( msg . format ( key = item )) def collapse_suggest_config ( config : dict ) -> dict : DOCS \"\"\"Use the default value of OptunaSuggest for config items. So that the configs can be used in the case that optuna is opted out. Args: config: The configuration dictionary Returns: The collapsed configuration \"\"\" from .optuna import OptunaSuggest config = config . copy () collapsed = { key : val . default for key , val in config . items () if isinstance ( val , OptunaSuggest )} if isinstance ( config , FrozenDiot ): with config . thaw (): config . update ( collapsed ) return config config . update ( collapsed ) return FrozenDiot ( config ) def normalize_tvt_ratio ( DOCS tvt_ratio : Optional [ Union [ RatioType , Iterable [ RatioType ]]] ) -> Optional [ Tuple [ RatioType , List [ RatioType ], List [ RatioType ]]]: \"\"\"Normalize the train-val-test data ratio into a format of (.7, [.1, .1], [.05, .05]). For `config.data_tvt`, the first element is required. If val or test ratios are not provided, it will be filled with `None` All numbers could be absolute numbers (>1) or ratios (<=1) Args: tvt_ratio: The train-val-test ratio Returns: The normalized ratios Raises: PlkitConfigException: When the passed-in tvt_ratio is in malformat \"\"\" if not tvt_ratio : return None is_iter = lambda container : isinstance ( container , ( tuple , list )) if not is_iter ( tvt_ratio ): tvt_ratio = [ tvt_ratio ] tvt_ratio = list ( tvt_ratio ) if len ( tvt_ratio ) < 3 : tvt_ratio += [ None ] * ( 3 - len ( tvt_ratio )) if tvt_ratio [ 1 ] and not is_iter ( tvt_ratio [ 1 ]): tvt_ratio [ 1 ] = [ tvt_ratio [ 1 ]] if tvt_ratio [ 2 ] and not is_iter ( tvt_ratio [ 2 ]): tvt_ratio [ 2 ] = [ tvt_ratio [ 2 ]] return tuple ( tvt_ratio ) @contextmanager DOCS def warning_to_logging (): \"\"\"Patch the warning message formatting to only show the message\"\"\" orig_format = warnings . formatwarning logging . captureWarnings ( True ) warnings . formatwarning = ( lambda msg , category , * args , ** kwargs : f ' { category . __name__ !r} : { msg } ' ) yield warnings . formatwarning = orig_format logging . captureWarnings ( False ) @contextmanager DOCS def capture_stdout (): \"\"\"Capture the stdout\"\"\" _stdout = sys . stdout sys . stdout = stringio = StringIO () yield stringio del stringio sys . stdout = _stdout @contextmanager DOCS def capture_stderr (): \"\"\"Capture the stderr\"\"\" _stderr = sys . stderr sys . stderr = stringio = StringIO () yield stringio del stringio sys . stderr = _stderr @contextmanager DOCS def output_to_logging ( stdout_level : str = 'info' , stderr_level : str = 'error' ): \"\"\"Capture the stdout or stderr to logging\"\"\" with capture_stderr () as err , capture_stdout () as out : yield getattr ( logger , stdout_level )( out . getvalue ()) getattr ( logger , stderr_level )( err . getvalue ()) def log_config ( config , title = 'Configurations' , items_per_row = 1 ): DOCS \"\"\"Log the configurations in a table in terminal Args: config (dict): The configuration dictionary title (str): The title of the table items_per_row (int): The number of items to print per row \"\"\" table = Table ( title = title ) items = list ( config . items ()) for i in range ( items_per_row ): table . add_column ( \"Item\" ) table . add_column ( \"Value\" ) for i in range ( 0 , len ( items ), items_per_row ): row_items = [] for x in range ( items_per_row ): try : row_items . append ( items [ i + x ][ 0 ]) row_items . append ( repr ( items [ i + x ][ 1 ])) except IndexError : # pragma: no cover row_items . append ( '' ) row_items . append ( '' ) table . add_row ( * row_items ) console = Console ( file = StringIO (), markup = False ) console . print ( table ) logger . info ( '' ) for line in console . file . getvalue () . splitlines (): logger . info ( line ) def plkit_seed_everything ( config : FrozenDiot ): DOCS \"\"\"Try to seed everything and set deterministic to True if seed in config has been set Args: config: The configurations \"\"\" if config . get ( 'seed' ) is None : return seed_everything ( config . seed ) with config . thaw (): config . setdefault ( 'deterministic' , True )","title":"plkit.utils"}]}